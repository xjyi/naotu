{"root":{"data":{"id":"bx8b4ie4xs00","created":1569335836142,"text":"高并发"},"children":[{"data":{"id":"bx8b4r2ulns0","created":1569335855051,"text":"MQ 消息队列","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"bx8b8mdmtt40","created":1569336158276,"text":"为什么要使用MQ","note":"* 业务场景遇到什么技术挑战\n\t* 如果不用会很麻烦\n    \t* 但是用MQ就可以很好的处理\n        \n    * 适合场景\n    \t* 解耦\n        * 异步\n        * 削峰\n \n* 解耦\n\t* 系统对接多个系统\n    \t* 多套接口\n        \n    * 使用MQ，需要消息的就去获取\n    \t* 生产者消费者解耦\n        \n    * 场景\n    \t* 待补充\n\n\n* 异步\n\t* 一般互联网接口要求响应时间200ms以内\n    \t* 一个接口如果调用其他多个系统\n        * 总耗时可能长达1s，则性能太差\n    * 此时可以用MQ做异步处理\n    \t* 发送到MQ的时长很短，不需要等系统响应\n        \n        \n* 削峰\n\t* 5000QBS属于高并发（约100万用户）\n    \t* mysql一般能扛到2000个请求每秒\n        * 一万用户在线，50qbs也是正常的\n   \n   * 用户请求到达系统前，先用mq存储","layout":null},"children":[]},{"data":{"id":"bx95923cvjs0","created":1569420825780,"text":"优缺点","note":"* 优点\n\t* 见“为什么使用”\n    \n* 缺点\n\t* 系统可用性降低\n    \t* MQ故障\n        * 解决:钉钉预警\n    \n    * 系统复杂性提高\n    \t* 数据发送了多次\n        * 解决：接口的幂等性（请求流水号）\n        \n        \n        * 消息丢了/顺序乱了\n    \n    * 一致性问题\n    \t* 某一个MQ执行失败\n    \n","layout":null},"children":[]},{"data":{"id":"bx8bdi39oq80","created":1569336540763,"text":"技术选型","note":"* 中小型公司用rabbitMQ\n\n* 大公司用rocketMQ\n\t* 性能各方面做的很好\n    * 大公司有能力维护rocket源码\n    \n\n* 大数据领域（实时计算）、数据采集\n\t* kafaka是业内标准，规范","expandState":"collapse","layout":null},"children":[{"data":{"id":"bx95styjopc0","created":1569422375358,"text":"activeMQ","note":"1. activeMQ\n\t* 单机吞吐量 万级\n    \t* 每秒处理数据量\n    * 时效性\n    \t* 毫秒ms\n    * 可用性 高\n    \t* 基于主从架构\n    * 消息可靠性\n    \t* 较低概率丢失数据\n        \n \n* 总结\n\t* 技术成熟\n    \t* 出了很久了（以前mq技术首选）\n        \n    * 缺点\n    \t* 有较低概率丢数据\n      \t* 官方社区不活跃\n        * 较少用在大规模吞吐的场景","layout":null},"children":[]},{"data":{"id":"bx95t04j1p40","created":1569422388781,"text":"rabbitMQ","note":"2. rabbitMQ\n\t* 单机吞吐量 万级\n    * 时效性（最大优点，低延迟）\n    \t* 微秒\n    * 可用性 高\n    \t* 基于主从架构\n        \n\n* 总结\n\t* 基于erlang开发\n    \t* 并发能力强，性能好，延时低\n        * 即消息在mq内部处理的时间少\n        * 此优点不明显\n        * 源码不好阅读\n    * 管理界面友好，中小型公司多使用\n    * 社区相对活跃","layout":null},"children":[]},{"data":{"id":"bx95t5gijco0","created":1569422400389,"text":"rocketMQ","note":"3. rocketMQ\n\t* 单机吞吐量 十万级\n    * 时效性\n    \t* 毫秒ms\n    * 可用性非常高\n    \t* 分布式架构\n\n* 总结\n\t* topic 增加到几百上千时\n    \t* 吞吐量有小幅下降\n    * 阿里开源，品质保障\n    \t* java语言\n    * 社区活跃度尚可\n    \n    * 公司假如不再维护，会有较大坏影响","layout":null},"children":[]},{"data":{"id":"bx95tdzbojk0","created":1569422418941,"text":"kafka","note":"\n4. kafka\n\t* 单机吞吐量 十万级（最大优点，吞吐量高）\n    \t* 一般配合大数据类的系统进行实时数据计算，日志采集\n        \n    * 时效性\n    \t* 毫秒ms\n        \n    * 可用性非常高\n    \t* 分布式架构，一个数据多个副本\n        * 少数机器宕机，不丢失数据，不会导致不可用\n        \n* 总结\n\t* topic较多时（上百）\n    \t* 性能下降较快\n        \n    * 只能支持简单的MQ功能\n    \t","layout":null},"children":[]}]},{"data":{"id":"bx9zoi4swdc0","created":1569506669456,"text":"如何 保证消息队列的高可用","note":null,"expandState":"collapse","layout":null},"children":[{"data":{"id":"bxa0caftac80","created":1569508533447,"text":"rabbitMQ","note":"* rabbitMQ\n\t* 非分布式\n    * 有集群\n    \t* 单机模式\n        * 普通集群模式\n        * [镜像集群模式]\n\n* 开启镜像集群模式\n\t* 管理后台新增一个策略\n    \t* 即该策略\n    * 可以要求数据同步到所有节点\n    \t* 也可以要求同步到指定数量节点\n        \n* 普通集群\n\t* 创建的 每一个队列，只会保存在其中一个节点（机器上）\n    \t* 该节点包含了元数据、实际的数据\n        * 元数据即是一些配置信息，如结构、名称等\n        * 其他节点上保存的是队列的元数据，以及有实际数据的节点位置\n    \n    \n    * 消费者去到没有数据的节点拿数据，则该节点会去有数据的节点拉取\n    \t* 可能导致mq内部大量数据传输\n    \n    \n    * 可以提高消费吞吐量\n    \t* 可用性几乎没有保障\n        * 存数据的节点宕机，则无法消费\n        \n* 镜像集群模式\n\t* 支持高可用\n    \t* 一个节点挂了，其他节点还有数据\n    * 队列的元数据、数据在每个节点上都有\n    \t* 写入数据的节点会对其他节点进行同步\n    * 非分布式\n    \t* 数据量达到单机承受布不了的话会有问题","layout":null},"children":[]},{"data":{"id":"bxa0hwtc5co0","created":1569508973975,"text":"kafka","note":"* 分布式\n\t* 每台机器（节点）有一个broker进程\n    \t* 每台机器+机器上的broker进程，就是kafka集群中的一个基点\n    \n    * 创建一个topic\n    \t* topic会被分成多个partition（每个只有一部分数据）\n        * 不同partition可以存在不同的broker上\n        * 假如没有HA机制，一个节点挂了，数据就会丢失一部分\n\n\n\n* topic\n\t* 存储消息的一个逻辑概念\n    \t* 可认为是消息集合\n\n\n* HA机制（高可用机制）\n\t* kafka 0.8之后，提供了高可用机制\n    * replia副本机制\n    \t* 每个partition数据会同步到其他机器上，形成多个replia副本\n        \n        * 副本间会选举出一个leader，其他的为follower\n        \n        * 生产、消费都是跟leader打交道（其他不行），leader会同步数据到其他副本\n        \n        * 生产消费都是先全部同步成功，在返回ack给生产、消费者\n        \n    * 则一个节点挂了，数据还在，还能用\n    \t* 自动感知leader挂了，选举出新的leader","layout":null},"children":[]}]},{"data":{"id":"bxb99ohnf340","created":1569635278887,"text":"如何保证消费消息时的幂等性","note":"* 即防止重复消费\n\t* 加请求流水号\n    \t* 放进redis，使用setNX处理\n        * 提交事务之后删除\n        * 请求全部先检查缓存，再检查数据库 \n    * 数据库主键唯一限制\n\n* MQ是不保证消息只发一次的\n\n\n\n* kafka中\n\t* 每条消息都有一个offset\n    \t* 代表了消息的顺序的序号\n        * kafka根据消息进入的顺序进行分配的\n     \n    * 消费者从kafka消费的时候，是按照顺序去消费的 \n    \t* 消费者消费完会提交offset，即通知kafka已经消费该条消息\n        \n        * 基于zookeeper实现\n        * 提交offset不是实时的，是定时提交一次（所以已经消费了的，可能没提交offsset就重启了）\n        \n    * zookeeper会记录消费者当前消费到offset等于几的消息\n    \t* kafka能感知到该offset\n     ","layout":null},"children":[]},{"data":{"id":"bxba90ie7000","created":1569638047799,"text":"如何保证消息的可靠性传输","layout":null,"note":"* 如何处理消息丢失问题\n\n\n\t","expandState":"collapse"},"children":[{"data":{"id":"bxbbp5r99kg0","created":1569642134155,"text":"生产者到MQ","expandState":"expand","layout":null},"children":[{"data":{"id":"bxbbghbai0g0","created":1569641454034,"text":"事务机制","note":"* rabbitMQ 支持事务\n\t* channel.txSelect()声明启动事务模式\n    * channel.txComment()提交事务\n    * channel.txRollback()回滚事务\n    \n    * 事务中发生异常可先回滚，再次发送消息\n    \n\n* 特点\n\t* 事务机制，是同步的\n    \t* 生产者发送消息会阻塞（等待响应结果）\n        * 影响吞吐量","layout":null},"children":[]},{"data":{"id":"bxbbj1vo5n40","created":1569641655530,"text":"确认机制","note":"* rabbitmq channel设置成confirm机制\n\t* 再发送消息\n    \n    * rabbitmq接收到消息之后\n    \t* 回调生产者，通知已经接收成功\n    * rabbitMq接收消息报错\n    \t* 回调接口，通知失败。生产者此时可选择再次重发\n        \n\n\t* 不阻塞，吞吐量高\n\n\n* kafka\n\t* 设置acks=all 也是确认机制","layout":null},"children":[]}]},{"data":{"id":"bxbbpy56jnc0","created":1569642195947,"text":"MQ数据丢失","note":"* rabbitMQ\n1. 创建queue时将其设置为持久化的\n\t* 将会持久化queue的元数据\n    \n2. 发送消息时设置deliveryMode为2\n\t* 消息持久化\n    \n* 可以结合确认机制\n\t* 持久化后才发送接收成功通知\n    \n\n* kafka\n1. leader宕机，部分数据没来得及同步到follower\n   * 数据丢失\n        \n2. 参数设置\n   * replication.factor大于1\n        * 即至少2副本（副本计算包括leader）\n        \n    * min.insync.replicas大于1\n        * 即leader感知到至少有一个follower跟自己保持联系\n        * 确保leader挂了还有一个follower\n        \n    * 在生产者端设置acks=all\n    \t* 即要求每条数据必须写入所有的replia后，才算是成功\n        \n    * 生产者端设置retries=很大的一个值\n    \t* 写入失败，无线重试\n        * 即上面的配置，至少2副本等条件，不满足也算是失败，会不断重试","layout":null},"children":[]},{"data":{"id":"bxbbufe2vx40","created":1569642546947,"text":"消费者丢失数据","note":"* rabbitmq设置了autoAck 自动通知mq\n\t* 才会出现\n    \n    * 一般要关掉\n    \t* rabbitmq会重发给其他消费者\n     \n     \n     \n* kafka\n\t* 跟rabbitmq差不多\n    \t* 设置了自动提交offset\n    * 改为手动提交即可","layout":null},"children":[]}]},{"data":{"id":"bxbgjwa9cbs0","created":1569655827265,"text":"如何保证消息的顺序性","note":"* 如mysql binlog同步\n\t* 顺序性必须保证\n\n* rabbitMQ\n\t* queue中，消息只能被消费一次\n    \t* 多消费者情况下，顺序性不能保证\n    * 解决\n    \t* 需要保证顺序性的数据，都一个队列对应一个消费者\n        \n\n* kafka\n\t* 写入一个partition的数据，一定是有顺序的\n    \t* 生产者写的时候，可以指定一个key（如订单id）\n        * 跟此订单相关的数据，一定分配到一个partition中\n        * 可使用hash进行分发\n    \n    * 一个partition只对应一个消费者\n    \t* 本来也做不到对应多个消费者\n    \t* 则可以保证顺序性\n        \n    * 消费者内部多线程处理\n    \t* 思路跟上面多个partition一样\n        * 再次进行hash分发，相同订单号的放进一个一个内存队列，一个线程对应一个内存队列\n        * 则可以保证顺序性\n     \n     * 单线程的消费者\n    \t* 一条需要几十ms的话，一秒处理几十条数据，吞吐量较低\n        * 所以一般还是多线程\n        \n        * 压测，4核8G的机器，单机开32线程，每秒可处理上千条数据","layout":null},"children":[]},{"data":{"id":"bxbi48jsytk0","created":1569660242357,"text":"消息队列满了怎么处理","note":"* 或者问：几百万数据积压怎么处理\n\t* 对于rabbitmq，可以设置过期时间\n\t\t* 时间长消息会丢失（因此不能这样设置）\n        * 如果已经发生，则写程序从数据源头重新导出，进行补偿\n\n\n* 消费端出现问题\n\t* 导致mq满了，或者大量积压。以及消息已经延迟等问题\n    \n* 处理\n\t* 当务之急，肯定是修复消费者 \n    \n    * 新加一个Topic（kafka），并且partition是原来的10倍\n    \t* 消费者消费到数据后，不执行原来的逻辑，而是重写写到新的topic\n        \n        * 新加机器，作为消费者去处理新的topic\n        \n\t* 如果mq满了，则要加新的临时mq处理\n    \t* 加不了那就扔掉数据，后续写程序进行补偿","layout":null},"children":[]},{"data":{"id":"bxbafics9gg0","created":1569638556827,"text":"项目场景","note":"* 账务系统放款\n\t* 账务方面计算费用，是比较耗时的 一个业务\n    \t* 做成异步\n        \n    * 同时账务系统对接多个贷前入口\n    \t* 解耦","layout":null},"children":[]},{"data":{"id":"bxbiwnbvr4g0","created":1569662468726,"text":"如何设计一个消息队列","note":"* 首先MQ需要支持扩容\n\t* 即可伸缩性\n    * 设计分布式系统，参考kafka\n    \t* 每个broker是一个机器，是集群的其中一个节点\n        * 一个topic可以拆成多个partition来存储\n        * 资源不够就增加partition（增加机器）\n        * 有需要的话还可以做数据迁移\n        \n* 其实考虑数据做持久化（落盘）\n\t* 保证数据不丢失\n    * 在磁盘做顺序写，性能会比随机写性能更好\n    \t\n        \n* 其次 考虑mq的可用性\n\t* 如kafaka的多副本的高可用保障机制\n    \n* 数据丢失问题处理\n\t* 确认机制等","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"bxbji7jgrxk0","created":1569664158368,"text":"关于顺序写和随机写","note":"* 随机和顺序读写，是存储器的两种输入输出方式。\n\t* 连续的磁盘，删除数据后留下空洞\n    \t* 读取这些不连续的空间的数据，就是随机写，速度慢\n        * 因为磁头要不断的调整磁道的位置，以在不同位置上的读写数据，\n    * 连续的磁盘进行读写，速度快","layout":null},"children":[]}]}]},{"data":{"id":"bxbjseo53sw0","created":1569664957529,"text":"redis 分布式缓存","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"bxbjzn6q3e80","created":1569665524616,"text":"为什么要用缓存","note":"* 高性能，高并发\n\n* 不变的数据，如系统配置、商品信息\n\t* 多人访问，每次从数据库获取，则浪费性能\n    \t* 存进缓存，速度快\n        \n    * 常用的复杂查询的结果也可以放进缓存\n    \n    \n        \n* 访问高峰期时（高并发）\n\t* 对数据库造成很大压力，可能宕机\n    \t* 使用缓存减轻压力\n \n \n \n* 缓存为什么快\n\t* 缓存是走内存的，天然速度快，能支撑高并发访问\n\t\t* 几万/s的访问也是没问题的\n\n\t* 数据库则不行\n\t\t* 一般建议并发不能超过2000/s\n        \n   ","layout":null},"children":[]},{"data":{"id":"bxbm7rv5axk0","created":1569671803933,"text":"与mencache的区别","note":"* redis作者给出的比较\n\t1. 支持更多数据结构与操作\n    \n    2. 内存使用效率对比（不重要）\n    \t* 简单的key-value，是memcache高\n        * redis采用hash结构来做key-value的话，redis会更高\n        * 因为是组合式的压缩\n        \n    3. 性能（跟第二点一样，区别比较轻微，可不说）\n    \t* redis只是用单核（单线程）\n        \t* 平均到每个核上，redis在存储小数据时，性能比memcache高\n            * 100k以上的大数据，memcached性能高一些\n        * memcache可以使用多核\n    \n    4. memcached 没有原生的集群模式\n    \t* 依靠客户端实现分片（集群中分片写入数据）\n        * redis原生支持cluster模式(官方支持)","layout":null},"children":[]},{"data":{"id":"bxbm2zh3hcw0","created":1569671428677,"text":"redis的线程模型","note":"* 文件事件处理器（file event handler）\n\t* redis基于rector模式开发的网络事件处理器\n    * 是单线程的\n    \t* 采用IO多路复用机制同时监听多个socket\n        * 根据socket上的事件来选择对应的事件处理器处理这个事件\n     \n     \n\n* 文件事件处理器结构\n\t* 多个socket\n\n\t* IO多路复用程序\n\n\t* 文件事件分派器\n    \n    * 事件处理器\n    \t* 命令请求处理器\n        * 命令回复处理器\n        * 连接应答处理器\n        \n* redis单线程模型为啥效率高\n\t* 纯内存操作\n    * 核心是基于非阻塞的IO多路复用机制\n    \t* 由于不阻塞，效率高（相当于不等待，一直处理各种事情）\n        \n    * 单线程避免线程频繁上下切换\n    \t* 内核态、用户态切换耗性能\n\t","layout":null},"children":[]},{"data":{"id":"bxbmwjwq9ag0","created":1569673745719,"text":"客户端与redis的一次通信","note":"* 基于socket通信模型进行通信 \n\t* redis 中有一个server socket监听请求\n \n* redis启动\n\t* 会将AE_READABLE事件与连接应答处理器进行关联\n\n\n\n\n1. IO多路复用程序会监听socket\n    * 将监听到事件的socket（及其产生的事件）压到一个队列里面\n    \n    * 每次取出一个socket给事件分派器\n    \t* 处理完之后才拿下一个\n    \n    * 文件事件分派器会将队列里面的socket取出\n    \t* 交给对应的处理器进行处理\n        \n        \n\n\n\n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxboek46s2g0","created":1569677977835,"text":"连接","note":"2. 例：客户端连接到redis的server socket，请求建立连接\n\t* server socket接收到请求后，会产生AE_READABLE事件\n        \n    * 第1点中的处理步骤\n        \n    * 最终交给连接应答处理器进行处理\n        * 连接应答处理器与客户端建立连接，创建客户端对应的socket（客户端本身有一个socket）\n        \n        * 将此socket的AE_READABLE事件与命令请求处理器关联","layout":null},"children":[]},{"data":{"id":"bxboerr4n3c0","created":1569677994459,"text":"请求","note":"3. 客户端发起写读写请求\n\t* 则已经建立连接的socket会产生AE_READABLE事件\n    \n    * 同样，IO多路复用程序处理\n    \t* 第一点\n        \n    * 交给命令请求处理器处理\n    \t* 从socket中读出请求的key，value（假设是写请求）\n        * 在内存中key，value的设置\n\t\n    \n* 处理完毕后（准备好响应数据后）\n    * 将socket的AE_WRITABLE事件跟命令回复处理器关联\n        \n    * 客户端准备好读取数据时，socket上会产生AE_WRITABLE事件\n    \n    * 同样，IO多路复用程序处理\n    \t* 最终交给命令回复处理器处理\n   \n    * 命令回复处理器对socket输出本次操作的一个结果\n    \n    * 将此socket与AE_WRITABLE事件解除关联\n        \n        ","layout":null},"children":[]}]},{"data":{"id":"bxcj9ew2ksg0","created":1569765029046,"text":"五种数据类型","note":"1. String\n\t* 存字符串\n\n2. Hash\n\t* 存简单对象等\n    \t* 第二个key作为字段\n3. list\n\t* 有序list（有序就是插入顺序）\n    \n4. set\n\t* 无序集合，自动去重\n    \t* 集群多台机器一起做去重等\n        \n5. sorted set\n\t* 自动去重，排序，按照分数排序\n    \t* 用户分数排行榜\n        * IP地址区域分类等","layout":null},"children":[]},{"data":{"id":"bxcjtk2unmg0","created":1569766607624,"text":"redis过期策略","note":"* 定期删除+惰性删除\n\t* redis默认每隔100ms，随机抽取一些有设置供货期时间的key\n    \t* 检查是否过期，过期就删了\n        * 注意是随机抽一些，不是全部遍历\n        \n    * 惰性\n    \t* 获取这个key时会检查一下，已经过期了就删\n   \n   \n* 内存淘汰机制\n\t* 防止堆积太多数据\n    \t* 以上两种方式，可能还是会导致大量数据残留\n    \n    * noeviction\n    \t* 内存不足时写入数据，写入报错\n    * allkeys-lru（常用）\n    \t* 内存不足时写入数据\n        * 移除最近最少使用的key\n    \n    * allkeys-random\n    \t* 内存不足时，随机移除某个key\n        \n    * volatile-lru\n    \t*  内存不足时，在有设置过期时间的key中，移除最近最少使用的\n    * volatile-random\n    \t*  内存不足时，在有设置过期时间的key中，随机移除key\n    * volatile-ttl\n    \t*  内存不足时，在有设置过期时间的key中，优先移除有更早过期时间的（快过期的）","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxckd21bxjk0","created":1569768135633,"text":"手写LRU","note":"* 结合linkedHashMap 处理\n\t* 继承linkedHashMap\n    \t* 构造函数的accessOrder传入true\n    * 重写removeEldestEntry\n    \t* 满足特定条件返回true，将删除最老的元素\n        * 条件即大于设定大小时\n","layout":null},"children":[]}]},{"data":{"id":"bxd5dcwkiz40","created":1569827402598,"text":"高并发高可用","note":"* 高并发中，缓存是必不可少的\n\t* redis不能支持高并发的瓶颈在于单机\n    \t* 单机支持QBS是一到几万（一般不到10w）\n    \n    * 主要针对的是读高并发\n    \n    \n* 不可用\n\t* 系统因各种原因挂了，此时可能在抢修之类\n    \t* 用户无法访问使用，就是不可用\n \n \n*  99.99%高可用\n\t* 全年内99.99%的时间可以使用\n    * 一般99%都是高可用了\n\n\n\n* redis高可用\n\t* 一个slave挂了，不影响可用性\n    * master挂了\n    \t* 就是相当于缓存不可用\n        * 大量的请求直接涌入mysql，进而导致系统挂了\n\n\n\n* 基于哨兵的主从-高可用架构（故障转移 failover）\n    * 主备切换\n        * master故障，自动检测，并将一个slave变成master\n        \n    * 监测master的是sentinal node\n    \t* 哨兵","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxd61knidnc0","created":1569829300204,"text":"读写分离","note":"* 一主多从\n\t* 主master节点负责写（缓存使用场景一般都是读多写少）\n    \t* 主节点同步数据到从节点\n    * 从slave节点负责读\n    \n    * 优点（水平扩容）\n    \t* 承担更多QBS的时候只需新增从节点\n\n\n","expandState":"expand","layout":null},"children":[{"data":{"id":"bxd71piokuo0","created":1569832131906,"text":"redis replication","note":"* 主从复制机制redis replication\n\n\t* redis使用异步方式复制数据到slave节点\n    \t* 2.8开始，slave node会周期性确认自己每次复制的数据量\n     \n    * 一个master node 可以配置多个slave node\n    * slave node 也可以连接其他slave node\n    \n    * slave node做复制时\n    \t* 不会block master node的正常工作\n        * 也不会block 自己的查询操作（使用旧数据集提供服务）\n\t\t* 复制完成时需要删除旧数据集，加载新数据集。此时暂停对外服务（毫秒到秒之间的级别）\n    \n    * slave node主要用于横向扩容\n    \t* 做读写分离\n        * 扩容的slave node可以增加读的吞吐量\n    \n    \n    \n    \n* master node的持久化\n\t* 使用了主从架构，master node持久化必须开启\n    \t* 不建议使用slave node作为master node的数据热备\n        * 因为不开持久，master重启后，数据是空的，经过复制，slave node数据也没了\n    * 甚至，RDB文件也进行备份\n        \n    * 即使在高可用机制下\n    \t* slave node自定接管master，也有可能sentinal没检测到master failure，master就自动重启了，还是会导致以上情况\n        \n        \n        \n   ","layout":null},"children":[]},{"data":{"id":"bxd7fnxir5k0","created":1569833225548,"text":"主从复制原理","note":"1. 启动slave node时\n\t* 发送一个PSYNC命令给master node\n    \t* 如果是slave node重连master node，则只会复制给slave部分缺失的数据\n        * 如果是第一次连接master，会触发full resynchronization\n    \n    * full resynchronization\n    \t* master启动一个后台线程。生成一份全量RDB快照文件\n        * 并同时在内存中缓存最新的数据\n        \n        * slave会先保存RDB文件到磁盘 并加载到内存\n        * master把内存中的数据同步到slave\n        \n    * slave如果网络故障断开连接\n    \t* 会自动重连\n        * master如果发现多个slave来重连，只会启动一个RDB save操作，用一份数据服务所有slave \n\n2. 正常连接时\n\t* 来一条写请求，master会异步同步给slave\n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxdzckuzk800","created":1569911974849,"text":"详细复制步骤","note":"1. slave node 启动，仅保存master node的信息\n    * master node的host，ip等\n    \t* 信息是从redis.conf的slaveof获取的信息\n        * 此时复制流程还没开始\n \n \n2. slave node内部有一个定时任务\n\t* 每秒检查是否有新的master node要连接和复制\n    \t* 有的话，跟master node建立socket网络连接\n  \n  \n3. slave node发送ping命令给master node\n\n\n4. 口令认证\n\t* 如果master设置了requirepass\n    \t* 则slave node序发送masterauth口令进行认证\n    \n    \n5. master node第一次执行全量复制，将所有数据发送给slave node\n\n\n6. master node后续持续写命令，异步复制给slave node","expandState":"expand","layout":null},"children":[{"data":{"id":"bxdzrrlrs0w0","created":1569913164992,"text":"数据同步相关核心机制","note":"* 指第一次slave连接master时执行的全量复制\n\t1. master 和slave都会维护一个offset\n    \t* master、slave都会在自身不断累加offset\n        \n        * slave每秒都会不断上报自己的offset给master\n        * 同时master也会保持每个slave的offset\n        \n    2. 第一点不特指只用在全量复制过程\n    \t* 主要是master和slave都要知道各自数据的offset，才能知道相互之间的数据不一致情况 \n        \n    3. backlog\n    \t* 全量复制时。master node中有一个backlog，默认1M\n        * master node给slave node复制数据时，也会在backlog中同时写一份\n        * 全量复制中断时，做增量复制的\n    4. master runid\n    \t* 使用命令：info server 可查看该run id\n        * 因为根据host+ip定位master node是不靠谱的\n        * 例如master重启，或master根据RDB备份恢复到某一阶段的数据，此时需要全量重新同步，就可以利用变化了的runid进行识别\n        * runid不同，就会触发全量复制\n        * 不更改runid重启redis命令：redis-cli debug reload\n     \n    5. psync\n    \t* slave node发送psync runid offset 给master 节点\n        * master会根据自身情况响应信息\n        * 可能是 FULLRESYNC runid offset 触发全量复制，也可能是continue触发增量复制","layout":null},"children":[]},{"data":{"id":"bxe16v4tj6g0","created":1569917169246,"text":"全量复制","note":"1. master执行bgsave，在本地生成rdb快照文件\n\n2. master 将rdb文件发送给slave ，如果复制（发送）时间超过repl-timeout指定时间\n\t* 默认60s（可根据具体文件大小适当调整）\n    * 超过时间认为复制失败\n\n3. 生成rdb的同时，会将新的命令缓存在内存中\n\t* slave保存完rdb后\n    \t* 再将新的命令复制给slave\n        \n    * client-output-buffer-limit slave 256MB 64MB 60（配置文件参数）\n    \t* 表示在复制期间，内存缓冲区持续（60s内）消耗超过64MB，或者一次性消耗超过256MB，则停止复制，本次复制失败\n        \n4. slave节点接到rdb文件后，清空自己的旧数据\n\t* 重新加载rdb到自己的内存\n    \t* 且同时使用旧数据集对外提供服务\n    * 如果开启了AOF\n    \t* 立即执行BGREWROTEAOF，重写AOPF（基于当前最新内存数据）\n        \n* 整个复制时长\n\t* 4~6G数据，大约1.5~2min","layout":null},"children":[]},{"data":{"id":"bxe1lyfi9gg0","created":1569918351885,"text":"增量复制","note":"* 全量复制过程中。网络断掉，slave重连\n\t* 触发增量复制\n    \n    * master从backlog中获取丢失的数据，发送给slave\n    \t* 根据offset判断","layout":null},"children":[]},{"data":{"id":"bxe1p0651940","created":1569918590765,"text":"heartbeat","note":"* 正常状态下的主从节点（完成复制之后等）\n\t* master每隔10s发送一次heartbeat\n    * salve每隔1s发送一次heartbeat\n\n* 异步复制\n\t* 之后新的命令，master先自己写入数据\n    \t* 之后异步发给clave","layout":null},"children":[]}]},{"data":{"id":"bxdenqfbw2g0","created":1569853605661,"text":"断点续传+ 无磁盘化复制","note":"3. 断点续传\n\t* redis 2.8开始支持\n    \t* 主从根据RDB文件进行大数据集复制时\n        * 复制过程中，网络断掉，可以接着上次复制的地方继续复制，而不是重新从头开始\n        \n    * master会在内存中维护一个backlog\n    \t* master和slave都会保存replica offset\n        * offset就是保存在backlog中\n        * 此时如果网络断了重连，会从offset开始开始复制\n        * offset找不到的话，就要执行resynchronization\n        \n\n4. 无磁盘化复制\n\t* master直接在内存中创建rdb，发给slave，不会在自己本地落盘\n    \n\t* 通过以下参数配置生效使用\n    \t* repl-diskless-sync\n    * 另外，参数repl-diskless-sync\n    \t* 等待一定事件再开始复制\n        * 等更多slave连上来一起复制\n        \n        \n5. 过期key处理\n\t* slave不会过期key\n    \t* 等master过期key\n        * master的key过期后，或LRU淘汰一个key，会模拟一条del命令发送给slave","layout":null},"children":[]}]}]},{"data":{"id":"bxe2gfmpshk0","created":1569920740252,"text":"sentinal 哨兵","note":"* 哨兵是redis集群架构中非常重要的组件\n\t* 集群监控\n    \t* 监控redis mater和slave进程是否正常工作\n        \n    * 消息通知\n    \t* 如果某个redis实例有故障，哨兵负责发送消息作为报警通知管理员\n        \n    * 故障转移\n    \t* 如果master node挂了，会自动转移到slave node上\n    \n    * 配置中心\n    \t* 如果发生了故障转移，通知client客户端新年的master地址\n\n\n* 一般就在redis实例所在机器上，对应放一个哨兵进程\n\n\n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxe3phhl4g80","created":1569924270682,"text":"哨兵集群","note":"* 哨兵本身是分布式的\n\t* 作为一个哨兵集群去运行，互相协调工作\n    \t* 判断master是宕机了，需要大部分的哨兵都同意\n        * 部分哨兵节点挂了，哨兵集群可以正常工作\n    \n    \n    * 哨兵至少3个实例，保证自己的健壮性\n    \t* quorum=N 表示需要多少哨兵同意\n        * 如果是2个实例，设置参数为1，此时挂了一个（master+1哨兵），则同意切换\n        * 但是需要选举出一个哨兵来执行故障转移（判断宕机+选举哨兵执行都是可以的）\n        * 但是此时需要majority来允许故障转移\n        \t* 即：（一半以上，4个节点的貌似是2）的个数的哨兵在运行\n          \n    * 哨兵+ rdis主从架构\n    \t* 不保证数据零丢失，只保证redis集群高可用","layout":null},"children":[]},{"data":{"id":"bxe45x0m9y80","created":1569925558312,"text":"哨兵主备切换数据丢失问题","note":"* 由于主从的复制是异步的\n\t* 因此slave的数据可能还没及时更换\n    * 此时如果master node挂了\n    \t* 则主从切换后，未同步的数据丢失\n\n\n* 网络故障（集群脑裂），导致网络分区\n\t* 例：master所在的网络不能与哨兵集群以及slave node访问\n    \t* 但是哨兵集群可以跟slave保持网络通讯\n        * 此时哨兵会认为master挂了，切换salve为master\n     \n    * 出现两个master节点\n    \t* 像人的大脑变成2个（所以叫脑裂）\n    \n    * 解决网络问题后，原先的master调整为slave\n    \t* 但是原本的master并没有挂，里面可能有数据并未同步到slave\n        * 此时丢失数据\n\n\n* 解决（降低损失）:参数配置\n    * min-slave-to-write N\n    * min-slave-max-lag M\n    \t* 减少异步数据的丢失\n        \n    * 要求至少有N个slave，数据复制和同步的延迟不能超过M秒\n    \t* 达不到此要求的话，master此时就不再接收任何请求\n        \n        * client此时可做降级，写到磁盘，再做限流，减少请求涌入的速度\n        * 或者将数据临时灌入kafka，每隔10分钟去队列里面取一次，尝试重新发回master\n    * 脑裂场景也是\n    \t* 没有slave反馈落后数据在10s之内，master不接受请求，则网络恢复后损失的数据少","layout":null},"children":[]},{"data":{"id":"bxe5gchd57k0","created":1569929196727,"text":"sdown和odown","note":"* sdown和odown\n\t* sdown（主观宕机）\n    \t* 一个哨兵自己觉得一个master宕机\n        * 哨兵ping一个master，超过配置is-master-down-after-milliseconds指定的毫秒数，就是主观宕机\n        \n    * odown（客观宕机）\n    \t* N个哨兵觉得master宕机\n        * N==quorum的配置数量\n        \n    * sdown转换到odown\n    \t* 一个哨兵在指定时间内，收到quorum指定数量的的其他哨兵也认为sdown的信息，就是odown","layout":null},"children":[]},{"data":{"id":"bxe627w66lc0","created":1569930910750,"text":"哨兵和slave集群的自动发现机制","note":"1. 哨兵相互之间的发现\n\t* 通过redis的pub/sub（发布订阅）实现\n    \t* 每个哨兵都会往_sentinal_:hello 这个channel发送一个消息\n        * 其他哨兵可以消费这个消息，并感知其他哨兵的存在\n    \n2.  每隔2s，每个哨兵都会往自己监控的某个master+slaves集群对应的_sentinal_:hello channel里发送一个消息\n    * 内容是自己的host、ip及runid，以及对这个master的监控配置\n    \n    * 每个哨兵也会去监听自己监控的每个master+slaves对应的_sentinal_:hello channel\n    \t* 然后会感知同样监听这个master+slaves的其他哨兵的存在\n        \n        * 每个哨兵还会跟其他哨兵交换对master的监控配置，对配置进行同步","layout":null},"children":[]},{"data":{"id":"bxe6domzhbc0","created":1569931809206,"text":"slave配置自动纠正","note":"* 哨兵会负责纠正slave的一些配置\n\t* 例slave如果要成为master的潜在候选人\n    \t* 则哨兵会确保slave在复制现有master的数据\n        \n    * slave连接到一个错误的master上\n    \t* 例如故障转移后\n        * 则哨兵会确保他们连接到正确的master上","layout":null},"children":[]},{"data":{"id":"bxe6hslzgkw0","created":1569932131309,"text":"slave-master选举算法","note":"* 一个master被认为odown了\n\t* 且majority哨兵允许了主备切换\n    \t* 某个哨兵将执行主备切换\n    * 此时需要选举一个slave\n\n\n* 选举算法\n\t1. 先查看跟master断开连接的时长\n    \t* 时长如果超过配置down-after-milliseconds的10倍+master的宕机时长\n        * 公式：（down-after-milliseconds* 10）+ millliseconds _since_master_id_in_SDOWN_state\n        \n        * 则该slave被认为[不适合]选举为master\n        \n    2. 剩下的slave进行排序\n    \t1. 按照slave优先级进行排序，该配置：slave-priority越小，优先级越高\n        2. 如果优先级相同，就看replica offset，哪个slave复制的数据越多，offset越靠后，优先级越高\n        \n        3. 以上条件都相等，就选runid小的slave","layout":null},"children":[]},{"data":{"id":"bxe6sspgaq80","created":1569932993525,"text":"quorum和majority","note":"* quorum数量的哨兵认为sdown\n\t* 变成odown\n    * 然后会选举一个哨兵来做主备切换\n    \t* 但是这个哨兵还得得到majority数量的哨兵的授权才能正式执行切换\n \n* quorum < majority\n\t* 可以进行切换\n    \t* 运行的哨兵比认为宕机的数量多\n* quorum >= majority\n\t* 正在运行的哨兵小于等于认为master宕机的哨兵\n    \t* 此时必须有quorum数量的哨兵都同意，才能切换\n        \n        \n* configuration epoch\n\t* 哨兵对一套redis master+slaves进行监控，会有相应的监控配置\n    \t* 执行切换的哨兵，要从新master中得到一个configuration epoch，是一个version号\n        * 每次切换的version号都必须唯一\n    \n    * 如果第一个哨兵切换失败\n    \t* 等待failover-timeout时间，然后接着继续执行切换\n        * 此时会重新获取一个新的configuration epoch，作为新的version、\n      \n* configuration的传播\n\t* 哨兵切换完成之后，会在自己本地更新生成最新的master配置\n    \t* 然后同步给其他哨兵\n        * 即之前说的pub/sub消息机制\n        \n    * 此时version号就很重要了\n    \t* 因为消息都是通过一个channel去发布和监听的\n        * 切换完成后，新的master配置对应新的version号，其他哨兵发现有新的版本号，就会更新自己的配置","layout":null},"children":[]}]},{"data":{"id":"bxe7dosnw8g0","created":1569934630659,"text":"总结","note":"* 高并发\n\t* 使用一主多从\n    \t* 主负责写，从负责读\n    \n    * 但是这种每台机器的数据是一样的\n    \t* 会有可能单机内存的上限\n\n \n* 高可用\n\t* 主从架构的话\n    \t* 加上哨兵机制即可\n        * 自动主备切换"},"children":[]}]},{"data":{"id":"bxe7moysd480","created":1569935336307,"text":"持久化","note":"* 持久化的意义\n\t* 故障恢复\n    \n    * 如遇到灾难性的故障\n    \t* 机器没了，断电等\n        \n    * 定期同步、备份持久化的数据到云存储服务上去\n    \t* 起新的机器，读取备份文件\n        * 则可以保证不丢失全部数据\n        \n        \n        \n* 持久化策略的选择\n\t* 一般是两种都开启\n    \t* 只开启RDB，则会丢失很多数据\n        *  只开启AOF，通过AOF做的冷备数据恢复慢\n        \t* 且RDB是数据快照，更加健壮，可以避免AOF复杂的备份和恢复机制导致的bug\n    \n    * AOF用来保证数据不丢失，作为数据恢复第一选择\n    * RDB做不同程度的冷备，AOF文件丢失或损坏的话，可用RDB快速恢复数据","expandState":"collapse"},"children":[{"data":{"id":"bxe8q27sutc0","created":1569938421353,"text":"RDB","note":"* 对redis中的数据执行周期性的持久化\n\n* 触发RDB时\n\t* 将当时redis内存中的数据的完整快照保存下来到rdb文件中"},"children":[{"data":{"id":"bxe9lhnccnc0","created":1569940884233,"text":"优点","note":"1. RDB会生成多个数据文件\n\t* 每个数据文件代表某一时刻的redis数据\n    \t* 这种多个数据文件的方式，非常适合做冷备份\n    * 可以定时将文件发到云服务存储\n    \n    * AOF做冷备的话，就是间隔一定时间copy一份\n    \t* 相对RDB麻烦\n    \n    \n2. RDB对redis的性能影响非常小，可以让redis保持高性能\n\t* 备份时间间隔长\n    * redis主进程只需fork一个子进程，让子进程执行磁盘IO即可\n    \n    * 每次写RDB都是写内存\n    \t* 只是在一定的时候，才会将数据写到磁盘\n    * AOF每次都要写文件\n    \t* 虽然可以快速写到OS cache中，但是还是有一定的时间开销，速度比RDB慢\n  \n  \n3. 相对于AOF，基于RDC文件重启和恢复redis进程，会更快速\n\t* AOF存放的是指令日志\n    \t* 需要回放、执行所有的指令\n    * RDB就是一份数据文件\n    \t* 直接加载到内存即可\n\n\n"},"children":[]},{"data":{"id":"bxe9x9wt0zk0","created":1569941807761,"text":"缺点","note":"* 故障恢复的话，RDB丢数据会比较多\n\t* 备份时间间隔更长\n    \n    * 因此不适合做第一优先的恢复方案\n\n\n\n* RDB在每次fork子进程来执行rdb文件生成时\n\t* 如果文件特别大\n    \t* 可能会导致客户端提供的服务暂停数毫秒，甚至数秒\n        \n    * 因此文件生成的间隔不能太短（频繁）\n    \t* （原文不能太长，存疑）"},"children":[]}]},{"data":{"id":"bxe8q6en8080","created":1569938430474,"text":"AOF","note":"* 把每条命令都写到AOF文件中\n\t* 使用append-only（追加）的方式写入日志文件中\n    \n\t* 现代操作系统中，写文件不是直接写磁盘。\n    \t* 先写到OS cache\n        * 一定时间后，再从os cache到disk file\n        * 每隔1s，调用一次操作系统的fsync函数，强制将OS cache中的数据刷到磁盘\n        \n\n\n* redis重启时\n\t* 可以通过回访AOF日志中的写入指令来重新构建整个数据集\n    * 同时使用两种持久化策略\n    \t* redis会使用AOF来重新构建数据，因为AOF数据更完整\n        * 可以做到最多丢失1s\n\n\n* AOF文件过大问题\n\t* redis内存是有限的\n    \t* 一定时间，redis就会用淘汰树算法，将一部分数据清除\n        \n    * 但是AOF是存放每条命令的\n    \t* 因此会不断膨胀\n        * 达到一定程度，AOF做rewrite操作\n        \n    * rewrite\n    \t* 基于内存中的数据，构造一个更小的AOF文件\n        * 旧的大文件删了"},"children":[{"data":{"id":"bxenf7v0po80","created":1569979888283,"text":"优点","note":"1. 更好的保护数据不丢失\n\t* 一般AOF每隔1s，通过一个后台线程执行一次fsync\n    \t* 最多丢失1s（该1s是配置de）\n    \n    * 使用append-only模式写入\n    \t* 没有任何磁盘寻址开销\n        * 写入性能非常高\n        * 文件不容易破损，文件尾部破损也容易修复\n        \n    * AOF文件过大时，执行rewrite操作\n    \t* 几乎不影响客户端的读写\n        \n        * 对其中的指令进行压缩，创建出一份需要恢复的最小日志\n        \t* 创建日志时，老的日志文件照常写入\n            * 新的merge后的日志文件ready后，替换老的日志文件（应该就是有一个menge的操作）\n            \n    * AOF中记录的是指令，是人类可读的\n    \t* 可用于误删紧急恢复\n        * 例：使用flushall清空所有数据\n        \t* 此时只要还没发生rewrite\n            * 复制一份AOF文件，删除最后一条命令，将AOF文件放回去，重启。可恢复所有数据"},"children":[]},{"data":{"id":"bxenqtr1ye80","created":1569980797939,"text":"缺点","note":"* 由于记录的是日志\n\t* 同一份的数据，AOF的文件会更大\n\n\n* 开启AOF后，支持的写QBS会比RDB支持的写QBS低\n\t* 因为AOF一般会配置每秒fsync一次日志文件\n    \t* 当然每秒一次的fsync，性能还是很高的\n    \n    * 其实AOF可以支持一条数据都不丢\n    \t* fsync设置为每写入一条数据就执行一次\n        * 但是redis的写QBS大降\n        * 因此不能这样设置\n  \n* AOF 较为复杂，也因此更加脆弱，容易有bug\n\t* 以前曾经发生根据AOF回放，得到的数据跟之前不一致\n    \n5. 做数据恢复比较慢\n\t* 且做冷备的话不方便\n"},"children":[]}]}]},{"data":{"id":"bxeo9fg99080","created":1569982255730,"text":"redis cluster","note":"1. 以前redis搞多节点的话，需要借助中间件\n\t* codis、twemproxy等都是中间件\n    * 客户端读取中间件，中间件负责将数据分布式存储到多台机器的redis实例中\n    \n2. redis cluster\n\t* 原生支持的集群模式\n    \t* 自动将数据分片，每个master上放一部分数据\n    \n    * 可以支撑N个redis master node\n    \t* 每个master node都可以挂载多个slave node\n        \n    * 理论上可支持读写分离\n    \t* 但是实际上不用，横向扩容master即可\n        * 一定要做的话，较为复杂\n    \n    * 高可用\n    \t* master挂了，会自动将某个slave切换为master\n        * 不再需要手工搭建replicatin复制+主从架构+读写分离+哨兵集群的高可用\n        \n        \n","expandState":"collapse"},"children":[{"data":{"id":"bxeoyuegtkg0","created":1569984247377,"text":"对比replaicatipn+sentinal","note":"* 如果数据量不多（几个G）\n\t* 主要是承载高并发高性能场景\n    * 则使用replication\n    \t* 一个master，多个slave\n        * slave数量取决于读吞吐量\n        * 以及搭建一个sentinal集群\n\n* redis cluster\n\t* 针对海量数据\n    * 高并发+高可用"},"children":[]},{"data":{"id":"bxepkkn8nsg0","created":1569985950152,"text":"数据分布算法","note":"* 分布式数据储存核心算法\n\t* 使用不同的算法，决定在多个master节点上，数据如何分布到这些节点上\n    \n\n        \n","expandState":"collapse"},"children":[{"data":{"id":"bxewodfvog00","created":1570005995695,"text":"hash算法","note":"* 已经很少在缓存领域用了\n\n* 计算key的hashcode，对节点数量进行取模\n    * 当然取模有很多种方式，余数或者位运算等（取模本身的意思就是获取余数）\n    \n* 其中一个节点宕机了\n    * 导致该节点数据全部失效\n    \n    * 需要对剩下的机器重新取模，再重新分配\n    * 由于节点变少，hash算法算出来的机器可能不是之前的有数据的机器，\n    \t* 相当于整个redis缓存大部分数据相当于没有了\n        * 因此就会直达数据库\n    \n\n* 高可用场景下，不可接受\n    * 失效的部分直接走数据库，数据库压力大"},"children":[]},{"data":{"id":"bxewpwyi0gg0","created":1570006116544,"text":"一致性hash算法","note":"* 一致性hash算法（自动缓存迁移）+虚拟节点（自动负载均衡）\n    * memcached 会用\n    \n    \n* 使用一个hash环，将机器放在圆环上（的某个点）\n    * 有一个key过来的话，计算其hash值\n    * 将hash值与圆环上的各个点做对比（每个点都有hash值），\n    \t* 看hash值 落在圆环的哪个部位\n        * key落在圆环上后，顺时针旋转去寻找距离自己最近的一个节点\n   \n   \n   \n    \n* 自动缓存迁移\n    * 通过圆环的查找，即使挂了一个节点 \n    \t* 也可以顺时针到达下一个节点\n\n\n* 相比hash算法，不会导致大部分数据失效\n\t* 只是挂掉的节点上的数据有影响\n    \t* 这些数据走数据库，并记录在下一个节点上\n   \n   \n*  数据负载均衡\n\t* 每个节点可以增加虚拟节点\n    \t* 让每个机器的节点均匀散落在环上\n    *  一旦某台机器挂了，由于虚拟节点的存在\n    \t* 负载会相对均匀的散落在各个机器上\n    \n    * 同时可以解决缓存热点问题\n    \t* 即某个区间的hash值对应的数据特别多\n        * 使用虚拟节点可以分散数据\n   "},"children":[]},{"data":{"id":"bxexh3s528o0","created":1570008247230,"text":"hash slot算法","note":"* redis cluster 使用这种算法\n\t\n    * redis cluster有固定的16384个hash slot（hash槽）\n    \t* 每个master持有部分hash slot\n    \n    * 会对每个key计算crc16值\n    \t* 再对16384取模，获取对应的hash slot\n        \n    * 新增、减少master节点\n    \t* 增加节点，让其他master的hash slot移动部分过去\n        * 减少节点，让它的hash slot移动到其他master上去\n        * 移动hash slot的成本是非常低的\n    \n\n* 跟一致性hash的效果差不多\n\n* 客户端可以对指定的数据\n\t* 让他们走同一个hash slot\n    \t* 通过hash tag手动指定key对应的slot\n        * 同一个hash tag下的key，都会在一个hash slot中\n        * 例： \n        \t* set key1：{100} 和 set key2：{100}\n            * 100是hash tag，则相同的tag的数据会录入到相同的slot"},"children":[]}]},{"data":{"id":"bxexwmyxwsg0","created":1570009464462,"text":"内部通信机制","note":"* 通信\n\t* redis cluster架构下。每个redis要开放2个端口\n    \t* 如一个是6379\n        * 另一个就是加上10000的数，即16379\n        \n    * 两外一个端口 用于节点间的通信\n    \t* cluster bus用的（集群总线\n    \t* 用于故障检测，配置更新，故障转移授权\n    * cluster bus用了一种二进制的协议\n    \t* 可以在节点间进行高效数据交换，并占用更少的网络带宽和处理时间\n\n* 每个节点每隔一段时间，都会向另外几个节点但发送ping消息\n\t* 其他节点接收到之后会返回pong\n\n* 使用gossip协议进行通信\n\t* 维护集群的元数据用的\n    \t* 集群之间有很多元数据\n        * 如\n        \t* ghashslot与node之间的映射表关系\n            * master与slave之间的关系\n            * 故障的信息等\n            * 节点的新增、删除\n            * hash slot等信息\n        \n","expandState":"collapse"},"children":[{"data":{"id":"bxeyk7ogsts0","created":1570011311917,"text":"gossip协议","note":"* gossip\n    * 每个master节点都保存一份完整的元数据\n    * 有变更的话，会把变更数据发给其他的master，其他master更新数据\n    * 优点\n    \t* 元数据的更新比较分散，更新请求会陆陆续续打到各个节点上，有一定的延时，降低了压力\n        \n    * 缺点\n    \t* 元数据更新有延迟，可能导致集群的一些操作会有一些滞后\n        \n        \n* 集中式的\n\t* 除了gossip之外，还有一种叫集中式\n    \t* 集中式就是集中在一个地方进行维护、储存，如使用zookeeper\n        \n        * 大数据的strom就是使用了集中式的元数据储存架构，底层基于zookeeper集群对元数据\n        \n    * 集中式的优点\n    \t* 元数据的更新、读取，时效性非常好\n        * 一旦元数据出现变更，立即更新到集中式存储中\n        * 其他节点读取时可以立即感知到\n    * 缺点\n    \t* 元数据的更新压力集中在一个地方，可能导致元数据的存储有压力"},"children":[]},{"data":{"id":"bxeze3oy1400","created":1570013654164,"text":"gossip协议的消息","note":"1. gossip协议包含的多种消息\n\t* meet\n    \t* 某个节点给新加入的节点发送meet\n        * 让新节点加入集群\n        * 然后新节点就会开始与其他节点进行通信\n    \n    * ping\n    \t* 每个节点每秒都会频繁给其他节点发送ping\n        * 其中包含自己的状态还有自己维护的集群元数据\n        * 互相通过pinng交换元数据以及对元数据的更新\n        \n    * pong\n    \t* 接到ping或者meet后返回pong\n        * 包含自己的状态及其他信息\n        * 也可以用于信息广播及更新\n    \n    * fail\n    \t* 某个节点判断另外一个节点fail之后，就会发送fail给其他节点\n        * 通知其他节点：某个节点宕机了\n\n*  ping\n\t* 每个节点每秒会执行10次ping\n    \t* 而且要携带一些元数据，因此可能会增加网络负担\n        * 每次会选择5个最久没有通信的其他节点\n        \n    * 如果发现与某个节点的通信延迟达到cluster_node_timeout/2\n    \t* 即已经这么久没有通信\n        * 立即发送ping，避免数据交换延迟较长\n        * cluster_node_timeout调长一点的话，可以降低发送的频率\n    \n    * 每次ping带上自己的数据\n    \t* 以及带上整个集群的1/10的节点信息，发送出去进行出去进行数据交换\n    * 至少包含3个其他节点信息，做多包含总节点-2的其他节点信息 "},"children":[]}]},{"data":{"id":"bxezujx9sgw0","created":1570014943322,"text":"jedis","note":"* jedis\n\t* 面型集群的redis的java client\n    \n* jedis与集群交互的一些原理\n\t* 请求重定向\n    \t* 客户端可能会挑选任意一个redis实例发送命令\n        * 每个redis实例收到命令后，会计算ke对应的hash slot\n        \t* 在本地的话就在本地处理\n            * 否则返回moved给客户端，让客户端进行重定向（moved错误）\n            \n    * cluster keyslot mykey\n    \t* 可以查看一个key对应的hash slot\n   \n   \t* 使用redis-cli时，加上-c 参数\n    \t* 支持自动的请求重定向\n        * redis-cli接收到moved后，会自动重定向对应的节点执行命令\n        \n    * hash slot查找\n    \t* 节点间通过goosip协议进行数据交换，就知道每个hash slot在哪个节点上\n\n\n        \n        ","expandState":"collapse"},"children":[{"data":{"id":"bxf0xzkjl000","created":1570018033584,"text":"smart jedis","note":"1. 基于重定向的客户端\n\t* 很消耗网络IO\n    \t* 因为大部分情况下，可能都会出现一次请求重定向，才能找到正确的节点\n    * 因此大部分客户端都是smart的，包括jedis\n    \n* 原理\n\t* 客户端本地维护一份hash slot与node的映射的缓存\n    \t* 大部分情况下，直接走本地缓存据可以找到hash slot对应的node在哪\n        * 不再需要通过节点的moved重定向\n    \n"},"children":[]},{"data":{"id":"bxf1fpfusyo0","created":1570019422087,"text":"JedisCluster工作原理","note":"* JedisCluster工作原理\n\t* JedisCluster初始化时，随机选择一个node\n    \t* 初始化hash slot->noded的映射表（因为这些映射元数据在每个节点都有）\n        * 并为每个节点创建一个jedispool连接池\n        \n    * 每次基于jediscluster执行操作时\n    \t* 先计算出key对应的hashslot，并在映射表中找到对应的节点\n        \t* 如果该节点依旧持有该hash slot，则ok\n        \t* 如果进行了reshard（猜测是重新hash之类）导致hashslot不在原node上，返回 moved\n            \n        * 返回moved的话，jedisCluster就会利用该节点的元数据，更新本地的hash slot与node的映射缓存\n        \t* 重复操作，直至找到节点，超过5次报错：JedisClusterMaxREtirectionException\n\n* 如果hash slot正在迁移\n\t* 会返回ask重定向给jedis\n    \n    * jedis接到后，会重新定位到目标节点去执行\n    \t* 但是由于ask发生在hash slot迁移过程中\n        * 因此此时不会去更新hashslot本地映射缓存\n        \n    * 如果时moved的话，会更新缓存\n    \t* 因为此时已经确定hashslot已经迁移完"},"children":[]}]},{"data":{"id":"bxf1kyoxiug0","created":1570019834048,"text":"高可用和主备切换原理","note":"1. 判断节点宕机\n\t* 一个节点认为另外一个节点宕机\n    \t* 是pfail 主观宕机\n    * 多个节点认为另外一个节点宕机\n    \t* fail 客观宕机\n        * 跟哨兵原理几乎一致\n        \n    * 在cluster-node-timeout内，某个节点一直没有返回pong\n    \t* 被认为是pfail\n    * 一个节点认为某个节点pfail\n    \t* 会在goosip消息中，ping给其他节点\n        * 超过半数的节点认为pfail，就会变成fail\n        \n        \n2. master宕机后，就要选择一个从节点作为新的master\n\t* 从节点过滤\n    \t* 检查每个salve node与master node的断开连接时间\n        * 超过了cluster-node-timeout * cluster-slave-validity-factor。则没有资格\n        \n    * 从节点选举\n    \t* 每个从节点，根据自己对master复制数据的offset，设置一个选举时间\n        * offset越大，选举时间越靠前，优先进行选举\n        \n        * 所有master开始对slave投票。如果大部分master node（N/2 +1） 都投票给了某个从节点，则选举通过\n        \n        * 从节点执行主备切换，变成master\n        \n* 整个流程跟哨兵类似"},"children":[]},{"data":{"id":"bxfpfvpqo2w0","created":1570087142383,"text":"生产环境部署架构","note":"* 十台机器\n\t* 5主5从\n    \t* 每个主实例挂一个从实例\n        * 因此是高可用的，任何一个主实例宕机，都会故障迁移，从实例变master\n        \n        \n    * 5个节点对外提供读写服务\n    \t* 每个节点读写高峰QBS\n        * 5w/sec（可以说是压测的数据）\n    \n    * 机器配置\n    \t* 32G内存+8核CPU+1T磁盘\n        * 分配给redis的内存是10G（一般不能超过这个值）\n        \n\n* 往内存中写的什么数据\n\t* 每条数据的大小\n    * 以此算出使用了多少内存\n    \t* 常驻内存等\n        \n* 日常高峰\n\t* 3500/sec请求量\n    * 这些请求一般访问哪些接口？\n    \t* 提前准备。。。\n    "},"children":[]}]},{"data":{"id":"bxf209iqec00","created":1570021033080,"text":"缓存雪崩，缓存穿透","note":"1. 缓存雪崩\n\t* 整个缓存挂了，请求直接压在数据库。\n    * 雪崩，一般特指很多key到期。大量请求直接访问数据库，压死数据库\n    \n\t* 事前\n    \t* key的设置过期时间要隔开，不要太集中\n    \t* redis要是高可用，避免全盘崩溃：主从+哨兵 或者redis cluster\n        \n        * 系统内增加ehcache（有个系统内的小缓存）也算属于事中\n        \t* 处理请求可以先查看ehcache，没有再访问redis，再没有再查数据库\n            * 查到的数据放入ehcache和redis\n    \n    * 事中\n    \t* 有了ehcahe后，还得配合做限流+降级\n        * 使用hystrix框架实现\n        \n        * 可进行配置，每秒能通过限流组件的访问数\n        \t* 没法通过限流组件的请求，会调用你自己开发好的限流组件\n            * 一般是返回一些默认的值，如：友情提示等\n        \n        * 可以保证数据库不挂\n        \t* 则一部分请求还是可以被处理的\n     \n    * 事后\n    \t* redis做持久化\n        * 可以快速恢复\n\n* 缓存穿透\n\t* 解决：数据库中没有的值，也写到缓存\n    \t* 值是约定好的，代表不存在的值即可\n      \t* 则请求不会大量穿透到数据库"},"children":[]},{"data":{"id":"bxf2q7akdvs0","created":1570023065701,"text":"保证缓存与数据库的双写一致","note":"* 经典的缓存+数据库读写的模式\n\t* cache aside pattren \n    \n    * 读数据时，先读缓存\n    \t* 缓存没有数据，再读数据库\n        * 然后将数据放入缓存\n    \n    * 更新的时候，先更新数据库，再删除缓存\n    \t* 作者说的先缓存应该是记错了\n   \n   \n\n","expandState":"collapse"},"children":[{"data":{"id":"bxfk10i5c9c0","created":1570071871796,"text":"为什么是删除缓存，不是更新缓存","note":"   \n* 为什么是删除缓存而不是更新缓存\n\t* 因为缓存的数据可能需要经过复杂的计算\n    \t* 更新之后，不保证在下次更新前有请求来读取\n        * 这样就浪费一次计算了，复杂的计算的代价就更大\n        * 删除的话，体现一个lazy懒加载的思想。让它需要用到的时候再计算"},"children":[]},{"data":{"id":"bxfk1baz4o00","created":1570071895307,"text":"先更新数据库","note":" \n* 先更新数据库据，后删除缓存\n\t* [缓存删除失败]怎么办\n    \t* 没有任何处理的话，这样就导致了缓存数据是旧的\n        \n    * 此时其实是不能用事务控制的\n    \t* 假如事务在确认删除缓存成功才提交\n    \t* 则在删除成功，提交事务之前，还是有可能有其他读线程拿到了旧的数据\n    \n    * 使用MQ\n    \t* 将key放进消息队列，不断尝试删除，直至成功"},"children":[]},{"data":{"id":"bxfktlej83c0","created":1570074111487,"text":"先删除缓存","note":"* 先删除缓存的话（是可行的）\n\t* 在更新数据库，并提交事务之前\n    \t* 可能有其他查询获取了旧的数据，并放进缓存\n        * 则缓存的数据是旧的，数据库是新的，导致双写不一致\n    \n\t* 另外先删除缓存的好处，是即使更新数据库失败（不是主要问题）\n    \t* 再次获取缓存，还是数据库的内容，所以没有导致不一致\n\n\n\n* 解决\n    * 将 [删缓存+修改数据库]的操作以及[读数据库，写到缓存]的操作，进行串行化\n    \t* 全部放进一个队列中，并用一个线程进行逐个处理\n        * 队列（需要支持分布式，或者使用nginx hash路由到同一台机器）\n        * 使用内存队列，还有热点商品的请求倾斜的问题\n        \n    * 则在删除完数据库，修改成功之前，所有读数据库的请求将会排队等待\n        * 解决了不一致问题\n     \n* 优化\n\t* 根据数据的唯一id，做hash\n    \t* 分给多个队列处理，减少锁的粒度\n     \n    * 多个读请求排队从数据库获取数据的地方\n    \t* 可判断队列前面假如已经有一个相同的读请求\n        * 则等待几十毫秒，看看缓存中是否有\n        \t* 有就获取缓存返回\n            * 没有再次等待，超过N次就直接读数据库的旧值返回（不放缓存）\n\n"},"children":[{"data":{"id":"bxflpqtxdxk0","created":1570076630955,"text":"注意","note":"* 注意\n\t* 读请求发现缓存没数据（需保证在超时时间内完成）\n    \t* 不能直接等待\n        * 要检查队列中有更新操作才行\n    \n    * 写操作不能太频繁\n    \t* 否则会导致大量的读操作长期阻塞+最终走数据库\n        \n    * 因此压测是很重要的 "},"children":[]}]}]},{"data":{"id":"bxfoz668j8w0","created":1570085832958,"text":"redis并发竞争问题","note":"* 使用分布式锁解决\n\t* 保证同一时间只有一个请求在操作\n    \n* 如果并发更新缓存时，有一些时序要求\n\t* 即旧数据可能会覆盖新数据\n    \t* 则加版本号，如时间戳等\n        * 暂时没想到使用场景。。\n        \n*  如最新版本是经过计算的，旧版的数据完全 没用\n\t* 则直接根据时间戳判断，缓存里面的数据是不是比当前要更新的更新\n    \t* 是的话就放弃本次操作"},"children":[]}]},{"data":{"id":"bxfqow4hluw0","created":1570090669663,"text":"分布式系统dubbo"},"children":[{"data":{"id":"bxfrhnzgkts0","created":1570092924505,"text":"为什么使用分布式框架Dubbo","note":"* 为什么要拆分系统\n\t* 系统太大，多人开发维护。\n    \t* 代码冲突，合并等\n        * 不能单独进行技术升级等\n        \n    * 例：原好易借借贷产品，与理财、风控、账务等各个模块共用一份代码\n    \t* 使用非常痛苦\n\n* 怎么拆\n\t* 按照开发组、业务拆即可\n    \t* 后续继续可拆为存管相关，授信相关等"},"children":[]},{"data":{"id":"bxfu91ht66w0","created":1570100711965,"text":"原理","expandState":"collapse","note":null},"children":[{"data":{"id":"bxfsswbogps0","created":1570096625774,"text":"分层","expandState":"collapse"},"children":[{"data":{"id":"bxfst38f6tc0","created":1570096640814,"text":"servicer层","note":"* servicer层，接口层\n\t* provider、comsumer接口\n    \t* 给服务提供者、消费者 实现 \n\t\t* 具体接口调用的业务实现"},"children":[]},{"data":{"id":"bxft41btjq80","created":1570097498672,"text":"config 层","note":"* 配置层\n\t* 对Dubbo进行各种配置"},"children":[]},{"data":{"id":"bxft5378nko0","created":1570097581113,"text":"proxy层","note":"* 服务代理层\n\t* 无论是provider还是comsumer\n    \t* 都会生成代理\n        * 代理之间进行网络通信\n        \n\t* 透明生成客户端的stub和服务单的skeleton\n    \t* 待了解。。\n        \n    * 服务提供者的代理监听网络请求，消费者发起请求等"},"children":[]},{"data":{"id":"bxftafa6vjs0","created":1570097999233,"text":"registry层","note":"* 服务注册层\n\t* 负责服务的注册以及发现\n    \t* provider注册自己作为一个服务\n        * comsumer去注册中心寻找自己调用的服务"},"children":[]},{"data":{"id":"bxftrui8crk0","created":1570099364562,"text":"cluster层","note":"* provider可以部署在多台机器上\n\t* 多个provider组成一个集群\n    \n* 封装多个服务提供者的路由\n\t* 并做负载均衡\n    * 将多个实例组合成一个服务"},"children":[]},{"data":{"id":"bxftu2om9480","created":1570099539091,"text":"monitor层","note":"* 监控层\n\t* consumer调用provider，对调用次数，调用时间等统计信息进行监控"},"children":[]},{"data":{"id":"bxfu3sejvs00","created":1570100300356,"text":"protocol层","note":"* 远程调用层\n\t* 封装rpc调用\n    * 负责具体的provider和comsumer之间调用接口的网络通信"},"children":[]},{"data":{"id":"bxfu5m9l1hk0","created":1570100443723,"text":"exchange层","note":"* 信息交换层\n\t* 信息交换\n    * 封装请求响应模式\n    \t* 同步转异步"},"children":[]},{"data":{"id":"bxfu7r1yl2o0","created":1570100610875,"text":"transport层","note":"* 网络传输层\n\t* 抽象mina和netty为统一接口"},"children":[]},{"data":{"id":"bxfu6pmbaow0","created":1570100529388,"text":"serialize层","note":"* 数据序列化层"},"children":[]}]},{"data":{"id":"bxfvibw4gxk0","created":1570104260986,"text":"工作流程（注册）","note":"1. 启动时，服务提供者和消费者与注册中心交互\n\t* 服提供者注册其提供的服务\n\t* 服务消费者获取服务提供者地址列表（缓存到本地）\n    \t* 调用时根据负载算法直接调用提供者\n        \n        \n2. 注册中心通过长连接感知服务提供者的存在\n\t* 服务提供者宕机，注册中心将立即推送事件通知消费者\n    \t* 消费者刷新本地对服务注册信息的缓存\n        \n3. comsumer 和 provider都异步的通知监控中心\n\n\n4. 注册中心挂了\n\t* 可以继续通信\n    * 短期内不影响\n    \t* 启动dubbo时，消费者会从zk拉取注册的生产者的地址接口等数据，缓存在本地。\n        * 每次调用时，按照本地存储的地址进行调用\n        \n\t* 倒是无法从注册中心去同步最新的服务列表\n    \t* 短期的注册中心挂掉是不要紧的，但一定要尽快修复"},"children":[]},{"data":{"id":"bxfvqjq1b200","created":1570104904945,"text":"网络通信协议","note":"* 通信协议\n\t* 一般就是请求时加在地址前的\n    \t* 如： http://  dubbo://\n        \n        \n* dubbo支持的网络通信协议\n\n1. Dubbo协议\n    * 默认的协议\n    \n    * 单一长连接，NIO异步通信\n        * 每个消费者与每个生产者（有服务调用的）建立一个长期保持的连接\n        \t* 基于这个连接不断发送请求\n        * NIO异步通信，性能很高\n            \n    * 基于hessian作为序列化协议\n        * 序列化就是将数据序列化为一些可以通过网络传输格式（因为此处用于网络传输）\n        \t* 如二进制等\n        * 接收到之后再反序列化\n        \n    * 适用场景\n    \t* 传输的数据量少(100k内)，但是并发量高\n        \t* 长连接可以避免经常创建、销毁连接\n            * 但是如果数据量太大的话，会导致并发能力降低\n     ","expandState":"expand"},"children":[{"data":{"id":"bxfw7mjgggw0","created":1570106243268,"text":"其他协议","note":"* 其他的协议用的都不多\n\t* 有道详细笔记\n\n* rmi协议（一般少用）\n\t* Java标准二进制序列化\n    * 多个短连接\n    * 适合场景\n    \t* 提供者跟消费者差不多\n    \t* 文件传输\n        \n* hessian协议（一般少用）\n\t* Hessian二进制序列化\n    * 多个短连接\n     * 适合场景\n    \t* 提供者比消费者还多\n    \t* 文件传输\n        \n* http协议\n\t* 走json序列化\n    \n*  webservice\n\t* SOAP文本序列化"},"children":[]}]},{"data":{"id":"bxfwdeuj8o80","created":1570106696709,"text":"序列化协议","note":"* dubbo基于不同的通信协议\n\t* 支持hessian、java二进制序列化、json、SOAP文本序列化等多种序列化协议\n    \t* hessian时默认的序列化协议"},"children":[]}]},{"data":{"id":"bxfwlk5t0kw0","created":1570107335188,"text":"负载均衡策略","note":"* 负载均衡 loadbalance\n\n1. random 随机\n\t* 默认的策略（一般就用这个即可）\n    \t* 正常每台机器接收的请求数差不多\n    * 另外可以对provider设置不同的权重\n    \t* 权重越大，随机的概率大一点\n         \n         \n2. roundrobin\n\t* 轮询\n    \t* 均匀的将流量打到各个机器上\n    \t* 性能有差异的话可以设置权重\n\n      \n3. leastactive\n\t* 最少活跃。 自动感知一下\n    \t* 如果某个机器性能较差，能处理的请求较少，不是很活跃\n        * 则分配更少的请求\n  \n  \n4. consistant hash\n\t* 一致性hash算法\n    \t* 相同参数的请求一定分发到一个provider\n        * provider挂了的话，会基于虚拟节点，均匀分配剩余的流量，抖动不会太大\n        \n    * 有些用处\n    \t* 如某些id的数据，必须分发到指定的机器\n        * 就用这种策略"},"children":[]},{"data":{"id":"bxfx7dtlek80","created":1570109045400,"text":"集群容错策略","note":"* 请求某一台机器时\n\t* 发现机器挂了（经过多次重试）\n    * 将请求发到其他机器\n    \n* failover （默认）\n\t* 失败自动切换\n    \t* 自动重试其他机器\n    \t* 常见于读操作\n    \n* failfast\n\t* 一次调用失败后立即报错\n    \t* 常见于写操作\n\n* failsafe\n\t* 出现异常时忽略掉\n    \t* 常用于不重要的接口调用，如记录日志（失败的话也是打一下日志这样）\n  \n* failback\n\t* 失败了后台自动记录请求\n    \t* 定时重发\n        * 适合写消息队列这种\n\n* forking\n\t* 并行调用多个provider\n    \t* 只要一个成功就立即返回\n        * 其他慢的就不要了\n \n* broadcacst\n\t* 广播调用所有provider\n    \t* 任意一个报错就报错\n        * 常用于通知所有provider更新缓存或日志等本地资源信息\n  \n"},"children":[]},{"data":{"id":"bxfxtpul31s0","created":1570110795593,"text":"动态代理策略","note":"* 默认使用javassist动态字节码生成，创建代理类\n\t* 可以通过spi扩展机制配置自己的动态代理策略（待了解。。）"},"children":[]},{"data":{"id":"bxfxwiipiso0","created":1570111014730,"text":"spi思想","note":"* service provider interface\n\t* 服务提供接口\n    \t* 一个接口有多个实现类\n        * 可以配置使用指定的实现类\n        \t* 系统根据配置，加载对应的实现类，使用此类的实例对象\n        \n        * 可用于插件扩展等场景，如java中的JDBC也是类似思想（但是JDBC不是SPI机制）\n        \n    \n    \n* JDK中提供了这样一个功能\n    * 在自己jar包的META—INF/services/目录下，放一个跟接口同名的文件\n    \t* 里面指定接口的实现，是自己这个jar包里的某个类\n      \n      \n    \n\n            ","expandState":"expand"},"children":[{"data":{"id":"bxfyjtf6am00","created":1570112840837,"text":"dubbo中的使用","note":"* dubbo中\n    * 如系统运行时，dubbo需要判断使用Protocol接口的那个实现类取实例对象\n    \n    * Protocol prococol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension()\n    \t* 执行上面代码，就会去找对应的配置\n        * 获取实现类，实例化对象使用\n        \n    * 微内核，可插拔，大量的组件\n    \t* 可使用自己的组件实现功能，如RPC\n        \n* 实现\n\t* 接口上加上注解\n    \t* @SPI(\"dubbo\") 表示通过SPI机制来提供实现类\n        * 实现类通过dubbo作为默认的key去配置文件里找\n        * 配置文件名于接口全名一致\n\t* 在/META_INF/dubbo/internal/com.alibaba.dubbo.rpc.Protocal 文件中\n    \t* 里面就是key=类全名的写法\n        * 如dubbo = com.ali...Protocol\n\n* 动态替换默认的实现类\n\t* 该接口（加了spi注解的）中\n    \t* 有@Adaptive注解的方法\n        \n        * 运行时会生成代理类，代理类中 有 此带有@Adaptive注解的方法\n        \n        * 代理类会根据传进来的key，去找对应的实现类，没的话就是默认值dubbo了"},"children":[]}]}]}]},"template":"right","theme":"fresh-green","version":"1.4.43"}