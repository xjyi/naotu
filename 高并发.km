{"root":{"data":{"id":"bx8b4ie4xs00","created":1569335836142,"text":"高并发"},"children":[{"data":{"id":"bx8b4r2ulns0","created":1569335855051,"text":"MQ 消息队列","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"bx8b8mdmtt40","created":1569336158276,"text":"为什么要使用MQ","note":"* 业务场景遇到什么技术挑战\n\t* 如果不用会很麻烦\n    \t* 但是用MQ就可以很好的处理\n        \n    * 适合场景\n    \t* 解耦\n        * 异步\n        * 削峰\n \n* 解耦\n\t* 系统对接多个系统\n    \t* 多套接口\n        \n    * 使用MQ，需要消息的就去获取\n    \t* 生产者消费者解耦\n        \n    * 场景\n    \t* 待补充\n\n\n* 异步\n\t* 一般互联网接口要求响应时间200ms以内\n    \t* 一个接口如果调用其他多个系统\n        * 总耗时可能长达1s，则性能太差\n    * 此时可以用MQ做异步处理\n    \t* 发送到MQ的时长很短，不需要等系统响应\n        \n        \n* 削峰\n\t* 5000QBS属于高并发（约100万用户）\n    \t* mysql一般能扛到2000个请求每秒\n        * 一万用户在线，50qbs也是正常的\n   \n   * 用户请求到达系统前，先用mq存储","layout":null},"children":[]},{"data":{"id":"bx95923cvjs0","created":1569420825780,"text":"优缺点","note":"* 优点\n\t* 见“为什么使用”\n    \n* 缺点\n\t* 系统可用性降低\n    \t* MQ故障\n        * 解决:钉钉预警\n    \n    * 系统复杂性提高\n    \t* 数据发送了多次\n        * 解决：接口的幂等性（请求流水号）\n        \n        \n        * 消息丢了/顺序乱了\n    \n    * 一致性问题\n    \t* 某一个MQ执行失败\n    \n","layout":null},"children":[]},{"data":{"id":"bx8bdi39oq80","created":1569336540763,"text":"技术选型","note":"* 中小型公司用rabbitMQ\n\n* 大公司用rocketMQ\n\t* 性能各方面做的很好\n    * 大公司有能力维护rocket源码\n    \n\n* 大数据领域（实时计算）、数据采集\n\t* kafaka是业内标准，规范","expandState":"collapse","layout":null},"children":[{"data":{"id":"bx95styjopc0","created":1569422375358,"text":"activeMQ","note":"1. activeMQ\n\t* 单机吞吐量 万级\n    \t* 每秒处理数据量\n    * 时效性\n    \t* 毫秒ms\n    * 可用性 高\n    \t* 基于主从架构\n    * 消息可靠性\n    \t* 较低概率丢失数据\n        \n \n* 总结\n\t* 技术成熟\n    \t* 出了很久了（以前mq技术首选）\n        \n    * 缺点\n    \t* 有较低概率丢数据\n      \t* 官方社区不活跃\n        * 较少用在大规模吞吐的场景","layout":null},"children":[]},{"data":{"id":"bx95t04j1p40","created":1569422388781,"text":"rabbitMQ","note":"2. rabbitMQ\n\t* 单机吞吐量 万级\n    * 时效性（最大优点，低延迟）\n    \t* 微秒\n    * 可用性 高\n    \t* 基于主从架构\n        \n\n* 总结\n\t* 基于erlang开发\n    \t* 并发能力强，性能好，延时低\n        * 即消息在mq内部处理的时间少\n        * 此优点不明显\n        * 源码不好阅读\n    * 管理界面友好，中小型公司多使用\n    * 社区相对活跃","layout":null},"children":[]},{"data":{"id":"bx95t5gijco0","created":1569422400389,"text":"rocketMQ","note":"3. rocketMQ\n\t* 单机吞吐量 十万级\n    * 时效性\n    \t* 毫秒ms\n    * 可用性非常高\n    \t* 分布式架构\n\n* 总结\n\t* topic 增加到几百上千时\n    \t* 吞吐量有小幅下降\n    * 阿里开源，品质保障\n    \t* java语言\n    * 社区活跃度尚可\n    \n    * 公司假如不再维护，会有较大坏影响","layout":null},"children":[]},{"data":{"id":"bx95tdzbojk0","created":1569422418941,"text":"kafka","note":"\n4. kafka\n\t* 单机吞吐量 十万级（最大优点，吞吐量高）\n    \t* 一般配合大数据类的系统进行实时数据计算，日志采集\n        \n    * 时效性\n    \t* 毫秒ms\n        \n    * 可用性非常高\n    \t* 分布式架构，一个数据多个副本\n        * 少数机器宕机，不丢失数据，不会导致不可用\n        \n* 总结\n\t* topic较多时（上百）\n    \t* 性能下降较快\n        \n    * 只能支持简单的MQ功能\n    \t","layout":null},"children":[]}]},{"data":{"id":"bx9zoi4swdc0","created":1569506669456,"text":"如何 保证消息队列的高可用","note":null,"expandState":"collapse","layout":null},"children":[{"data":{"id":"bxa0caftac80","created":1569508533447,"text":"rabbitMQ","note":"* rabbitMQ\n\t* 非分布式\n    * 有集群\n    \t* 单机模式\n        * 普通集群模式\n        * [镜像集群模式]\n\n* 开启镜像集群模式\n\t* 管理后台新增一个策略\n    \t* 即该策略\n    * 可以要求数据同步到所有节点\n    \t* 也可以要求同步到指定数量节点\n        \n* 普通集群\n\t* 创建的 每一个队列，只会保存在其中一个节点（机器上）\n    \t* 该节点包含了元数据、实际的数据\n        * 元数据即是一些配置信息，如结构、名称等\n        * 其他节点上保存的是队列的元数据，以及有实际数据的节点位置\n    \n    \n    * 消费者去到没有数据的节点拿数据，则该节点会去有数据的节点拉取\n    \t* 可能导致mq内部大量数据传输\n    \n    \n    * 可以提高消费吞吐量\n    \t* 可用性几乎没有保障\n        * 存数据的节点宕机，则无法消费\n        \n* 镜像集群模式\n\t* 支持高可用\n    \t* 一个节点挂了，其他节点还有数据\n    * 队列的元数据、数据在每个节点上都有\n    \t* 写入数据的节点会对其他节点进行同步\n    * 非分布式\n    \t* 数据量达到单机承受布不了的话会有问题","layout":null},"children":[]},{"data":{"id":"bxa0hwtc5co0","created":1569508973975,"text":"kafka","note":"* 分布式\n\t* 每台机器（节点）有一个broker进程\n    \t* 每台机器+机器上的broker进程，就是kafka集群中的一个基点\n    \n    * 创建一个topic\n    \t* topic会被分成多个partition（每个只有一部分数据）\n        * 不同partition可以存在不同的broker上\n        * 假如没有HA机制，一个节点挂了，数据就会丢失一部分\n\n\n\n* topic\n\t* 存储消息的一个逻辑概念\n    \t* 可认为是消息集合\n\n\n* HA机制（高可用机制）\n\t* kafka 0.8之后，提供了高可用机制\n    * replia副本机制\n    \t* 每个partition数据会同步到其他机器上，形成多个replia副本\n        \n        * 副本间会选举出一个leader，其他的为follower\n        \n        * 生产、消费都是跟leader打交道（其他不行），leader会同步数据到其他副本\n        \n        * 生产消费都是先全部同步成功，在返回ack给生产、消费者\n        \n    * 则一个节点挂了，数据还在，还能用\n    \t* 自动感知leader挂了，选举出新的leader","layout":null},"children":[]}]},{"data":{"id":"bxb99ohnf340","created":1569635278887,"text":"如何保证消费消息时的幂等性","note":"* 即防止重复消费\n\t* 加请求流水号\n    \t* 放进redis，使用setNX处理\n        * 提交事务之后删除\n        * 请求全部先检查缓存，再检查数据库 \n    * 数据库主键唯一限制\n\n* MQ是不保证消息只发一次的\n\n\n\n* kafka中\n\t* 每条消息都有一个offset\n    \t* 代表了消息的顺序的序号\n        * kafka根据消息进入的顺序进行分配的\n     \n    * 消费者从kafka消费的时候，是按照顺序去消费的 \n    \t* 消费者消费完会提交offset，即通知kafka已经消费该条消息\n        \n        * 基于zookeeper实现\n        * 提交offset不是实时的，是定时提交一次（所以已经消费了的，可能没提交offsset就重启了）\n        \n    * zookeeper会记录消费者当前消费到offset等于几的消息\n    \t* kafka能感知到该offset\n     ","layout":null},"children":[]},{"data":{"id":"bxba90ie7000","created":1569638047799,"text":"如何保证消息的可靠性传输","layout":null,"note":"* 如何处理消息丢失问题\n\n\n\t","expandState":"collapse"},"children":[{"data":{"id":"bxbbp5r99kg0","created":1569642134155,"text":"生产者到MQ","expandState":"expand","layout":null},"children":[{"data":{"id":"bxbbghbai0g0","created":1569641454034,"text":"事务机制","note":"* rabbitMQ 支持事务\n\t* channel.txSelect()声明启动事务模式\n    * channel.txComment()提交事务\n    * channel.txRollback()回滚事务\n    \n    * 事务中发生异常可先回滚，再次发送消息\n    \n\n* 特点\n\t* 事务机制，是同步的\n    \t* 生产者发送消息会阻塞（等待响应结果）\n        * 影响吞吐量","layout":null},"children":[]},{"data":{"id":"bxbbj1vo5n40","created":1569641655530,"text":"确认机制","note":"* rabbitmq channel设置成confirm机制\n\t* 再发送消息\n    \n    * rabbitmq接收到消息之后\n    \t* 回调生产者，通知已经接收成功\n    * rabbitMq接收消息报错\n    \t* 回调接口，通知失败。生产者此时可选择再次重发\n        \n\n\t* 不阻塞，吞吐量高\n\n\n* kafka\n\t* 设置acks=all 也是确认机制","layout":null},"children":[]}]},{"data":{"id":"bxbbpy56jnc0","created":1569642195947,"text":"MQ数据丢失","note":"* rabbitMQ\n1. 创建queue时将其设置为持久化的\n\t* 将会持久化queue的元数据\n    \n2. 发送消息时设置deliveryMode为2\n\t* 消息持久化\n    \n* 可以结合确认机制\n\t* 持久化后才发送接收成功通知\n    \n\n* kafka\n1. leader宕机，部分数据没来得及同步到follower\n   * 数据丢失\n        \n2. 参数设置\n   * replication.factor大于1\n        * 即至少2副本（副本计算包括leader）\n        \n    * min.insync.replicas大于1\n        * 即leader感知到至少有一个follower跟自己保持联系\n        * 确保leader挂了还有一个follower\n        \n    * 在生产者端设置acks=all\n    \t* 即要求每条数据必须写入所有的replia后，才算是成功\n        \n    * 生产者端设置retries=很大的一个值\n    \t* 写入失败，无线重试\n        * 即上面的配置，至少2副本等条件，不满足也算是失败，会不断重试","layout":null},"children":[]},{"data":{"id":"bxbbufe2vx40","created":1569642546947,"text":"消费者丢失数据","note":"* rabbitmq设置了autoAck 自动通知mq\n\t* 才会出现\n    \n    * 一般要关掉\n    \t* rabbitmq会重发给其他消费者\n     \n     \n     \n* kafka\n\t* 跟rabbitmq差不多\n    \t* 设置了自动提交offset\n    * 改为手动提交即可","layout":null},"children":[]}]},{"data":{"id":"bxbgjwa9cbs0","created":1569655827265,"text":"如何保证消息的顺序性","note":"* 如mysql binlog同步\n\t* 顺序性必须保证\n\n* rabbitMQ\n\t* queue中，消息只能被消费一次\n    \t* 多消费者情况下，顺序性不能保证\n    * 解决\n    \t* 需要保证顺序性的数据，都一个队列对应一个消费者\n        \n\n* kafka\n\t* 写入一个partition的数据，一定是有顺序的\n    \t* 生产者写的时候，可以指定一个key（如订单id）\n        * 跟此订单相关的数据，一定分配到一个partition中\n        * 可使用hash进行分发\n    \n    * 一个partition只对应一个消费者\n    \t* 本来也做不到对应多个消费者\n    \t* 则可以保证顺序性\n        \n    * 消费者内部多线程处理\n    \t* 思路跟上面多个partition一样\n        * 再次进行hash分发，相同订单号的放进一个一个内存队列，一个线程对应一个内存队列\n        * 则可以保证顺序性\n     \n     * 单线程的消费者\n    \t* 一条需要几十ms的话，一秒处理几十条数据，吞吐量较低\n        * 所以一般还是多线程\n        \n        * 压测，4核8G的机器，单机开32线程，每秒可处理上千条数据","layout":null},"children":[]},{"data":{"id":"bxbi48jsytk0","created":1569660242357,"text":"消息队列满了怎么处理","note":"* 或者问：几百万数据积压怎么处理\n\t* 对于rabbitmq，可以设置过期时间\n\t\t* 时间长消息会丢失（因此不能这样设置）\n        * 如果已经发生，则写程序从数据源头重新导出，进行补偿\n\n\n* 消费端出现问题\n\t* 导致mq满了，或者大量积压。以及消息已经延迟等问题\n    \n* 处理\n\t* 当务之急，肯定是修复消费者 \n    \n    * 新加一个Topic（kafka），并且partition是原来的10倍\n    \t* 消费者消费到数据后，不执行原来的逻辑，而是重写写到新的topic\n        \n        * 新加机器，作为消费者去处理新的topic\n        \n\t* 如果mq满了，则要加新的临时mq处理\n    \t* 加不了那就扔掉数据，后续写程序进行补偿","layout":null},"children":[]},{"data":{"id":"bxbafics9gg0","created":1569638556827,"text":"项目场景","note":"* 账务系统放款\n\t* 账务方面计算费用，是比较耗时的 一个业务\n    \t* 做成异步\n        \n    * 同时账务系统对接多个贷前入口\n    \t* 解耦","layout":null},"children":[]},{"data":{"id":"bxbiwnbvr4g0","created":1569662468726,"text":"如何设计一个消息队列","note":"* 首先MQ需要支持扩容\n\t* 即可伸缩性\n    * 设计分布式系统，参考kafka\n    \t* 每个broker是一个机器，是集群的其中一个节点\n        * 一个topic可以拆成多个partition来存储\n        * 资源不够就增加partition（增加机器）\n        * 有需要的话还可以做数据迁移\n        \n* 其实考虑数据做持久化（落盘）\n\t* 保证数据不丢失\n    * 在磁盘做顺序写，性能会比随机写性能更好\n    \t\n        \n* 其次 考虑mq的可用性\n\t* 如kafaka的多副本的高可用保障机制\n    \n* 数据丢失问题处理\n\t* 确认机制等","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"bxbji7jgrxk0","created":1569664158368,"text":"关于顺序写和随机写","note":"* 随机和顺序读写，是存储器的两种输入输出方式。\n\t* 连续的磁盘，删除数据后留下空洞\n    \t* 读取这些不连续的空间的数据，就是随机写，速度慢\n        * 因为磁头要不断的调整磁道的位置，以在不同位置上的读写数据，\n    * 连续的磁盘进行读写，速度快","layout":null},"children":[]}]}]},{"data":{"id":"bxbjseo53sw0","created":1569664957529,"text":"redis 分布式缓存","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"bxbjzn6q3e80","created":1569665524616,"text":"为什么要用缓存","note":"* 高性能，高并发\n\n* 不变的数据，如系统配置、商品信息\n\t* 多人访问，每次从数据库获取，则浪费性能\n    \t* 存进缓存，速度快\n        \n    * 常用的复杂查询的结果也可以放进缓存\n    \n    \n        \n* 访问高峰期时（高并发）\n\t* 对数据库造成很大压力，可能宕机\n    \t* 使用缓存减轻压力\n \n \n \n* 缓存为什么快\n\t* 缓存是走内存的，天然速度快，能支撑高并发访问\n\t\t* 几万/s的访问也是没问题的\n\n\t* 数据库则不行\n\t\t* 一般建议并发不能超过2000/s\n        \n   ","layout":null},"children":[]},{"data":{"id":"bxbm7rv5axk0","created":1569671803933,"text":"与mencache的区别","note":"* redis作者给出的比较\n\t1. 支持更多数据结构与操作\n    \n    2. 内存使用效率对比（不重要）\n    \t* 简单的key-value，是memcache高\n        * redis采用hash结构来做key-value的话，redis会更高\n        * 因为是组合式的压缩\n        \n    3. 性能（跟第二点一样，区别比较轻微，可不说）\n    \t* redis只是用单核（单线程）\n        \t* 平均到每个核上，redis在存储小数据时，性能比memcache高\n            * 100k以上的大数据，memcached性能高一些\n        * memcache可以使用多核\n    \n    4. memcached 没有原生的集群模式\n    \t* 依靠客户端实现分片（集群中分片写入数据）\n        * redis原生支持cluster模式(官方支持)","layout":null},"children":[]},{"data":{"id":"bxbm2zh3hcw0","created":1569671428677,"text":"redis的线程模型","note":"* 文件事件处理器（file event handler）\n\t* redis基于rector模式开发的网络事件处理器\n    * 是单线程的\n    \t* 采用IO多路复用机制同时监听多个socket\n        * 根据socket上的事件来选择对应的事件处理器处理这个事件\n     \n     \n\n* 文件事件处理器结构\n\t* 多个socket\n\n\t* IO多路复用程序\n\n\t* 文件事件分派器\n    \n    * 事件处理器\n    \t* 命令请求处理器\n        * 命令回复处理器\n        * 连接应答处理器\n        \n* redis单线程模型为啥效率高\n\t* 纯内存操作\n    * 核心是基于非阻塞的IO多路复用机制\n    \t* 由于不阻塞，效率高（相当于不等待，一直处理各种事情）\n        \n    * 单线程避免线程频繁上下切换\n    \t* 内核态、用户态切换耗性能\n\t","layout":null},"children":[]},{"data":{"id":"bxbmwjwq9ag0","created":1569673745719,"text":"客户端与redis的一次通信","note":"* 基于socket通信模型进行通信 \n\t* redis 中有一个server socket监听请求\n \n* redis启动\n\t* 会将AE_READABLE事件与连接应答处理器进行关联\n\n\n\n\n1. IO多路复用程序会监听socket\n    * 将监听到事件的socket（及其产生的事件）压到一个队列里面\n    \n    * 每次取出一个socket给事件分派器\n    \t* 处理完之后才拿下一个\n    \n    * 文件事件分派器会将队列里面的socket取出\n    \t* 交给对应的处理器进行处理\n        \n        \n\n\n\n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxboek46s2g0","created":1569677977835,"text":"连接","note":"2. 例：客户端连接到redis的server socket，请求建立连接\n\t* server socket接收到请求后，会产生AE_READABLE事件\n        \n    * 第1点中的处理步骤\n        \n    * 最终交给连接应答处理器进行处理\n        * 连接应答处理器与客户端建立连接，创建客户端对应的socket（客户端本身有一个socket）\n        \n        * 将此socket的AE_READABLE事件与命令请求处理器关联","layout":null},"children":[]},{"data":{"id":"bxboerr4n3c0","created":1569677994459,"text":"请求","note":"3. 客户端发起写读写请求\n\t* 则已经建立连接的socket会产生AE_READABLE事件\n    \n    * 同样，IO多路复用程序处理\n    \t* 第一点\n        \n    * 交给命令请求处理器处理\n    \t* 从socket中读出请求的key，value（假设是写请求）\n        * 在内存中key，value的设置\n\t\n    \n* 处理完毕后（准备好响应数据后）\n    * 将socket的AE_WRITABLE事件跟命令回复处理器关联\n        \n    * 客户端准备好读取数据时，socket上会产生AE_WRITABLE事件\n    \n    * 同样，IO多路复用程序处理\n    \t* 最终交给命令回复处理器处理\n   \n    * 命令回复处理器对socket输出本次操作的一个结果\n    \n    * 将此socket与AE_WRITABLE事件解除关联\n        \n        ","layout":null},"children":[]}]},{"data":{"id":"bxcj9ew2ksg0","created":1569765029046,"text":"五种数据类型","note":"1. String\n\t* 存字符串\n\n2. Hash\n\t* 存简单对象等\n    \t* 第二个key作为字段\n3. list\n\t* 有序list（有序就是插入顺序）\n    \n4. set\n\t* 无序集合，自动去重\n    \t* 集群多台机器一起做去重等\n        \n5. sorted set\n\t* 自动去重，排序，按照分数排序\n    \t* 用户分数排行榜\n        * IP地址区域分类等","layout":null},"children":[]},{"data":{"id":"bxcjtk2unmg0","created":1569766607624,"text":"redis过期策略","note":"* 定期删除+惰性删除\n\t* redis默认每隔100ms，随机抽取一些有设置供货期时间的key\n    \t* 检查是否过期，过期就删了\n        * 注意是随机抽一些，不是全部遍历\n        \n    * 惰性\n    \t* 获取这个key时会检查一下，已经过期了就删\n   \n   \n* 内存淘汰机制\n\t* 防止堆积太多数据\n    \t* 以上两种方式，可能还是会导致大量数据残留\n    \n    * noeviction\n    \t* 内存不足时写入数据，写入报错\n    * allkeys-lru（常用）\n    \t* 内存不足时写入数据\n        * 移除最近最少使用的key\n    \n    * allkeys-random\n    \t* 内存不足时，随机移除某个key\n        \n    * volatile-lru\n    \t*  内存不足时，在有设置过期时间的key中，移除最近最少使用的\n    * volatile-random\n    \t*  内存不足时，在有设置过期时间的key中，随机移除key\n    * volatile-ttl\n    \t*  内存不足时，在有设置过期时间的key中，优先移除有更早过期时间的（快过期的）","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxckd21bxjk0","created":1569768135633,"text":"手写LRU","note":"* 结合linkedHashMap 处理\n\t* 继承linkedHashMap\n    \t* 构造函数的accessOrder传入true\n    * 重写removeEldestEntry\n    \t* 满足特定条件返回true，将删除最老的元素\n        * 条件即大于设定大小时\n","layout":null},"children":[]}]},{"data":{"id":"bxd5dcwkiz40","created":1569827402598,"text":"高并发高可用","note":"* 高并发中，缓存是必不可少的\n\t* redis不能支持高并发的瓶颈在于单机\n    \t* 单机支持QBS是一到几万（一般不到10w）\n    \n    * 主要针对的是读高并发\n    \n    \n* 不可用\n\t* 系统因各种原因挂了，此时可能在抢修之类\n    \t* 用户无法访问使用，就是不可用\n \n \n*  99.99%高可用\n\t* 全年内99.99%的时间可以使用\n    * 一般99%都是高可用了\n\n\n\n* redis高可用\n\t* 一个slave挂了，不影响可用性\n    * master挂了\n    \t* 就是相当于缓存不可用\n        * 大量的请求直接涌入mysql，进而导致系统挂了\n\n\n\n* 基于哨兵的主从-高可用架构（故障转移 failover）\n    * 主备切换\n        * master故障，自动检测，并将一个slave变成master\n        \n    * 监测master的是sentinal node\n    \t* 哨兵","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxd61knidnc0","created":1569829300204,"text":"读写分离","note":"* 一主多从\n\t* 主master节点负责写（缓存使用场景一般都是读多写少）\n    \t* 主节点同步数据到从节点\n    * 从slave节点负责读\n    \n    * 优点（水平扩容）\n    \t* 承担更多QBS的时候只需新增从节点\n\n\n","expandState":"expand","layout":null},"children":[{"data":{"id":"bxd71piokuo0","created":1569832131906,"text":"redis replication","note":"* 主从复制机制redis replication\n\n\t* redis使用异步方式复制数据到slave节点\n    \t* 2.8开始，slave node会周期性确认自己每次复制的数据量\n     \n    * 一个master node 可以配置多个slave node\n    * slave node 也可以连接其他slave node\n    \n    * slave node做复制时\n    \t* 不会block master node的正常工作\n        * 也不会block 自己的查询操作（使用旧数据集提供服务）\n\t\t* 复制完成时需要删除旧数据集，加载新数据集。此时暂停对外服务（毫秒到秒之间的级别）\n    \n    * slave node主要用于横向扩容\n    \t* 做读写分离\n        * 扩容的slave node可以增加读的吞吐量\n    \n    \n    \n    \n* master node的持久化\n\t* 使用了主从架构，master node持久化必须开启\n    \t* 不建议使用slave node作为master node的数据热备\n        * 因为不开持久，master重启后，数据是空的，经过复制，slave node数据也没了\n    * 甚至，RDB文件也进行备份\n        \n    * 即使在高可用机制下\n    \t* slave node自定接管master，也有可能sentinal没检测到master failure，master就自动重启了，还是会导致以上情况\n        \n        \n        \n   ","layout":null},"children":[]},{"data":{"id":"bxd7fnxir5k0","created":1569833225548,"text":"主从复制原理","note":"1. 启动slave node时\n\t* 发送一个PSYNC命令给master node\n    \t* 如果是slave node重连master node，则只会复制给slave部分缺失的数据\n        * 如果是第一次连接master，会触发full resynchronization\n    \n    * full resynchronization\n    \t* master启动一个后台线程。生成一份全量RDB快照文件\n        * 并同时在内存中缓存最新的数据\n        \n        * slave会先保存RDB文件到磁盘 并加载到内存\n        * master把内存中的数据同步到slave\n        \n    * slave如果网络故障断开连接\n    \t* 会自动重连\n        * master如果发现多个slave来重连，只会启动一个RDB save操作，用一份数据服务所有slave \n\n2. 正常连接时\n\t* 来一条写请求，master会异步同步给slave\n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxdzckuzk800","created":1569911974849,"text":"详细复制步骤","note":"1. slave node 启动，仅保存master node的信息\n    * master node的host，ip等\n    \t* 信息是从redis.conf的slaveof获取的信息\n        * 此时复制流程还没开始\n \n \n2. slave node内部有一个定时任务\n\t* 每秒检查是否有新的master node要连接和复制\n    \t* 有的话，跟master node建立socket网络连接\n  \n  \n3. slave node发送ping命令给master node\n\n\n4. 口令认证\n\t* 如果master设置了requirepass\n    \t* 则slave node序发送masterauth口令进行认证\n    \n    \n5. master node第一次执行全量复制，将所有数据发送给slave node\n\n\n6. master node后续持续写命令，异步复制给slave node","expandState":"expand","layout":null},"children":[{"data":{"id":"bxdzrrlrs0w0","created":1569913164992,"text":"数据同步相关核心机制","note":"* 指第一次slave连接master时执行的全量复制\n\t1. master 和slave都会维护一个offset\n    \t* master、slave都会在自身不断累加offset\n        \n        * slave每秒都会不断上报自己的offset给master\n        * 同时master也会保持每个slave的offset\n        \n    2. 第一点不特指只用在全量复制过程\n    \t* 主要是master和slave都要知道各自数据的offset，才能知道相互之间的数据不一致情况 \n        \n    3. backlog\n    \t* 全量复制时。master node中有一个backlog，默认1M\n        * master node给slave node复制数据时，也会在backlog中同时写一份\n        * 全量复制中断时，做增量复制的\n    4. master runid\n    \t* 使用命令：info server 可查看该run id\n        * 因为根据host+ip定位master node是不靠谱的\n        * 例如master重启，或master根据RDB备份恢复到某一阶段的数据，此时需要全量重新同步，就可以利用变化了的runid进行识别\n        * runid不同，就会触发全量复制\n        * 不更改runid重启redis命令：redis-cli debug reload\n     \n    5. psync\n    \t* slave node发送psync runid offset 给master 节点\n        * master会根据自身情况响应信息\n        * 可能是 FULLRESYNC runid offset 触发全量复制，也可能是continue触发增量复制","layout":null},"children":[]},{"data":{"id":"bxe16v4tj6g0","created":1569917169246,"text":"全量复制","note":"1. master执行bgsave，在本地生成rdb快照文件\n\n2. master 将rdb文件发送给slave ，如果复制（发送）时间超过repl-timeout指定时间\n\t* 默认60s（可根据具体文件大小适当调整）\n    * 超过时间认为复制失败\n\n3. 生成rdb的同时，会将新的命令缓存在内存中\n\t* slave保存完rdb后\n    \t* 再将新的命令复制给slave\n        \n    * client-output-buffer-limit slave 256MB 64MB 60（配置文件参数）\n    \t* 表示在复制期间，内存缓冲区持续（60s内）消耗超过64MB，或者一次性消耗超过256MB，则停止复制，本次复制失败\n        \n4. slave节点接到rdb文件后，清空自己的旧数据\n\t* 重新加载rdb到自己的内存\n    \t* 且同时使用旧数据集对外提供服务\n    * 如果开启了AOF\n    \t* 立即执行BGREWROTEAOF，重写AOPF（基于当前最新内存数据）\n        \n* 整个复制时长\n\t* 4~6G数据，大约1.5~2min","layout":null},"children":[]},{"data":{"id":"bxe1lyfi9gg0","created":1569918351885,"text":"增量复制","note":"* 全量复制过程中。网络断掉，slave重连\n\t* 触发增量复制\n    \n    * master从backlog中获取丢失的数据，发送给slave\n    \t* 根据offset判断","layout":null},"children":[]},{"data":{"id":"bxe1p0651940","created":1569918590765,"text":"heartbeat","note":"* 正常状态下的主从节点（完成复制之后等）\n\t* master每隔10s发送一次heartbeat\n    * salve每隔1s发送一次heartbeat\n\n* 异步复制\n\t* 之后新的命令，master先自己写入数据\n    \t* 之后异步发给clave","layout":null},"children":[]}]},{"data":{"id":"bxdenqfbw2g0","created":1569853605661,"text":"断点续传+ 无磁盘化复制","note":"3. 断点续传\n\t* redis 2.8开始支持\n    \t* 主从根据RDB文件进行大数据集复制时\n        * 复制过程中，网络断掉，可以接着上次复制的地方继续复制，而不是重新从头开始\n        \n    * master会在内存中维护一个backlog\n    \t* master和slave都会保存replica offset\n        * offset就是保存在backlog中\n        * 此时如果网络断了重连，会从offset开始开始复制\n        * offset找不到的话，就要执行resynchronization\n        \n\n4. 无磁盘化复制\n\t* master直接在内存中创建rdb，发给slave，不会在自己本地落盘\n    \n\t* 通过以下参数配置生效使用\n    \t* repl-diskless-sync\n    * 另外，参数repl-diskless-sync\n    \t* 等待一定事件再开始复制\n        * 等更多slave连上来一起复制\n        \n        \n5. 过期key处理\n\t* slave不会过期key\n    \t* 等master过期key\n        * master的key过期后，或LRU淘汰一个key，会模拟一条del命令发送给slave","layout":null},"children":[]}]}]},{"data":{"id":"bxe2gfmpshk0","created":1569920740252,"text":"sentinal 哨兵","note":"* 哨兵是redis集群架构中非常重要的组件\n\t* 集群监控\n    \t* 监控redis mater和slave进程是否正常工作\n        \n    * 消息通知\n    \t* 如果某个redis实例有故障，哨兵负责发送消息作为报警通知管理员\n        \n    * 故障转移\n    \t* 如果master node挂了，会自动转移到slave node上\n    \n    * 配置中心\n    \t* 如果发生了故障转移，通知client客户端新年的master地址\n\n\n* 一般就在redis实例所在机器上，对应放一个哨兵进程\n\n\n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxe3phhl4g80","created":1569924270682,"text":"哨兵集群","note":"* 哨兵本身是分布式的\n\t* 作为一个哨兵集群去运行，互相协调工作\n    \t* 判断master是宕机了，需要大部分的哨兵都同意\n        * 部分哨兵节点挂了，哨兵集群可以正常工作\n    \n    \n    * 哨兵至少3个实例，保证自己的健壮性\n    \t* quorum=N 表示需要多少哨兵同意\n        * 如果是2个实例，设置参数为1，此时挂了一个（master+1哨兵），则同意切换\n        * 但是需要选举出一个哨兵来执行故障转移（判断宕机+选举哨兵执行都是可以的）\n        * 但是此时需要majority来允许故障转移\n        \t* 即：（一半以上，4个节点的貌似是2）的个数的哨兵在运行\n          \n    * 哨兵+ rdis主从架构\n    \t* 不保证数据零丢失，只保证redis集群高可用","layout":null},"children":[]},{"data":{"id":"bxe45x0m9y80","created":1569925558312,"text":"哨兵主备切换数据丢失问题","note":"* 由于主从的复制是异步的\n\t* 因此slave的数据可能还没及时更换\n    * 此时如果master node挂了\n    \t* 则主从切换后，未同步的数据丢失\n\n\n* 网络故障（集群脑裂），导致网络分区\n\t* 例：master所在的网络不能与哨兵集群以及slave node访问\n    \t* 但是哨兵集群可以跟slave保持网络通讯\n        * 此时哨兵会认为master挂了，切换salve为master\n     \n    * 出现两个master节点\n    \t* 像人的大脑变成2个（所以叫脑裂）\n    \n    * 解决网络问题后，原先的master调整为slave\n    \t* 但是原本的master并没有挂，里面可能有数据并未同步到slave\n        * 此时丢失数据\n\n\n* 解决（降低损失）:参数配置\n    * min-slave-to-write N\n    * min-slave-max-lag M\n    \t* 减少异步数据的丢失\n        \n    * 要求至少有N个slave，数据复制和同步的延迟不能超过M秒\n    \t* 达不到此要求的话，master此时就不再接收任何请求\n        \n        * client此时可做降级，写到磁盘，再做限流，减少请求涌入的速度\n        * 或者将数据临时灌入kafka，每隔10分钟去队列里面取一次，尝试重新发回master\n    * 脑裂场景也是\n    \t* 没有slave反馈落后数据在10s之内，master不接受请求，则网络恢复后损失的数据少","layout":null},"children":[]},{"data":{"id":"bxe5gchd57k0","created":1569929196727,"text":"sdown和odown","note":"* sdown和odown\n\t* sdown（主观宕机）\n    \t* 一个哨兵自己觉得一个master宕机\n        * 哨兵ping一个master，超过配置is-master-down-after-milliseconds指定的毫秒数，就是主观宕机\n        \n    * odown（客观宕机）\n    \t* N个哨兵觉得master宕机\n        * N==quorum的配置数量\n        \n    * sdown转换到odown\n    \t* 一个哨兵在指定时间内，收到quorum指定数量的的其他哨兵也认为sdown的信息，就是odown","layout":null},"children":[]},{"data":{"id":"bxe627w66lc0","created":1569930910750,"text":"哨兵和slave集群的自动发现机制","note":"1. 哨兵相互之间的发现\n\t* 通过redis的pub/sub（发布订阅）实现\n    \t* 每个哨兵都会往_sentinal_:hello 这个channel发送一个消息\n        * 其他哨兵可以消费这个消息，并感知其他哨兵的存在\n    \n2.  每隔2s，每个哨兵都会往自己监控的某个master+slaves集群对应的_sentinal_:hello channel里发送一个消息\n    * 内容是自己的host、ip及runid，以及对这个master的监控配置\n    \n    * 每个哨兵也会去监听自己监控的每个master+slaves对应的_sentinal_:hello channel\n    \t* 然后会感知同样监听这个master+slaves的其他哨兵的存在\n        \n        * 每个哨兵还会跟其他哨兵交换对master的监控配置，对配置进行同步","layout":null},"children":[]},{"data":{"id":"bxe6domzhbc0","created":1569931809206,"text":"slave配置自动纠正","note":"* 哨兵会负责纠正slave的一些配置\n\t* 例slave如果要成为master的潜在候选人\n    \t* 则哨兵会确保slave在复制现有master的数据\n        \n    * slave连接到一个错误的master上\n    \t* 例如故障转移后\n        * 则哨兵会确保他们连接到正确的master上","layout":null},"children":[]},{"data":{"id":"bxe6hslzgkw0","created":1569932131309,"text":"slave-master选举算法","note":"* 一个master被认为odown了\n\t* 且majority哨兵允许了主备切换\n    \t* 某个哨兵将执行主备切换\n    * 此时需要选举一个slave\n\n\n* 选举算法\n\t1. 先查看跟master断开连接的时长\n    \t* 时长如果超过配置down-after-milliseconds的10倍+master的宕机时长\n        * 公式：（down-after-milliseconds* 10）+ millliseconds _since_master_id_in_SDOWN_state\n        \n        * 则该slave被认为[不适合]选举为master\n        \n    2. 剩下的slave进行排序\n    \t1. 按照slave优先级进行排序，该配置：slave-priority越小，优先级越高\n        2. 如果优先级相同，就看replica offset，哪个slave复制的数据越多，offset越靠后，优先级越高\n        \n        3. 以上条件都相等，就选runid小的slave","layout":null},"children":[]},{"data":{"id":"bxe6sspgaq80","created":1569932993525,"text":"quorum和majority","note":"* quorum数量的哨兵认为sdown\n\t* 变成odown\n    * 然后会选举一个哨兵来做主备切换\n    \t* 但是这个哨兵还得得到majority数量的哨兵的授权才能正式执行切换\n \n* quorum < majority\n\t* 可以进行切换\n    \t* 运行的哨兵比认为宕机的数量多\n* quorum >= majority\n\t* 正在运行的哨兵小于等于认为master宕机的哨兵\n    \t* 此时必须有quorum数量的哨兵都同意，才能切换\n        \n        \n* configuration epoch\n\t* 哨兵对一套redis master+slaves进行监控，会有相应的监控配置\n    \t* 执行切换的哨兵，要从新master中得到一个configuration epoch，是一个version号\n        * 每次切换的version号都必须唯一\n    \n    * 如果第一个哨兵切换失败\n    \t* 等待failover-timeout时间，然后接着继续执行切换\n        * 此时会重新获取一个新的configuration epoch，作为新的version、\n      \n* configuration的传播\n\t* 哨兵切换完成之后，会在自己本地更新生成最新的master配置\n    \t* 然后同步给其他哨兵\n        * 即之前说的pub/sub消息机制\n        \n    * 此时version号就很重要了\n    \t* 因为消息都是通过一个channel去发布和监听的\n        * 切换完成后，新的master配置对应新的version号，其他哨兵发现有新的版本号，就会更新自己的配置","layout":null},"children":[]}]},{"data":{"id":"bxe7dosnw8g0","created":1569934630659,"text":"总结","note":"* 高并发\n\t* 使用一主多从\n    \t* 主负责写，从负责读\n    \n    * 但是这种每台机器的数据是一样的\n    \t* 会有可能单机内存的上限\n\n \n* 高可用\n\t* 主从架构的话\n    \t* 加上哨兵机制即可\n        * 自动主备切换"},"children":[]}]},{"data":{"id":"bxe7moysd480","created":1569935336307,"text":"持久化","note":"* 持久化的意义\n\t* 故障恢复\n    \n    * 如遇到灾难性的故障\n    \t* 机器没了，断电等\n        \n    * 定期同步、备份持久化的数据到云存储服务上去\n    \t* 起新的机器，读取备份文件\n        * 则可以保证不丢失全部数据\n        \n        \n        \n* 持久化策略的选择\n\t* 一般是两种都开启\n    \t* 只开启RDB，则会丢失很多数据\n        *  只开启AOF，通过AOF做的冷备数据恢复慢\n        \t* 且RDB是数据快照，更加健壮，可以避免AOF复杂的备份和恢复机制导致的bug\n    \n    * AOF用来保证数据不丢失，作为数据恢复第一选择\n    * RDB做不同程度的冷备，AOF文件丢失或损坏的话，可用RDB快速恢复数据","expandState":"collapse"},"children":[{"data":{"id":"bxe8q27sutc0","created":1569938421353,"text":"RDB","note":"* 对redis中的数据执行周期性的持久化\n\n* 触发RDB时\n\t* 将当时redis内存中的数据的完整快照保存下来到rdb文件中"},"children":[{"data":{"id":"bxe9lhnccnc0","created":1569940884233,"text":"优点","note":"1. RDB会生成多个数据文件\n\t* 每个数据文件代表某一时刻的redis数据\n    \t* 这种多个数据文件的方式，非常适合做冷备份\n    * 可以定时将文件发到云服务存储\n    \n    * AOF做冷备的话，就是间隔一定时间copy一份\n    \t* 相对RDB麻烦\n    \n    \n2. RDB对redis的性能影响非常小，可以让redis保持高性能\n\t* 备份时间间隔长\n    * redis主进程只需fork一个子进程，让子进程执行磁盘IO即可\n    \n    * 每次写RDB都是写内存\n    \t* 只是在一定的时候，才会将数据写到磁盘\n    * AOF每次都要写文件\n    \t* 虽然可以快速写到OS cache中，但是还是有一定的时间开销，速度比RDB慢\n  \n  \n3. 相对于AOF，基于RDC文件重启和恢复redis进程，会更快速\n\t* AOF存放的是指令日志\n    \t* 需要回放、执行所有的指令\n    * RDB就是一份数据文件\n    \t* 直接加载到内存即可\n\n\n"},"children":[]},{"data":{"id":"bxe9x9wt0zk0","created":1569941807761,"text":"缺点","note":"* 故障恢复的话，RDB丢数据会比较多\n\t* 备份时间间隔更长\n    \n    * 因此不适合做第一优先的恢复方案\n\n\n\n* RDB在每次fork子进程来执行rdb文件生成时\n\t* 如果文件特别大\n    \t* 可能会导致客户端提供的服务暂停数毫秒，甚至数秒\n        \n    * 因此文件生成的间隔不能太短（频繁）\n    \t* （原文不能太长，存疑）"},"children":[]}]},{"data":{"id":"bxe8q6en8080","created":1569938430474,"text":"AOF","note":"* 把每条命令都写到AOF文件中\n\t* 使用append-only（追加）的方式写入日志文件中\n    \n\t* 现代操作系统中，写文件不是直接写磁盘。\n    \t* 先写到OS cache\n        * 一定时间后，再从os cache到disk file\n        * 每隔1s，调用一次操作系统的fsync函数，强制将OS cache中的数据刷到磁盘\n        \n\n\n* redis重启时\n\t* 可以通过回访AOF日志中的写入指令来重新构建整个数据集\n    * 同时使用两种持久化策略\n    \t* redis会使用AOF来重新构建数据，因为AOF数据更完整\n        * 可以做到最多丢失1s\n\n\n* AOF文件过大问题\n\t* redis内存是有限的\n    \t* 一定时间，redis就会用淘汰树算法，将一部分数据清除\n        \n    * 但是AOF是存放每条命令的\n    \t* 因此会不断膨胀\n        * 达到一定程度，AOF做rewrite操作\n        \n    * rewrite\n    \t* 基于内存中的数据，构造一个更小的AOF文件\n        * 旧的大文件删了"},"children":[{"data":{"id":"bxenf7v0po80","created":1569979888283,"text":"优点","note":"1. 更好的保护数据不丢失\n\t* 一般AOF每隔1s，通过一个后台线程执行一次fsync\n    \t* 最多丢失1s（该1s是配置de）\n    \n    * 使用append-only模式写入\n    \t* 没有任何磁盘寻址开销\n        * 写入性能非常高\n        * 文件不容易破损，文件尾部破损也容易修复\n        \n    * AOF文件过大时，执行rewrite操作\n    \t* 几乎不影响客户端的读写\n        \n        * 对其中的指令进行压缩，创建出一份需要恢复的最小日志\n        \t* 创建日志时，老的日志文件照常写入\n            * 新的merge后的日志文件ready后，替换老的日志文件（应该就是有一个menge的操作）\n            \n    * AOF中记录的是指令，是人类可读的\n    \t* 可用于误删紧急恢复\n        * 例：使用flushall清空所有数据\n        \t* 此时只要还没发生rewrite\n            * 复制一份AOF文件，删除最后一条命令，将AOF文件放回去，重启。可恢复所有数据"},"children":[]},{"data":{"id":"bxenqtr1ye80","created":1569980797939,"text":"缺点","note":"* 由于记录的是日志\n\t* 同一份的数据，AOF的文件会更大\n\n\n* 开启AOF后，支持的写QBS会比RDB支持的写QBS低\n\t* 因为AOF一般会配置每秒fsync一次日志文件\n    \t* 当然每秒一次的fsync，性能还是很高的\n    \n    * 其实AOF可以支持一条数据都不丢\n    \t* fsync设置为每写入一条数据就执行一次\n        * 但是redis的写QBS大降\n        * 因此不能这样设置\n  \n* AOF 较为复杂，也因此更加脆弱，容易有bug\n\t* 以前曾经发生根据AOF回放，得到的数据跟之前不一致\n    \n5. 做数据恢复比较慢\n\t* 且做冷备的话不方便\n"},"children":[]}]}]},{"data":{"id":"bxeo9fg99080","created":1569982255730,"text":"redis cluster","note":"1. 以前redis搞多节点的话，需要借助中间件\n\t* codis、twemproxy等都是中间件\n    * 客户端读取中间件，中间件负责将数据分布式存储到多台机器的redis实例中\n    \n2. redis cluster\n\t* 原生支持的集群模式\n    \t* 自动将数据分片，每个master上放一部分数据\n    \n    * 可以支撑N个redis master node\n    \t* 每个master node都可以挂载多个slave node\n        \n    * 理论上可支持读写分离\n    \t* 但是实际上不用，横向扩容master即可\n        * 一定要做的话，较为复杂\n    \n    * 高可用\n    \t* master挂了，会自动将某个slave切换为master\n        * 不再需要手工搭建replicatin复制+主从架构+读写分离+哨兵集群的高可用\n        \n        \n","expandState":"collapse"},"children":[{"data":{"id":"bxeoyuegtkg0","created":1569984247377,"text":"对比replaicatipn+sentinal","note":"* 如果数据量不多（几个G）\n\t* 主要是承载高并发高性能场景\n    * 则使用replication\n    \t* 一个master，多个slave\n        * slave数量取决于读吞吐量\n        * 以及搭建一个sentinal集群\n\n* redis cluster\n\t* 针对海量数据\n    * 高并发+高可用"},"children":[]},{"data":{"id":"bxepkkn8nsg0","created":1569985950152,"text":"数据分布算法","note":"* 分布式数据储存核心算法\n\t* 使用不同的算法，决定在多个master节点上，数据如何分布到这些节点上\n    \n\n        \n","expandState":"collapse"},"children":[{"data":{"id":"bxewodfvog00","created":1570005995695,"text":"hash算法","note":"* 已经很少在缓存领域用了\n\n* 计算key的hashcode，对节点数量进行取模\n    * 当然取模有很多种方式，余数或者位运算等（取模本身的意思就是获取余数）\n    \n* 其中一个节点宕机了\n    * 导致该节点数据全部失效\n    \n    * 需要对剩下的机器重新取模，再重新分配\n    * 由于节点变少，hash算法算出来的机器可能不是之前的有数据的机器，\n    \t* 相当于整个redis缓存大部分数据相当于没有了\n        * 因此就会直达数据库\n    \n\n* 高可用场景下，不可接受\n    * 失效的部分直接走数据库，数据库压力大"},"children":[]},{"data":{"id":"bxewpwyi0gg0","created":1570006116544,"text":"一致性hash算法","note":"* 一致性hash算法（自动缓存迁移）+虚拟节点（自动负载均衡）\n    * memcached 会用\n    \n    \n* 使用一个hash环，将机器放在圆环上（的某个点）\n    * 有一个key过来的话，计算其hash值\n    * 将hash值与圆环上的各个点做对比（每个点都有hash值），\n    \t* 看hash值 落在圆环的哪个部位\n        * key落在圆环上后，顺时针旋转去寻找距离自己最近的一个节点\n   \n   \n   \n    \n* 自动缓存迁移\n    * 通过圆环的查找，即使挂了一个节点 \n    \t* 也可以顺时针到达下一个节点\n\n\n* 相比hash算法，不会导致大部分数据失效\n\t* 只是挂掉的节点上的数据有影响\n    \t* 这些数据走数据库，并记录在下一个节点上\n   \n   \n*  数据负载均衡\n\t* 每个节点可以增加虚拟节点\n    \t* 让每个机器的节点均匀散落在环上\n    *  一旦某台机器挂了，由于虚拟节点的存在\n    \t* 负载会相对均匀的散落在各个机器上\n    \n    * 同时可以解决缓存热点问题\n    \t* 即某个区间的hash值对应的数据特别多\n        * 使用虚拟节点可以分散数据\n   "},"children":[]},{"data":{"id":"bxexh3s528o0","created":1570008247230,"text":"hash slot算法","note":"* redis cluster 使用这种算法\n\t\n    * redis cluster有固定的16384个hash slot（hash槽）\n    \t* 每个master持有部分hash slot\n    \n    * 会对每个key计算crc16值\n    \t* 再对16384取模，获取对应的hash slot\n        \n    * 新增、减少master节点\n    \t* 增加节点，让其他master的hash slot移动部分过去\n        * 减少节点，让它的hash slot移动到其他master上去\n        * 移动hash slot的成本是非常低的\n    \n\n* 跟一致性hash的效果差不多\n\n* 客户端可以对指定的数据\n\t* 让他们走同一个hash slot\n    \t* 通过hash tag手动指定key对应的slot\n        * 同一个hash tag下的key，都会在一个hash slot中\n        * 例： \n        \t* set key1：{100} 和 set key2：{100}\n            * 100是hash tag，则相同的tag的数据会录入到相同的slot"},"children":[]}]},{"data":{"id":"bxexwmyxwsg0","created":1570009464462,"text":"内部通信机制","note":"* 通信\n\t* redis cluster架构下。每个redis要开放2个端口\n    \t* 如一个是6379\n        * 另一个就是加上10000的数，即16379\n        \n    * 两外一个端口 用于节点间的通信\n    \t* cluster bus用的（集群总线\n    \t* 用于故障检测，配置更新，故障转移授权\n    * cluster bus用了一种二进制的协议\n    \t* 可以在节点间进行高效数据交换，并占用更少的网络带宽和处理时间\n\n* 每个节点每隔一段时间，都会向另外几个节点但发送ping消息\n\t* 其他节点接收到之后会返回pong\n\n* 使用gossip协议进行通信\n\t* 维护集群的元数据用的\n    \t* 集群之间有很多元数据\n        * 如\n        \t* ghashslot与node之间的映射表关系\n            * master与slave之间的关系\n            * 故障的信息等\n            * 节点的新增、删除\n            * hash slot等信息\n        \n","expandState":"collapse"},"children":[{"data":{"id":"bxeyk7ogsts0","created":1570011311917,"text":"gossip协议","note":"* gossip\n    * 每个master节点都保存一份完整的元数据\n    * 有变更的话，会把变更数据发给其他的master，其他master更新数据\n    * 优点\n    \t* 元数据的更新比较分散，更新请求会陆陆续续打到各个节点上，有一定的延时，降低了压力\n        \n    * 缺点\n    \t* 元数据更新有延迟，可能导致集群的一些操作会有一些滞后\n        \n        \n* 集中式的\n\t* 除了gossip之外，还有一种叫集中式\n    \t* 集中式就是集中在一个地方进行维护、储存，如使用zookeeper\n        \n        * 大数据的strom就是使用了集中式的元数据储存架构，底层基于zookeeper集群对元数据\n        \n    * 集中式的优点\n    \t* 元数据的更新、读取，时效性非常好\n        * 一旦元数据出现变更，立即更新到集中式存储中\n        * 其他节点读取时可以立即感知到\n    * 缺点\n    \t* 元数据的更新压力集中在一个地方，可能导致元数据的存储有压力"},"children":[]},{"data":{"id":"bxeze3oy1400","created":1570013654164,"text":"gossip协议的消息","note":"1. gossip协议包含的多种消息\n\t* meet\n    \t* 某个节点给新加入的节点发送meet\n        * 让新节点加入集群\n        * 然后新节点就会开始与其他节点进行通信\n    \n    * ping\n    \t* 每个节点每秒都会频繁给其他节点发送ping\n        * 其中包含自己的状态还有自己维护的集群元数据\n        * 互相通过pinng交换元数据以及对元数据的更新\n        \n    * pong\n    \t* 接到ping或者meet后返回pong\n        * 包含自己的状态及其他信息\n        * 也可以用于信息广播及更新\n    \n    * fail\n    \t* 某个节点判断另外一个节点fail之后，就会发送fail给其他节点\n        * 通知其他节点：某个节点宕机了\n\n*  ping\n\t* 每个节点每秒会执行10次ping\n    \t* 而且要携带一些元数据，因此可能会增加网络负担\n        * 每次会选择5个最久没有通信的其他节点\n        \n    * 如果发现与某个节点的通信延迟达到cluster_node_timeout/2\n    \t* 即已经这么久没有通信\n        * 立即发送ping，避免数据交换延迟较长\n        * cluster_node_timeout调长一点的话，可以降低发送的频率\n    \n    * 每次ping带上自己的数据\n    \t* 以及带上整个集群的1/10的节点信息，发送出去进行出去进行数据交换\n    * 至少包含3个其他节点信息，做多包含总节点-2的其他节点信息 "},"children":[]}]},{"data":{"id":"bxezujx9sgw0","created":1570014943322,"text":"jedis","note":"* jedis\n\t* 面型集群的redis的java client\n    \n* jedis与集群交互的一些原理\n\t* 请求重定向\n    \t* 客户端可能会挑选任意一个redis实例发送命令\n        * 每个redis实例收到命令后，会计算ke对应的hash slot\n        \t* 在本地的话就在本地处理\n            * 否则返回moved给客户端，让客户端进行重定向（moved错误）\n            \n    * cluster keyslot mykey\n    \t* 可以查看一个key对应的hash slot\n   \n   \t* 使用redis-cli时，加上-c 参数\n    \t* 支持自动的请求重定向\n        * redis-cli接收到moved后，会自动重定向对应的节点执行命令\n        \n    * hash slot查找\n    \t* 节点间通过goosip协议进行数据交换，就知道每个hash slot在哪个节点上\n\n\n        \n        ","expandState":"collapse"},"children":[{"data":{"id":"bxf0xzkjl000","created":1570018033584,"text":"smart jedis","note":"1. 基于重定向的客户端\n\t* 很消耗网络IO\n    \t* 因为大部分情况下，可能都会出现一次请求重定向，才能找到正确的节点\n    * 因此大部分客户端都是smart的，包括jedis\n    \n* 原理\n\t* 客户端本地维护一份hash slot与node的映射的缓存\n    \t* 大部分情况下，直接走本地缓存据可以找到hash slot对应的node在哪\n        * 不再需要通过节点的moved重定向\n    \n"},"children":[]},{"data":{"id":"bxf1fpfusyo0","created":1570019422087,"text":"JedisCluster工作原理","note":"* JedisCluster工作原理\n\t* JedisCluster初始化时，随机选择一个node\n    \t* 初始化hash slot->noded的映射表（因为这些映射元数据在每个节点都有）\n        * 并为每个节点创建一个jedispool连接池\n        \n    * 每次基于jediscluster执行操作时\n    \t* 先计算出key对应的hashslot，并在映射表中找到对应的节点\n        \t* 如果该节点依旧持有该hash slot，则ok\n        \t* 如果进行了reshard（猜测是重新hash之类）导致hashslot不在原node上，返回 moved\n            \n        * 返回moved的话，jedisCluster就会利用该节点的元数据，更新本地的hash slot与node的映射缓存\n        \t* 重复操作，直至找到节点，超过5次报错：JedisClusterMaxREtirectionException\n\n* 如果hash slot正在迁移\n\t* 会返回ask重定向给jedis\n    \n    * jedis接到后，会重新定位到目标节点去执行\n    \t* 但是由于ask发生在hash slot迁移过程中\n        * 因此此时不会去更新hashslot本地映射缓存\n        \n    * 如果时moved的话，会更新缓存\n    \t* 因为此时已经确定hashslot已经迁移完"},"children":[]}]},{"data":{"id":"bxf1kyoxiug0","created":1570019834048,"text":"高可用和主备切换原理","note":"1. 判断节点宕机\n\t* 一个节点认为另外一个节点宕机\n    \t* 是pfail 主观宕机\n    * 多个节点认为另外一个节点宕机\n    \t* fail 客观宕机\n        * 跟哨兵原理几乎一致\n        \n    * 在cluster-node-timeout内，某个节点一直没有返回pong\n    \t* 被认为是pfail\n    * 一个节点认为某个节点pfail\n    \t* 会在goosip消息中，ping给其他节点\n        * 超过半数的节点认为pfail，就会变成fail\n        \n        \n2. master宕机后，就要选择一个从节点作为新的master\n\t* 从节点过滤\n    \t* 检查每个salve node与master node的断开连接时间\n        * 超过了cluster-node-timeout * cluster-slave-validity-factor。则没有资格\n        \n    * 从节点选举\n    \t* 每个从节点，根据自己对master复制数据的offset，设置一个选举时间\n        * offset越大，选举时间越靠前，优先进行选举\n        \n        * 所有master开始对slave投票。如果大部分master node（N/2 +1） 都投票给了某个从节点，则选举通过\n        \n        * 从节点执行主备切换，变成master\n        \n* 整个流程跟哨兵类似"},"children":[]},{"data":{"id":"bxfpfvpqo2w0","created":1570087142383,"text":"生产环境部署架构","note":"* 十台机器\n\t* 5主5从\n    \t* 每个主实例挂一个从实例\n        * 因此是高可用的，任何一个主实例宕机，都会故障迁移，从实例变master\n        \n        \n    * 5个节点对外提供读写服务\n    \t* 每个节点读写高峰QBS\n        * 5w/sec（可以说是压测的数据）\n    \n    * 机器配置\n    \t* 32G内存+8核CPU+1T磁盘\n        * 分配给redis的内存是10G（一般不能超过这个值）\n        \n\n* 往内存中写的什么数据\n\t* 每条数据的大小\n    * 以此算出使用了多少内存\n    \t* 常驻内存等\n        \n* 日常高峰\n\t* 3500/sec请求量\n    * 这些请求一般访问哪些接口？\n    \t* 提前准备。。。\n    "},"children":[]}]},{"data":{"id":"bxf209iqec00","created":1570021033080,"text":"缓存雪崩，缓存穿透","note":"1. 缓存雪崩\n\t* 整个缓存挂了，请求直接压在数据库。\n    * 雪崩，一般特指很多key到期。大量请求直接访问数据库，压死数据库\n    \n\t* 事前\n    \t* key的设置过期时间要隔开，不要太集中\n    \t* redis要是高可用，避免全盘崩溃：主从+哨兵 或者redis cluster\n        \n        * 系统内增加ehcache（有个系统内的小缓存）也算属于事中\n        \t* 处理请求可以先查看ehcache，没有再访问redis，再没有再查数据库\n            * 查到的数据放入ehcache和redis\n    \n    * 事中\n    \t* 有了ehcahe后，还得配合做限流+降级\n        * 使用hystrix框架实现\n        \n        * 可进行配置，每秒能通过限流组件的访问数\n        \t* 没法通过限流组件的请求，会调用你自己开发好的限流组件\n            * 一般是返回一些默认的值，如：友情提示等\n        \n        * 可以保证数据库不挂\n        \t* 则一部分请求还是可以被处理的\n     \n    * 事后\n    \t* redis做持久化\n        * 可以快速恢复\n\n* 缓存穿透\n\t* 解决：数据库中没有的值，也写到缓存\n    \t* 值是约定好的，代表不存在的值即可\n      \t* 则请求不会大量穿透到数据库"},"children":[]},{"data":{"id":"bxf2q7akdvs0","created":1570023065701,"text":"保证缓存与数据库的双写一致","note":"* 经典的缓存+数据库读写的模式\n\t* cache aside pattren \n    \n    * 读数据时，先读缓存\n    \t* 缓存没有数据，再读数据库\n        * 然后将数据放入缓存\n    \n    * 更新的时候，先更新数据库，再删除缓存\n    \t* 作者说的先缓存应该是记错了\n   \n   \n\n","expandState":"collapse"},"children":[{"data":{"id":"bxfk10i5c9c0","created":1570071871796,"text":"为什么是删除缓存，不是更新缓存","note":"   \n* 为什么是删除缓存而不是更新缓存\n\t* 因为缓存的数据可能需要经过复杂的计算\n    \t* 更新之后，不保证在下次更新前有请求来读取\n        * 这样就浪费一次计算了，复杂的计算的代价就更大\n        * 删除的话，体现一个lazy懒加载的思想。让它需要用到的时候再计算"},"children":[]},{"data":{"id":"bxfk1baz4o00","created":1570071895307,"text":"先更新数据库","note":" \n* 先更新数据库据，后删除缓存\n\t* [缓存删除失败]怎么办\n    \t* 没有任何处理的话，这样就导致了缓存数据是旧的\n        \n    * 此时其实是不能用事务控制的\n    \t* 假如事务在确认删除缓存成功才提交\n    \t* 则在删除成功，提交事务之前，还是有可能有其他读线程拿到了旧的数据\n    \n    * 使用MQ\n    \t* 将key放进消息队列，不断尝试删除，直至成功"},"children":[]},{"data":{"id":"bxfktlej83c0","created":1570074111487,"text":"先删除缓存","note":"* 先删除缓存的话（是可行的）\n\t* 在更新数据库，并提交事务之前\n    \t* 可能有其他查询获取了旧的数据，并放进缓存\n        * 则缓存的数据是旧的，数据库是新的，导致双写不一致\n    \n\t* 另外先删除缓存的好处，是即使更新数据库失败（不是主要问题）\n    \t* 再次获取缓存，还是数据库的内容，所以没有导致不一致\n\n\n\n* 解决\n    * 将 [删缓存+修改数据库]的操作以及[读数据库，写到缓存]的操作，进行串行化\n    \t* 全部放进一个队列中，并用一个线程进行逐个处理\n        * 队列（需要支持分布式，或者使用nginx hash路由到同一台机器）\n        * 使用内存队列，还有热点商品的请求倾斜的问题\n        \n    * 则在删除完数据库，修改成功之前，所有读数据库的请求将会排队等待\n        * 解决了不一致问题\n     \n* 优化\n\t* 根据数据的唯一id，做hash\n    \t* 分给多个队列处理，减少锁的粒度\n     \n    * 多个读请求排队从数据库获取数据的地方\n    \t* 可判断队列前面假如已经有一个相同的读请求\n        * 则等待几十毫秒，看看缓存中是否有\n        \t* 有就获取缓存返回\n            * 没有再次等待，超过N次就直接读数据库的旧值返回（不放缓存）\n\n"},"children":[{"data":{"id":"bxflpqtxdxk0","created":1570076630955,"text":"注意","note":"* 注意\n\t* 读请求发现缓存没数据（需保证在超时时间内完成）\n    \t* 不能直接等待\n        * 要检查队列中有更新操作才行\n    \n    * 写操作不能太频繁\n    \t* 否则会导致大量的读操作长期阻塞+最终走数据库\n        \n    * 因此压测是很重要的 "},"children":[]}]}]},{"data":{"id":"bxfoz668j8w0","created":1570085832958,"text":"redis并发竞争问题","note":"* 使用分布式锁解决\n\t* 保证同一时间只有一个请求在操作\n    \n* 如果并发更新缓存时，有一些时序要求\n\t* 即旧数据可能会覆盖新数据\n    \t* 则加版本号，如时间戳等\n        * 暂时没想到使用场景。。\n        \n*  如最新版本是经过计算的，旧版的数据完全 没用\n\t* 则直接根据时间戳判断，缓存里面的数据是不是比当前要更新的更新\n    \t* 是的话就放弃本次操作"},"children":[]}]},{"data":{"id":"bxfqow4hluw0","created":1570090669663,"text":"分布式系统dubbo","expandState":"collapse"},"children":[{"data":{"id":"bxfrhnzgkts0","created":1570092924505,"text":"为什么使用分布式框架Dubbo","note":"* 为什么要拆分系统\n\t* 系统太大，多人开发维护。\n    \t* 代码冲突，合并等\n        * 不能单独进行技术升级等\n        \n    * 例：原好易借借贷产品，与理财、风控、账务等各个模块共用一份代码\n    \t* 使用非常痛苦\n\n* 怎么拆\n\t* 按照开发组、业务拆即可\n    \t* 后续继续可拆为存管相关，授信相关等"},"children":[]},{"data":{"id":"bxfu91ht66w0","created":1570100711965,"text":"原理","expandState":"collapse","note":null},"children":[{"data":{"id":"bxfsswbogps0","created":1570096625774,"text":"分层","expandState":"collapse"},"children":[{"data":{"id":"bxfst38f6tc0","created":1570096640814,"text":"servicer层","note":"* servicer层，接口层\n\t* provider、comsumer接口\n    \t* 给服务提供者、消费者 实现 \n\t\t* 具体接口调用的业务实现"},"children":[]},{"data":{"id":"bxft41btjq80","created":1570097498672,"text":"config 层","note":"* 配置层\n\t* 对Dubbo进行各种配置"},"children":[]},{"data":{"id":"bxft5378nko0","created":1570097581113,"text":"proxy层","note":"* 服务代理层\n\t* 无论是provider还是comsumer\n    \t* 都会生成代理\n        * 代理之间进行网络通信\n        \n\t* 透明生成客户端的stub和服务单的skeleton\n    \t* 待了解。。\n        \n    * 服务提供者的代理监听网络请求，消费者发起请求等"},"children":[]},{"data":{"id":"bxftafa6vjs0","created":1570097999233,"text":"registry层","note":"* 服务注册层\n\t* 负责服务的注册以及发现\n    \t* provider注册自己作为一个服务\n        * comsumer去注册中心寻找自己调用的服务"},"children":[]},{"data":{"id":"bxftrui8crk0","created":1570099364562,"text":"cluster层","note":"* provider可以部署在多台机器上\n\t* 多个provider组成一个集群\n    \n* 封装多个服务提供者的路由\n\t* 并做负载均衡\n    * 将多个实例组合成一个服务"},"children":[]},{"data":{"id":"bxftu2om9480","created":1570099539091,"text":"monitor层","note":"* 监控层\n\t* consumer调用provider，对调用次数，调用时间等统计信息进行监控"},"children":[]},{"data":{"id":"bxfu3sejvs00","created":1570100300356,"text":"protocol层","note":"* 远程调用层\n\t* 封装rpc调用\n    * 负责具体的provider和comsumer之间调用接口的网络通信"},"children":[]},{"data":{"id":"bxfu5m9l1hk0","created":1570100443723,"text":"exchange层","note":"* 信息交换层\n\t* 信息交换\n    * 封装请求响应模式\n    \t* 同步转异步"},"children":[]},{"data":{"id":"bxfu7r1yl2o0","created":1570100610875,"text":"transport层","note":"* 网络传输层\n\t* 抽象mina和netty为统一接口"},"children":[]},{"data":{"id":"bxfu6pmbaow0","created":1570100529388,"text":"serialize层","note":"* 数据序列化层"},"children":[]}]},{"data":{"id":"bxfvibw4gxk0","created":1570104260986,"text":"工作流程（注册）","note":"1. 启动时，服务提供者和消费者与注册中心交互\n\t* 服提供者注册其提供的服务\n\t* 服务消费者获取服务提供者地址列表（缓存到本地）\n    \t* 调用时根据负载算法直接调用提供者\n        \n        \n2. 注册中心通过长连接感知服务提供者的存在\n\t* 服务提供者宕机，注册中心将立即推送事件通知消费者\n    \t* 消费者刷新本地对服务注册信息的缓存\n        \n3. comsumer 和 provider都异步的通知监控中心\n\n\n4. 注册中心挂了\n\t* 可以继续通信\n    * 短期内不影响\n    \t* 启动dubbo时，消费者会从zk拉取注册的生产者的地址接口等数据，缓存在本地。\n        * 每次调用时，按照本地存储的地址进行调用\n        \n\t* 倒是无法从注册中心去同步最新的服务列表\n    \t* 短期的注册中心挂掉是不要紧的，但一定要尽快修复"},"children":[]},{"data":{"id":"bxfvqjq1b200","created":1570104904945,"text":"网络通信协议","note":"* 通信协议\n\t* 一般就是请求时加在地址前的\n    \t* 如： http://  dubbo://\n        \n        \n* dubbo支持的网络通信协议\n\n1. Dubbo协议\n    * 默认的协议\n    \n    * 单一长连接，NIO异步通信\n        * 每个消费者与每个生产者（有服务调用的）建立一个长期保持的连接\n        \t* 基于这个连接不断发送请求\n        * NIO异步通信，性能很高\n            \n    * 基于hessian作为序列化协议\n        * 序列化就是将数据序列化为一些可以通过网络传输格式（因为此处用于网络传输）\n        \t* 如二进制等\n        * 接收到之后再反序列化\n        \n    * 适用场景\n    \t* 传输的数据量少(100k内)，但是并发量高\n        \t* 长连接可以避免经常创建、销毁连接\n            * 但是如果数据量太大的话，会导致并发能力降低\n     ","expandState":"expand"},"children":[{"data":{"id":"bxfw7mjgggw0","created":1570106243268,"text":"其他协议","note":"* 其他的协议用的都不多\n\t* 有道详细笔记\n\n* rmi协议（一般少用）\n\t* Java标准二进制序列化\n    * 多个短连接\n    * 适合场景\n    \t* 提供者跟消费者差不多\n    \t* 文件传输\n        \n* hessian协议（一般少用）\n\t* Hessian二进制序列化\n    * 多个短连接\n     * 适合场景\n    \t* 提供者比消费者还多\n    \t* 文件传输\n        \n* http协议\n\t* 走json序列化\n    \n*  webservice\n\t* SOAP文本序列化"},"children":[]}]},{"data":{"id":"bxfwdeuj8o80","created":1570106696709,"text":"序列化协议","note":"* dubbo基于不同的通信协议\n\t* 支持hessian、java二进制序列化、json、SOAP文本序列化等多种序列化协议\n    \t* hessian时默认的序列化协议"},"children":[]}]},{"data":{"id":"bxfwlk5t0kw0","created":1570107335188,"text":"负载均衡策略","note":"* 负载均衡 loadbalance\n\n1. random 随机\n\t* 默认的策略（一般就用这个即可）\n    \t* 正常每台机器接收的请求数差不多\n    * 另外可以对provider设置不同的权重\n    \t* 权重越大，随机的概率大一点\n         \n         \n2. roundrobin\n\t* 轮询\n    \t* 均匀的将流量打到各个机器上\n    \t* 性能有差异的话可以设置权重\n\n      \n3. leastactive\n\t* 最少活跃。 自动感知一下\n    \t* 如果某个机器性能较差，能处理的请求较少，不是很活跃\n        * 则分配更少的请求\n  \n  \n4. consistant hash\n\t* 一致性hash算法\n    \t* 相同参数的请求一定分发到一个provider\n        * provider挂了的话，会基于虚拟节点，均匀分配剩余的流量，抖动不会太大\n        \n    * 有些用处\n    \t* 如某些id的数据，必须分发到指定的机器\n        * 就用这种策略"},"children":[]},{"data":{"id":"bxfx7dtlek80","created":1570109045400,"text":"集群容错策略","note":"* 请求某一台机器时\n\t* 发现机器挂了（经过多次重试）\n    * 将请求发到其他机器\n    \n* failover （默认）\n\t* 失败自动切换\n    \t* 自动重试其他机器\n    \t* 常见于读操作\n    \n* failfast\n\t* 一次调用失败后立即报错\n    \t* 常见于写操作\n\n* failsafe\n\t* 出现异常时忽略掉\n    \t* 常用于不重要的接口调用，如记录日志（失败的话也是打一下日志这样）\n  \n* failback\n\t* 失败了后台自动记录请求\n    \t* 定时重发\n        * 适合写消息队列这种\n\n* forking\n\t* 并行调用多个provider\n    \t* 只要一个成功就立即返回\n        * 其他慢的就不要了\n \n* broadcacst\n\t* 广播调用所有provider\n    \t* 任意一个报错就报错\n        * 常用于通知所有provider更新缓存或日志等本地资源信息\n  \n"},"children":[]},{"data":{"id":"bxfxtpul31s0","created":1570110795593,"text":"动态代理策略","note":"* 默认使用javassist动态字节码生成，创建代理类\n\t* 可以通过spi扩展机制配置自己的动态代理策略（待了解。。）"},"children":[]},{"data":{"id":"bxfxwiipiso0","created":1570111014730,"text":"spi思想","note":"* service provider interface\n\t* 服务提供接口\n    \t* 一个接口有多个实现类\n        * 可以配置使用指定的实现类\n        \t* 系统根据配置，加载对应的实现类，使用此类的实例对象\n        \n        * 可用于插件扩展等场景，如java中的JDBC也是类似思想（但是JDBC不是SPI机制）\n        \n    \n    \n* JDK中提供了这样一个功能\n    * 在自己jar包的META—INF/services/目录下，放一个跟接口同名的文件\n    \t* 里面指定接口的实现，是自己这个jar包里的某个类\n      \n      \n    \n\n            ","expandState":"collapse"},"children":[{"data":{"id":"bxfyjtf6am00","created":1570112840837,"text":"dubbo中的使用","note":"* dubbo中\n    * 如系统运行时，dubbo需要判断使用Protocol接口的那个实现类取实例对象\n    \n    * Protocol prococol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension()\n    \t* 执行上面代码，就会去找对应的配置\n        * 获取实现类，实例化对象使用\n        \n    * 微内核，可插拔，大量的组件\n    \t* 可使用自己的组件实现功能，如RPC\n        \n* 实现\n\t* 接口上加上注解\n    \t* @SPI(\"dubbo\") 表示通过SPI机制来提供实现类\n        * 实现类通过dubbo作为默认的key去配置文件里找\n        * 配置文件名于接口全名一致\n\t* 在/META_INF/dubbo/internal/com.alibaba.dubbo.rpc.Protocal 文件中\n    \t* 里面就是key=类全名的写法\n        * 如dubbo = com.ali...Protocol\n\n* 动态替换默认的实现类\n\t* 该接口（加了spi注解的）中\n    \t* 有@Adaptive注解的方法\n        \n        * 运行时会生成代理类，代理类中 有 此带有@Adaptive注解的方法\n        \n        * 代理类会根据传进来的key，去找对应的实现类，没的话就是默认值dubbo了"},"children":[]}]},{"data":{"id":"bxgb8l6j9m80","created":1570148635323,"text":"服务治理","note":"* 服务治理\n\t* 调用链路图的自动生成\n    * 服务访问压力及时长统计\n    \t* TP50\n        \t* 表示50%的请求\n        * 如TP99的请求延迟\n        \t* 就是99%的请求延迟是多少\n        * 接口每日调用次数\n        * 全链路（源头到各个服务调用完）每天走多少次\n        * 他们的TP50，TP90.TP99的请求延迟分别是多少\n    \n    * 其他\n    \t* 服务分层\n        \t* 避免循环依赖（上层依赖下层，service依赖service出现循环依赖等）\n        * 调用链路失败监控、报警\n        * 服务鉴权\n        \t* 例如 当前服务只能给让指定的服务消费者调用，其他调用不允许等\n        * 每个服务的可用性监控\n        \t* 接口调用成功率\n \n \n\n \n* 可以说公司没有人力做\n\t* 但是可以讲一下自己对服务治理的一些了解"},"children":[]},{"data":{"id":"bxgbmsi2drs0","created":1570149748356,"text":"服务降级","note":"* 服务降级\n\t* 如服务A调用服务B\n    \t* 服务B挂了，A重试几次还是不行\n        * 降级，走备用逻辑（提示错误等）\n        \t* 给用户返回响应\n    \n* dubbo中\n    * 提供了MOCk实现，统一返回null\n    \t* 就是配置:mock=\"return null\"\n    * 实现自己逻辑\n        * mock值修改为true\n        * 在接口的同一路径实现一个MOCK类，规则是接口名称+Mock后缀\n        * 在类中实现自己的降级逻辑\n        \t* 继承原接口，重写方法\n        \n        \n* 失败重试、超时重试\n\t* 调用异常或调用超时。可以选择重试\n    * <dubbo：reference id='xx' interface ='xx' check='true' async='false' retries='3' timeout='2000'/>\n    \t* retries 重试次数\n        * timeout 超时时长（毫秒）\n        * 另外 check='true' 表示启动时是否检查服务是否可用"},"children":[]},{"data":{"id":"bxgcqesrspc0","created":1570152853095,"text":"幂等性","note":"* 分布式系统的幂等性（不单指Dubbo）\n\t* 数据有唯一id\n    \t* 这个不是重点\n    \n    * 数据库中能记录已经处理过此请求\n    \n    * 数据库事务提交前控制：\n    \t* 一般调用就是加锁，分布式就是加分布式锁\n    \t* 防止短时间内重复调用\n        \n        * 注：名称叫类似分布式锁即可，因为真正分布式锁是要等待锁的，此处防重仅仅是[redis做了一个标记]\n        \n        * 另外利用数据id的唯一索引冲突\n        \t* 只有不冲突才进行下一步等"},"children":[]},{"data":{"id":"bxgdgoq3yuw0","created":1570154912170,"text":"接口顺序性如何保证","note":"1. 首先，大部分的接口是不需要顺序性的\n\n2. 极为严格的场景才会用到\n\t* 将需要严格按照顺序执行的请求，分配一个相同的标志，如订单id\n    \t* 利用一致性hash的负载均衡策略\n        * 相同id 的请求分配到指定的机器进行处理\n   \n    * 到达机器后，如果是多线程处理\n    \t* 还是用内存队列进行处理\n        *  队列可以有多个，相同数据id的放一起，单线程按顺序处理即可\n        \n    * 要求请求本身进来的时候是按顺序的才行\n    \t* 一般都ok\n        \n        \n    * 否则的话，100%的顺序性要求\n    \t* 分布式锁+ 请求加序号字段标记\n\t\t* 序号在后面的拿到锁，先释放锁，隔一会重试。直到前面的步骤已经处理完\n        \t* 基于分布式锁的机制，可以不等一会，如zookeeper中，监听下一个节点的锁释放\n            \n        * 加锁一般都比较耗性能\n        \t* 可以考虑将请求发给MQ进行异步化，先返回响应给用户"},"children":[]},{"data":{"id":"bxgeq2jh9qw0","created":1570158468632,"text":"如何设计一个类似dubbo的分布式rpc框架","note":"1. 首先，服务需要一个注册中心去注册接口信息\n\t* 可以使用zookeeper\n    * 服务提供者注册服务，消费者去拿对应的服务信息\n\n2. 拿到服务信息之后\n\t* 需要发起一次请求， 使用动态代理\n    * 服务提供者的代理\n    \t* 监听网络请求\n    * 服务消费者的代理\n    \t* 实现负载均衡\n        \n    * 发送请求\n    \t* 怎么发送\n        \t* 可以netty nio的方式\n            \n        * 发送的数据什么格式\n        \t* 如hession序列化协议\n\n3. 另外可以讲讲\n\t* SPI提供扩展\n    * 各种降级、超时等"},"children":[]}]},{"data":{"id":"bxgf045k9w80","created":1570159255785,"text":"zookeeper","expandState":"collapse"},"children":[{"data":{"id":"bxgf5og2k2g0","created":1570159691777,"text":"分布式协调","note":"* 分布式协调\n\t* 如A系统发送消息到MQ\n    \t* 想要知道B系统最终是不是消费成功\n    * A系统可在Zookeeper创建一个节点\n    \t* 并监听它的值变化\n        * B系统处理完之后修改值\n        * 则A系统可监听执行结果"},"children":[]},{"data":{"id":"bxgfcac33tc0","created":1570160209610,"text":"分布式锁","note":null},"children":[]},{"data":{"id":"bxgg2e2nde80","created":1570162255215,"text":"配置信息管理","note":"* 元数据的信息管理\n\t* 大数据的一些框架strom，kafka\n    * Dubbo的注册中心\n    \n    * 统一存放，提供给其他系统调用，存取。"},"children":[]},{"data":{"id":"bxgj9dul4wo0","created":1570171266607,"text":"HA高可用性","note":"* 感知机器是否存活\n\t* 挂了就切换为备用的系统"},"children":[]}]},{"data":{"id":"bxgkl9r3zxc0","created":1570175019169,"text":"分布式锁","expandState":"collapse"},"children":[{"data":{"id":"bxgkmuiak800","created":1570175142713,"text":"redis","note":"* redis分布式锁：RedLock\n\t* 官方支持的分布式锁算法\n    \n* 三个重要的考量点\n\t* 互斥\n    \t* 只有一个客户端获取锁\n        \n    * 不能死锁\n    \n    * 容错\n    \t* 大部分redis节点获取了这个锁就可以了\n        \n        \n* 未解决的问题\n\t* 设置锁过期时长为多大\n    \t* 太长，其他锁等待时间就长\n        * 太短，还没处理好就释放锁了\n     \n    * 需要手动自旋去获取锁\n    \t* 耗费CPU性能\n        \n    * redlock算法目前还是有一些国外大牛有质疑\n        "},"children":[{"data":{"id":"bxgkt3s8oq80","created":1570175633090,"text":"最简单的,基于NX实现的","note":"* 基于NX命令(not exists)\n\t* set mykey 随机值 NX PX 30000\n    \t* mykey 不存在才能插入成功\n        * 同时设置过期时间\n        \n    * 拿不到锁的，自旋获取\n    \n\n\n\n* 锁释放\n    * 加锁的值使用随机数，用于解锁\n    \n    * 只有是自己的锁才可以释放\n        * 避免极端情况下锁因为时间到了自己释放，第二个线程刚刚加锁，就被第一个线程释放了，导致锁完全失效\n        \n    * 如果要加上判断锁是否是自己的\n    \t* 一般不要直接判断后删\n        \t* 有原子性问题\n            \n        * 使用lua脚本删\n        \t* 可以实现原子性\n\n* 缺点\n\t* 单点系统\n\t\t* 故障后不可用\n\n\t* 主从\n\t\t* 主从复制有延迟，故障后主从切换会丢失数据。锁就不安全了"},"children":[]},{"data":{"id":"bxgldr0e2zc0","created":1570177250932,"text":"RedLock，基于redis cluster","note":"1. 获取当前时间戳，单位毫秒\n\n2. 轮流尝试在每个master节点上面创建锁\n\t* 尝试获取锁的操作设置较短过期时间\n    \t* 一般几十毫秒，即几十毫秒拿不到就当失败\n\n3. 大多数节点上成功就算成功\n\t* 超过一半\n    * cluster是分片的，为什么可以在几个master上面获取锁？\n    \t* 因为分片其实是人为的hash，将key分散给对应的master\n        * 使用锁时，单独去每个set是没问题的\n     \n    * 优点\n    \t* 即使小部分master挂了，也不影响锁的使用\n\n4. 建立好锁之后，根据步骤一中的时间，算出总耗时时间\n\t* 小于超时时间，则获取锁成功\n    * 否则失败\n    \t* 失败则依次释放这个锁\n\n5. 获取锁失败的\n\t* 需要不断轮询尝试获取锁"},"children":[]}]},{"data":{"id":"bxgporud9d40","created":1570189399190,"text":"zookeeper","note":"1. 对于需要加锁的资源\n\t1. 先在zookeeper指定父节点注册\n\n\t2. 创建的节点是临时有序的\n\n\t3. 获取父节点下所有子节点，并设置监听\n        * 只有一个：获取锁，执行业务，删除节点（finally），重新创建节点\n       * 不止一个：可判断是否为最小节点\n       \n    4. 不是最小，继续监听\n    \t1. 监听到有节点删除事件，获取所有子节点（并继续设置父节点监听）\n        2. 获取所有子节点，排序（Collection.sort()）\n        3. 本节点最小：获取锁，执行业务，删除节点（finally），重新创建节点\n        4. 不是最小，继续监听（循环）\n\n\n2. 太多节点监听父节点\n\t* 会有羊群效应\n\n* 改进\n    * 获取父节点所有子节点，判断自己是不是最小的节点（不监听父节点）\n    \t* 是：获得锁\n    \t* 不是：获取比自己小的节点，判断是否存在，并监听\n        \n     * 当比自己小的节点使用完，删除后，会触发监听\n    \n    * 每个节点都是监听比自己小的那个节点，不会一次触发全部，避免了羊群效应。\n        \n   "},"children":[{"data":{"id":"bxgqefpytlk0","created":1570191410271,"text":"优点","note":"1. 不需要轮询\n\t* 性能开销小\n  \n2. 没有超时时间bug\n\n* 缺点\n\t* zookeeper写是写磁盘的，速度上比不山redis"},"children":[]}]}]},{"data":{"id":"bxgr0iablig0","created":1570193139867,"text":"分布式session","note":"1. 基于Tomcat RedisSessionManager\n\t* 修改tomcat配置，将tomcat的session交给redis维护\n    * 缺点\n    \t* 依赖web容器\n        \n2. spring session + redis\n\t* 给spring session配置基于redis存储session数据\n    \n    * 然后配置一个spring session过滤器\n    \t* 然后session的相关操作都会交给spring session来管了\n        * 代码中直接用原生的session操作即可\n        \t* 操作会基于spring session从redis中获取数据"},"children":[]},{"data":{"id":"bxgrhkyji7k0","created":1570194477876,"text":"分布式事务","note":"* 你们公司怎么解决\n\t* 严格资金情况：TCC\n    * 一般场景：RocketMQ","expandState":"collapse"},"children":[{"data":{"id":"bxgrsk6as400","created":1570195338174,"text":"两阶段/XA方案","note":"* 先执行sql，都暂不提交事务\n\t* 都不报错的话，提交事务\n    * 其中一个报错，回滚\n    \n    * 等待每个sql的执行结果\n    \t* 效率太低\n    \n    * 一般用于一个系统跨多个库的分布式事务\n    \t* 这种情况本身就是不合规的\n        * 应该拆分为多个微服务\n    \n* 基于一个事务管理器去控制多个事务的状态\n\t* spring + JTA （未了解。。）"},"children":[]},{"data":{"id":"bxgscrligbk0","created":1570196921614,"text":"TCC方案","note":"* try、confirm、cancel\n\t* 使用补偿的概念\n\n\t* try\n    \t* 对各个服务资源做检测，以及对资源做锁定或预留\n    * confirm\n    \t* 在各个服务中执行实际的操作\n    * cancel\n    \t* 任何一个服务的业务方法出错，就需要补偿\n        \t* 即已经执行成功的业务逻辑进行回滚\n     \n\n* 例子\n\t* 转账\n    \t* try阶段，先预授权冻结2个系统的金额\n        * confirm，转账\n        * cancel，回滚，把原来的加减的金额还原\n        \n\n* 缺点\n\t* 补偿的逻辑需要逐个开发，没有通用性。\n    \t* 跟业务重耦合\n\n* 优点\n\t* 强一致性\n    \t* 钱相关的业务\n    \n* 为什么不逐个执行，事务都不提交，逐层嵌套的方式？\n\t* 因为本身事务就要尽量短，否则占用数据库资源，并且容易造成死锁等\n    * 与XA类似，性能相当差，甚至比XA还多了网络传输"},"children":[]},{"data":{"id":"bxgt17vq3h40","created":1570198837800,"text":"本地消息表","note":"* 基于数据库消息表\n\t* 例:A系统先执行业务表，再执行消息表\n    \t* 然后发MQ给B系统\n        * 并在zookeeper监听指定节点获取B系统状态\n     \n    * B系统先执行消息表，再执行业务表\n    \t* 先执行消息表是为了防重\n        \n    * B系统执行完，修改zk指定节点状态\n    \t* A系统监听到后，更新消息状态（成功，失败不更新）\n        \n        * 定时任务将未成功的重推\n\n* 缺点\n\t* 严重依赖数据库的消息表\n    \t* 高并发情况下对数据库压力大\n        \n* 是国外的ebay搞出来的这一套思想\n        "},"children":[]},{"data":{"id":"bxgtazd12g00","created":1570199602897,"text":"可靠消息最终一致性","note":"* 基于可靠消息\n\t* 如RockeyMQ\n    \t* 保证消息不丢\n        * 轮询prepared状态机制等\n\n1. A系统发送一个prepeared消息到MQ\n\t* 发送失败\n    \t* 则直接结束\n    * 成功\n    \t* 执行本地事务\n        \t* 成功，告诉MQ，发送确认消息\n    \t\t* 不成功，mq回滚消息\n\n2. 发送确认消息后\n\t* B系统回收到mq消息\n    \t* 执行本地事务\n\n3. mq自动定时轮询所有prepared消息\n\t* 防止A系统执行成功，发送确认消息失败\n    \n4. B系统失败\n\t* 不断重试，直至成功\n    * 实在不行，报警，人工处理"},"children":[]},{"data":{"id":"bxgu0r56duo0","created":1570201622477,"text":"最大努力通知方案","note":"* 最大努力通知\n\t* A系统本地事务执行完毕后\n    \t* 发送消息到MQ\n    * 最大努力通知服务\n    \t* 消费MQ消息，保存起来（数据库）\n        * 不断通知B系统，直至成功或达到N次后放弃（报警，人工处理）\n        \n        \n* 一般用于对事务要求不严格的情况\n\t* 最后没处理也可以之类"},"children":[]}]},{"data":{"id":"bxh62643y740","created":1570235586747,"text":"如何设计一个高并发系统","note":"* 类似如何设计一个QBS上万，但是响应时间在200ms的接口等\n\n* 要秀出所有高并发的知识\n\n* 系统访问达到峰值的表现\n\t* 即请求访问的压力已经快到系统的上限的表现\n    \t* 内存吃紧，内存使用率上升\n        * 高峰期CPU负载增强（甚至达到60%~80%）\n   \n1. 系统拆分\n\t* 首要做的事情\n    * 例如使用dubbo，拆分为多个系统，每个系统对应一个数据库\n    \n2. 使用缓存\n\t* 主要是解决大量的读请求\n    \t* 大部分高并发都是读多写少\n        * 读尽量走缓存\n    * 缓存可以轻松抗几万的并发\n    \n3. MQ\n\t* 对于大量的写请求\n    \t* 如果是一定时间内的高峰期\n        * 使用MQ削峰，大量写请求暂时积压在MQ里，异步去处理\n        \t* 此处写直接是整个事务没开始，没有数据一致的问题\n            * 客户端响应用户可以说是订单提交成功，正在处理中，稍后隔几秒可以进行轮询\n         * 削峰不能削的太厉害\n \n4. 分库分表\n\t* 分摊并发请求\n\n5. 读写分离\n\t* 一定要走数据库的查询多的话\n    * 数据的实时性要求不高的场景\n    \n6. es（ElasticSearch）\n\t* 全文搜索类的走es"},"children":[]},{"data":{"id":"bxh7wlz107k0","created":1570240793303,"text":"分库分表","note":"* 单表数据到了几百万就差不多了\n\t* 再多性能就会太差\n    \t* 上千万基本是要做拆分了\n    * 高并发场景下也扛不住这么大的压力\n\n* 解决的问题\n\t* 并发问题\n    * 单表数据量太大，查询性能问题\n    \n    \n\n\n* 一般说分库分表都是讲水平拆分\n\t* 垂直拆分一般在数据库设计的时候就做了\n    \n* 水平拆分数据分发的策略\n    * 根据某个字段来拆分\n    * hash分发\n    \t* 比较平均的分配\n        * 缺点是扩容时要重新hash，数据迁移\n        \n    * range范围分发，如时间范围\n        * 不常用。容易产生热点问题\n        * 优点：容易扩容","expandState":"collapse"},"children":[{"data":{"id":"bxh8n2jswoo0","created":1570242866856,"text":"分库分表中间件","note":"* proxy层中间件\n\t* 独立部署的中间件\n    \t* 客户端通过中间件操作访问数据库\n\n* client层\n\t* jar包\n    \t* 直接在业务系统里对数据进行分发\n\n","expandState":"collapse"},"children":[{"data":{"id":"bxhad28fofc0","created":1570247724747,"text":"较少使用","note":"\n1. cobar（基本没人用了）\n\t* 阿里b2b团队开发，开源\n    \t* proxy层方案\n    * 不支持读写分离，跨库join，分页、存储过程\n\n2. TDDL\n\t* 使用不多\n    \t* 因为还依赖淘宝的diamond配置管理系统 \n\t* 淘宝团队开发\n    \t* client层方案\n    * 不支持join，多表查询\n    \t* 用于一般的curd\n        * 支持读写分离\n\n3. atlas\n\t* 360开源\n    \t* proxy层方案\n    * 社区没怎么更新了，用的公司也少了"},"children":[]},{"data":{"id":"bxhaekfx4dk0","created":1570247842746,"text":"常用","note":"1. sharding-jdbc\n\t* 当当开源\n    \t* client层方案\n    * sql语法支持较多\n    \t* 支持分库分表\n        * 读写分离\n        * 分布式id生成\n        * 柔性事务\n        \t* 最大努力送达事务，TCC事务\n    * 社区较为活跃 \n    * 比mycat成熟\n     \n    * 优点\n    \t*  client层，不需要部署\n        * 不需要代理层的二次转发请求，性能较高\n    * 缺点\n    \t* 系统耦合sharding-jdbc\n        * 升级版本等需要各个系统都重新升级版本发布\n     \n     \n     \n2. mycat\n\t* 基于cobar改造\n    \t* 数据proxy层方案\n    * 功能较为完善，社区活跃\n    * 优点\n    \t* 对各个系统、项目透明，升级之类自己搞就可以\n    * 缺点\n    \t* 需要独立部署，运维\n        \n    \n* 比较\n\t* 中小型公司使用sharding-jdbc\n    \t* client层轻便。维护成本低\n        * 大公司使用也是没问题de\n    * 大型最好选用mycat这种proxy层方案\n    \t* 人员充足，可专人维护mycat\n        * 大量项目直接透明使用\n    "},"children":[]}]},{"data":{"id":"bxhcyjygxpk0","created":1570255051194,"text":"数据库如何拆分","note":"* 垂直拆分\n\t* 把一个很多字段的表拆分成多个表，或者多个库\n    \t* 每个库表的结构结构都不一样\n        * 每个库表包含部分字段\n        \n    * 一般会把较少的访问频率高的字段放到一个表\n    * 较多的，访问频率低的字段放到另外一个表\n        \n    * 因为数据库是有内存缓存\n    \t* buffer pool\n        * 经常访问的行的字段越少，在缓存里就可以缓存更多的行\n    \n    * 想象垂直的劈开表\n  \n  \n* 水平拆分\n\t* 想象水平的劈开\n    * 将一个数据量大的表拆分成多个库表\n    \t* 表的结构一样\n        * 数据根据一定规则分散到各个表\n    \n    * 优点\n    \t* 多个库，最高承载的QBS变大\n        * 单表数据量变少，sql执行的效率增加\n        * 每台机器的磁盘占用率降低\n    \n\n","layout_right_offset":{"x":-19,"y":8}},"children":[]},{"data":{"id":"bxhegnsws3s0","created":1570259291229,"text":"如何动态让系统切换到分库分表上","layout_right_offset":{"x":-2,"y":0},"note":"* 最low的，停机迁移\n\n* 不停机双写方案\n\t* 让系统同时写老库和新库\n    \n    * 后台程序将老库数据读出\n    \t* 发到分库分表中间件\n        * 分库分表的数据库中是否存在\n        \t* 不存在写入\n            * 已存在对比最后修改时间，看是否需要更新\n        \n    * 标准规范的数据库设计\n    \t* 都会有最后修改时间\n        * 根据此时间判断是都为最新数据\n    * 迁移完一轮，就执行一次检查\n    \t* 有差异的话，看是否要重新迁移\n        * 直到完全一致\n    * 一致后，只写新的分库分表即可"},"children":[]},{"data":{"id":"bxhf6gbhjyo0","created":1570261312407,"text":"如何设计可以动态扩容的分库分表方案","note":"* 停机扩容\n\t* 停机后，取出数据，重新hash分发到新库\n    * 不靠谱\n    \t* 单库单表迁移到分库分表，说明数据量还不是很大\n        * 但是分库分表扩容，数据量就肯定很大了\n        * 停机操作时间可能很长\n        \n    \n* 优化\n\t* 一开始做分库分表（就分个够）\n    \t* 就分为32个库，每个库32张表，共1024张表\n        * 无论是并发支撑还是数据支撑都肯定没问题了\n        \n        \n    * 数据库服务器可以只分为4个\n    \t* 数据库服务器中创建多个库\n        \n        * 承载并发的是指整个数据库服务器\n        \t* 大约2000qbs\n         \n        * 需要支撑更高并发、或磁盘容量不够等的时候\n        \t* 增加数据库服务器，并从旧的数据库服务器迁移部分库过来\n            \n\n* 优点\n    * 不需要将数据读出来，进行重新分发\n    \t* 没改变表的数量\n    * 整库迁移即可，dba有对应的工具，方便快捷\n    * 甚至还能缩容\n    \n\n\n        \n        ","expandState":"collapse"},"children":[{"data":{"id":"bxhg4oigr680","created":1570263994624,"text":"最多可扩容","note":"  \n* 最多可以扩展\n\t* 一个数据库服务器对应一个库\n    \n    * 甚至，以每个库中的表来扩\n    \t* 即上例中，最多可扩展至1024个数据库服务器\n        \t* 迁表\n        * 但是根据hash分发的规则可能有改动\n        \t* 如数据先到库，再hash到表等。   此处保留。。\n            \n      \t* 如果本身是直接对表进行hash，那就没有整个问题了 \n        \t* 但是代码估计还是要改，毕竟库变了\n            "},"children":[]},{"data":{"id":"bxhg8tgdikg0","created":1570264318839,"text":"hash分发","note":"* id对32取模\n\t* 获取库\n* 此时不要直接再对32取模获取表了\n\t* 防止集堆\n    * 可根据余数对32再次取模\n    \t* 获取表"},"children":[]}]},{"data":{"id":"bxhglekl2hc0","created":1570265305176,"text":"分库分表id主键问题","layout_right_offset":{"x":4,"y":-1},"note":"1. 简单设置id自增\n\t* 会有id冲突问题\n\n2. 统一使用全局唯一的专门用于生成主键的库\n\t* 创建专门用于生成id的一个表\n    * 拿到id之后来用\n    * 缺点\n    \t* 一个库，性能有瓶颈\n        * 并发不高，只是数据量大的话可以用\n\n3. UUID\n\t* 太长，做主键性能太差\n    \t* 普通索引也要记录主键\n        * 浪费磁盘，浪费内存\n    * 用于文件名。编号之类可以用\n    \n4. 系统当前时间\n\t* 高并发有问题\n    * 除非是结合其他用户id+业务字段等\n\n5. snowflake雪花算法\n\t* twitter开源的分布式id算法"},"children":[{"data":{"id":"bxhh2659pcg0","created":1570266619026,"text":"雪花","note":"* 64位的long型id\n\t* 64位的二进制\n    \n    * 第1位是0，固定\n    \t* 表示正数\n    \n    * 接着的41位\n    \t* 时间戳\n        \t* 做了一些处理、换算\n        \n    * 接着的5位\n    \t* 机房id\n        \n    * 接着的5位 \n    \t* 机器id\n        \n    * 最后12位\n    \t* 序号\n  \n* 最好是独立的一台机器提供分布式id生成服务\n\t* 传入自己的机房、机器id等\n    \t* 相同机房，相同机器，在同一毫秒进来\n        * 那么根据序号递增\n        \n    * 每台机器用一个也是可以的\n    \t* 但是一台机器就只能部署一台服务\n        \t* 正常的其实也是一台服务\n\t\n\n"},"children":[]}]}]},{"data":{"id":"bxhkkwchws00","created":1570276549944,"text":"读写分离","note":"* 解决读请求并发量高的情况\n\n* 虽然从库支持并行复制(5.6开始)\n\t* 但怎么说也会有一定延迟，毕竟是主库事务提交后才开始同步\n    \n* 除了机器配置问题，大事务等因素\n\t* 正常1000/s的并发，延迟一般几毫秒\n    * 2000/s的并发，延迟几十ms等\n\n\n            \n* 并行复制（缓解主从延迟）\n    * 5.6库并行\n    * 5.7开始有真正的并行\n        * 不操作同一个数据\n        * 分属不同事务等\n            \n\n* show slave status\n\t* seconds_behind_master\n    \t* 查看从库延迟多少毫秒\n        \n* 场景\n\t* 用于时效性要求不高的查询场景\n    \t* 时效性要求高的可以强制走主库\n        \n* 延迟解决具体措施     \n    1. 做分库，减轻压力\n    \t* 还需要做主从的话，同步压力也减少了\n    2. 并行复制\n    3. 时效性高的强制走主库\n    \t* 丧失读写分离意义\n","expandState":"collapse"},"children":[{"data":{"id":"bxhnui2bugw0","created":1570285765826,"text":"解决数据丢失","note":"* 解决数据丢失\n\t* 半同步复制（主要是解决数据丢失）\n    \t* semi-sync\n        * 强制将binlog同步给从库\n        \t* 从库写入本地relay log返回ack确认给主库，才认为操作完成\n      \n* 或者直接重启即可\n\t* 反正有redo log"},"children":[]},{"data":{"id":"bxhnwrjysgw0","created":1570285943211,"text":"并行复制","note":"* MySQL 5.6\n\t* 按库并行复制\n    \n    \n* MySQL 5.7\n\t* 同时处于prepare状态的事务\n    \t* 在备库执行时是可以并行的；\n\n\t* 处于prepare状态的事务，与处于commit状态的事务之间\n    \t* 在备库执行时也是可以并行的。\n    * 所有处于commit状态的事务可以并行。（源于MariaDB的组提交）\n    \t* 事务处于commit状态，表示已经通过了锁冲突的检验了。\n   \n   \n   \n* MySQL 5.7.22\n\t* 基于WRITESET的并行复制\n    \t* 对于事务涉及更新的每一行，计算出这一行的hash值，组成集合writeset。\n        * 如果两个事务没有操作相同的行，也就是说它们的writeset没有交集，就可以并行。\n        \n* 配置binlog-transaction-dependency-tracking\n\t* COMMIT_ORDER \n    \t* 5.7版本\n    * WRITESET\n    \t* 5.7.22版本\n    * WRITESET_SESSION\n    \t* 在WRITESET的基础上多了一个约束\n        \t* 即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序。"},"children":[]}]},{"data":{"id":"bxi1f9tplxc0","created":1570324067966,"text":"如何设计一个高可用系统","note":"* 高可用系统架构\n\t* 资源隔离\n    \t* 系统中，某一块出现故障\n        * 不会耗尽系统所有的资源\n        \t* 如线程资源\n            * 某块代码限制使用线程数的上限等\n        \n    * 限流\n    \t* 只允许部分请求进去系统。其他的拒接\n    * 熔断\n    \t* 某子服务挂了，后续服务则不进行访问\n        * 隔一段时间去看看是否已经恢复\n    * 降级\n    \t* 使用一些备用服务，暂时进行工作\n    \t\n    * 运维监控\n    \t* 监控+报警+优化\n\n\n\n\n* 基于Hystrix\n\t* 限流\n    * 熔断\n    * 降级\n    \n    * netflix开源","expandState":"expand","layout":null},"children":[{"data":{"id":"bxi2j2fxwww0","created":1570327186463,"text":"Hystrix","note":"* 分布式系统中，每个服务会调用很多其他服务\n\t* 其他依赖的服务故障是很正常的\n    \t* Hystrix可以让我们在分布式系统中对服务间调用进行控制\n        * 加入一些调用延迟或以来故障的容错机制\n        * 通过对依赖服务进行资源隔离\n        \t* 防止故障在整个系统中进行蔓延\n     \n    * 同时还提供故障时的降级机制\n    * 提高系统的可用性和稳定性\n    \n\n\n\n* 设计原则\n\t* 对依赖服务调用出现的延迟和失败\n    \t* 进行控制和容错保护\n   \n    * 阻止某一个依赖服务的故障在整个系统中蔓延\n    \t* 阻止任何一个依赖服务耗尽所有资源，如线程资源等\n       \n    \n    * 提供fail-fast（快速失败）和快速恢复的支持\n    \n    * 提供fall back 优雅降级\n    * 支持近实时的监控、报警及运维操作","layout_right_offset":{"x":-4,"y":172},"expandState":"expand","layout":null},"children":[{"data":{"id":"bxi4a06nu200","created":1570332118491,"text":"细节设计原则","note":"1. 阻止任何一个以来服务韩进所有的资源\n\t* 如线程资源\n    * 使用资源隔离技术，限制依赖服务的故障影响\n    \t* bulkhead 舱壁隔离技术\n        * swimlane 泳道技术\n        * circuit breaker 短路技术\n   \n   \n2. 避免去请求排队和积压\n\t* 使用限流+ fail fast快速失败来控制故障\n \n3. 提供fallback降级机制应对故障\n\n4. 通过实时的统计，监控，报警\n\t* 提高发现故障的速度\n    \n5. 实时的属性和配置热修改功能\n\t* 提高故障处理和恢复的速度\n    \n6. 对于依赖服务的调用\n\t* 保护其所有的故障情况\n    \t* 而不仅仅是网络故障等","layout":null},"children":[]},{"data":{"id":"bxi4i1kx9bk0","created":1570332748443,"text":"实现原理","note":"1. 通过HystrixCommand 或者 HystrixObservableCommand来封装对外部依赖的访问请求\n\t* 这个访问请求一般会运行在独立的线程中\n    \t* 即发起依赖请求用的是独立线程，达到资源隔离的效果\n        \n2. 对于超出我们设定的阈值的服务调用\n\t* 直接进行超时\n    \t* 默认为99.5%的访问时间\n        * 即99.5%的访问都可以在该时间范围内完成（的统计时间）\n        * 可以自己重新设置下\n    * 不允许耗费太长时间阻塞\n    \n3. 每一个依赖服务\n\t* 维护一个独立的线程池\n    \t\n    * 或者semaphore信号量\n    \t*  线程池或信号量已满时，拒绝对这个服务调用\n    \t\n4. 对依赖服务的调用进行统计\n\t* 成功次数\n    * 失败次数\n    * 拒绝次数\n    * 超时次数等\n \n5. 对某个依赖服务的调用失败超过一定阈值\n\t* 自行进行熔断\n    \t* 熔断时间内对服务的调用直接降级\n        * 一段时间内再自动尝试恢复\n\n6. 服务调用出现失败，被拒绝，超时，熔断（短路）等异常时\n\t* 自动调用fallback降级机制\n \n7. 对属性和配置的修改提供近实时的支持","layout":null},"children":[]},{"data":{"id":"bxiev04wars0","created":1570361975130,"text":"线程资源隔离技术","note":"1. 线程池隔离技术\n\t* 学术名称：舱壁隔离技术\n\t* 控制的是web容器线程的执行\n    \t* 如线程池满了，就执行线程池的拒绝策略\n        * 可以是自定义的fallback\n        \n    * 保障被调用服务异常时，web容器线程不会被阻塞住\n    \t* web容器的线程资源可以回收做其他事情\n    * 缺点\n    \t* 增加cpu开销，因为要多管理线程池\n        \n        \n* 信号量隔离技术\n\t* 区别于线程池技术\n    \t* 线程池的隔离，是用自己的线程去执行\n      \t* 信号量是直接作用于web容器的线程\n        \t* 让web容器的线程直接调用依赖的服\n            \n    * 线程池能更好的管理\n    \t* 如线程执行太久，就直接抛出timeout异常等\n        * 信号量是做不到的\n        \n    * 信号量由于不需要自己管理线程，也不用处理timeout等，性能相对会更好\n        \n      \n      \n      \n* 使用场景\n\t* 线程池适合绝大多数场景\n    * 信号量\n    \t* 一般不用于对外部依赖的访问\n        \n        * 而是对内部一些复杂业务逻辑的访问进行限制\n        \t* 因为不需要控制网络请求的调用、访问出现的类似timeout的这种问题\n    ","layout":null},"children":[]},{"data":{"id":"bxig865pahs0","created":1570365828084,"text":"线程池、接口、服务划分","note":"* 每个command\n\t* 都可以设置一个自己的名称\n    \t* commandkey\n        * 不设置默认是类名\n        \n    * 同时设置一个自己的组\n    \t* groupkey\n    \t* 组名是一定要设置的\n    * 一般是一个依赖服务的一个接口\n   \n\n* command group\n\t* 默认通过command group来定义一个线程池\n    \t* 以及根据command group聚合一些监控和报警信息\n    \n    * 同一个command group\n    \t* 进入同一个线程池\n        \n    * 代表一个依赖服务\n    \t* 逻辑上组织了一堆command key的调用\n        * 统计信息，成功次数，timeout超时次数，失败次数等\n        \t* 可以看见一个服务的一些整体访问情况，如访问QBS等\n            \n        * 如果想细粒度的隔离，则可划分多个线程池，但是使用同一个组\n        \t* 同个组可以放在一起统计\n            * 但是使用不同的线程池做资源隔离和限流（如接口访问量差异很大） \n     * 一般一个服务划分一个线程池\n   \n\n* command 线程池\n\t* threadpool key代表了一个HystrixThreaPool\n    \t* 用于进行统一监控，统计，缓存\n    * 默认的threaPool key就是command group名称  \n    \t* 每个command都会跟它的threadPool key对应的thread pool绑定在一起","layout":null},"children":[]},{"data":{"id":"bxii2s51g4g0","created":1570371047968,"text":"资源大小配置","note":"* 线程池\n\t* coreSize\n\t\t* 线程池大小，默认10 ，可配置\n    \n\t* queueSizeRejectionThreshlod\n\t\t* 队列大小，默认5\n    \t* hystrix中此线程池队列满了触发拒绝策略\n        \n        \n* 信号量\n\t* maxConcuirentRequest\n    \t* 最大最大并发量\n        * 默认10 超过的请求直接拒绝","layout":null},"children":[]},{"data":{"id":"bxiw14x2oig0","created":1570410414595,"text":"command执行的步骤","note":"* command开始执行时\n\n1. 此command是否开启请求缓存 request cache\n\t* 已开启，且调用的结果在缓存中有\n    \t* 直接从缓存中返回结果\n    \n    \t* 假如需要请求同一个command，且参数一样，则直接从请求缓存中取\n        \t\n        \n2. 此command是否开启短路器\n\t* circuit breaker\n\t* 已开启，不执行，直接调用fallback降级机制\n    \n    \n3. 检查对应的线程池，队列，semphore是否满了\n\t* 满了就调用fallback机制\n    \n4. 开始执行command\n\t* 如果是线程池隔离技术\n    \t* 有timeout的机制，超过该时长，抛出一个TimeoutException\n        * timeout也会触发去fallback降级，后面返回的结果也不要了\n        \n        * 但是已经在执行的线程，是无法中止的，只是提前返回超时异常给web容器线程\n        \n5. 执行command报错，也会调用fallback机制\n      ","layout":null},"children":[{"data":{"id":"bxiyg9w457c0","created":1570417243103,"text":"请求缓存","note":"* 可以判断当前结果是不是从缓存中取得\n\t* isResponseFromCache\n            \n    * 有需要可以根据key删除缓存\n    \t* clear\n        \n          \n* 需要自己重写getCachekey方法\n    * 重写就表示开启请求缓存\n        \n     * 基于request context\n        * 创建，销毁是要自己去做的\n        \t* 如在filter中，每次请求施加一个请求上下文\n            \n        * 即对于一次请求，才执行取缓存操作（应该时最大作用范围）\n            \n            \n* 用途\n\t* 防止请求中没有去重等\n        "},"children":[]}]},{"data":{"id":"bxiwn2hykk80","created":1570412133339,"text":"短路健康检查","note":"1. Hystrix 会将一个依赖服务的调用事件，发给circuit breaker短路器\n\t* 调用成功、失败、超时等\n    \n    * 短路器会对各种事件调用次数进行统计\n    \t* 根据统计结果决定是否要打开短路器\n        * 打开的话，一段时间内的调用直接短路\n        * 在之后的第一次检查发现调用成功，则关闭短路器"},"children":[]},{"data":{"id":"bxiwu05vtb40","created":1570412676804,"text":"fall back 降级机制","note":"1. 一般返回一些默认值\n\t* 静态的代码逻辑等\n\t* 或者从内存中的缓存提取一些数据返回\n    \t* 如之前说redis挂了，可以尝试在内存缓存ehcache中看看有没有数据\n        \n    * 此时尽量不要进行网络请求了\n        \n        \n* 实现fallback\n\t* HystrixCommand\n    \t* 重写getFallback方法\n        \n    * HystrixObservbleCommand\n    \t* 重写resumeWithFallback\n\n* 控制fallback的最大允许请求并发数量\n\t* MaxConcurrentRequests\n    * 默认10\n    \t* 超过这个值，直接拒绝\n    * 通过信号量实现de\n\n2. 降级的逻辑中\n\t* 如果一定要网络调用\n    \t* 也要放在一个HystrixCommand中进行隔离\n        \n\n\n\n3. 没有实现fallback的话，或者fallback出现异常\n\t* executre()\n    \t* 直接抛出异常\n    * queue()\n    \t* 返回一个future，调用get时抛出异常\n    * observe()\n    \t* 返回一个Observable对象\n        * 调用subscribe()订阅它时调用onError()\n        \n    * toObserveable()\n    \t* 返同上"},"children":[]},{"data":{"id":"bxizd5kiwig0","created":1570419819712,"text":"短路器工作原理","note":"* 以下几个参数（包括子节点笔记）都是HystrixCommandProperties中设置\n\n1. 经过短路器的流量达到一定阈值（默认10s内20个请求）\n\t* ciruitBreakerRequestVolumeThreshold\n    * 达到阈值才会进行下一步的判断\n    \t* 不到阈值不会短路 \n    * 执行command就会经过短路器\n\n\n\n2. 短路器统计到的异常占比达到一定阈值\n\t* ciruitBreakerErrorThresholdPercentage\n    \t* 默认50%\n        \n    * 则开启短路\n    \t* close状态变成open状态\n        * 所有经过短路器的请求全部被短路\n        \t* 不走原服务，直接走fallback降级\n          \n\n3. 自动恢复\n\t\n    * circuitBreakerSleepWindowInMilliseconds\n    * 经过一段时间，默认5秒，会变成half-open半开状态\n    \t* 让一个请求经过短路器\n        * 如果请求出成功了，则自动恢复，转到close状态"},"children":[{"data":{"id":"bxj04q8r8sg0","created":1570421980546,"text":"配置","note":"1. circuit breaker\n\t* withCircuitBreakerEnabled\n    \t* 控制短路器是否允许工作，默认true\n        \n\n2. 强制打开短路器（直接走降级）\n\t* WithCircuitBreakerForceOpen\n    * 默认false\n    \n3. 强制关闭短路器（手动停止短路）\n\t* WithCircuitBreakerForceClosed"},"children":[]}]}]},{"data":{"id":"bxi3sndsfcw0","created":1570330758433,"text":"故障蔓延","note":"* 在任何一个系统中\n\t* 并发去发起多个网络请求去调用一些服务\n    \t* 是依赖自己的线程资源的\n    \t* 线程总资源有限\n    \n    * 调用的服务不可用，故障\n    \t* 调用出现延迟\n    \t* 在超时时间前，会出现阻塞卡住\n    * 则此时线程资源不能及时回收\n    \t* 并发高场景下，线程资源很快用尽，无法接受新的请求\n        * 甚至大量线程不断运转，导致自己宕机\n        * 当前服务也出现故障\n\n","layout_right_offset":{"x":-3,"y":-260},"layout":null},"children":[]}]}]},"template":"right","theme":"fresh-green","version":"1.4.43"}