{"root":{"data":{"id":"bxcash4fucw0","created":1569741132836,"text":"数据结构与算法","note":"1. 广义\n\t* 数据结构\n    \t* 指一组数据的存储结构\n    * 算法\n    \t* 操作数据的一组方法\n\n2. 狭义\n\t* 某些著名的数据结构和算法\n    \t* 比如队列、栈、堆、二分查找、动态规划\n        * 这些都是前人智慧的结晶"},"children":[{"data":{"id":"bxocsqnvixs0","created":1570965515302,"text":"复杂度分析","note":"1. 通过统计，监控等获取算法的执行时间及占用内存\n\t* 叫事后统计法\n    \t* 依赖测试环境，如硬件。还有数据规模等\n    * 因此不准确","expandState":"collapse"},"children":[{"data":{"id":"bxoctcl8pe00","created":1570965563032,"text":"时间复杂度","note":"* 大O时间复杂度\n\t* 表示代码执行时间随数据规模增长的变化趋势\n    * 也叫渐进时间复杂度\n    \t* 简称时间复杂度\n        \n        \n* 时间复杂度分析\n\t* 大O的复杂度表示方法，表示的是一种变化趋势\n    \t* 因此一般会忽略常量、低阶，系数等\n        * 只记录最大阶的量级即可\n        \t* 如循环执行次数最多的一段代码\n            \n        \n        \n* 加法法则\n    * 总复杂度等于量级最大的那段代码的复杂度\n        * 常量级别的代码（与n五无关的），即使执行上万遍，复杂度都是 O(1)\n        * 因为对变化趋势没有影响\n        \n\t* 如代码的复杂度里有O(n),O(n^2)， 则只看最大的O(n^2)\n        \n  \n* 乘法法则\n    * 嵌套代码的复杂度等于嵌套内外代码复杂度的乘积\n    \t* 如O(n) 与 O(n^2) 的乘积是O(n^3)\n        \n        \n* 多个数据规模, 即数据规模不止一个\n\t* 如 m n\n    \t* 由于两个数字都是变化的\n        * 因此复杂度根据两数据规模一起决定\n        \n        * 如O(m+n) , O(m* n) 等\n   "},"children":[{"data":{"id":"bxodo0eh8a00","created":1570967965791,"text":"复杂度量级","note":"* 多项式量级(按数量级递增)\n\t* 常数阶\n    \t* O(1)\n        * 执行了很多行，只要跟数据规模没关系，就是O(1)\n        \n    * 对数阶\n    \t* O(logn)\n        \n    * 线性阶\n    \t* O(n)\n        \n    * 线性对数阶\n    \t* O(nlogn)\n        \n    * 平方阶，立方阶...k次方阶等\n    \t* O(n^2) O(n^3)\n\n\n\n\n\n\n* 非多项式量级\n\t* O(2^n)和O(n!)\n    \t* 指数阶，阶乘阶\n\n        \n    * 是非常低效的算法\n    \t* 数据规模越来越大时，非多项式量级算法的执行时间会急剧增加"},"children":[]},{"data":{"id":"bxoe1xbbt5s0","created":1570969056168,"text":"对数阶","note":"* O(logn)\n\t* 对数阶的复杂度表示方法中\n    \t* 忽略对数的底\n     \n\t* 因为任何底数的对数\n    \t* 都可以转化为：其他底数的对数 乘以 一个常数\n        * 见有道笔记，此处不好表示底数\n        \n        \n        \n* 对于线性对数阶O(nlogn)\n\t* 相当于O(logn)的代码循环执行了n遍\n    \t* 乘法法则\n        \n    * 归并排序、快速排序的时间复杂度都是O(nlogn)"},"children":[]},{"data":{"id":"bxofdyorb8g0","created":1570972820637,"text":"最好、最坏情况时间复杂度","note":"* 最好\n\t* 在最理想的情况下，执行这段代码的时间复杂度\n \n* 最坏\n\t* 在最糟糕的情况下，执行这段代码的时间复杂度"},"children":[]},{"data":{"id":"bxofe9y9uuw0","created":1570972845157,"text":"平均情况时间复杂度","note":"* 通常是将每种情况的耗时相加\n\t* 除以N种情况的个数\n    \t* 得到一个平均值\n        \n        \n* 但是，不算上各种情况的概率是不准确的\n\t* 因此，应该每种情况单独算上出现的概率\n    \t* 得到加权平均值\n        \n\n* 因此，也叫\n\t* 权平均时间复杂度\n    \t* 或者期望时间复杂度\n   \n   \n   \n* 注\n\t* 一般使用一个复杂度就可以了\n    \n    * 只有同一块代码在不同的情况下，时间复杂度有[量级的差距]\n    \t* 我们才会使用这三种复杂度表示法来区分\n\t"},"children":[]},{"data":{"id":"bxofedd85ls0","created":1570972852591,"text":"均摊时间复杂度","note":"* 相比前面的复杂度分析，使用场景更加特殊、更加有限。\n\n\n* 摊还分析法\n\t* 通过摊还分析得到的时间复杂度\n    \t* 叫均摊时间复杂度\n    \n    * 能够通过摊还分析法处理，就不用使用加权平均去算，会更加简便\n     \n     \n     \n* 使用场景：对于一些特殊的复杂度分析\n\t* 例如，经过n个的O(1) 操作之后\n    \t* 有一个 O(n) 的操作\n    * 并且是有规律的，有一定时序关系的\n    \n    * 则可以将该次O(n) 的操作，均摊到每个O(1)操作\n    \t* 总体复杂度就是O(1)\n      \n      \n      \n* 在能够应用均摊时间复杂度分析的场合\n\t* 一般均摊时间复杂度就等于最好情况时间复杂度\n    \n    * 可以认为\n    \t* 均摊时间复杂度就是一种特殊的平均时间复杂度"},"children":[]}]},{"data":{"id":"bxoctj3oixk0","created":1570965577208,"text":"空间复杂度","note":"* 渐进空间复杂度\n\t* 表示算法的存储空间与数据规模之间的增长关系\n    \n* 常见的空间复杂度就是O(1)、O(n)、O(n^2)  \n\t* 如对于传入的n个参数\n    \t* new对应大小的数组\n        * 则空间复杂度就是O(n)\n    ","layout_right_offset":{"x":-22.499999105930385,"y":95.83332952525899}},"children":[]}]},{"data":{"id":"bxxkv0b7e6g0","created":1571902301521,"text":"基础数据结构","expandState":"collapse"},"children":[{"data":{"id":"bxp1qn1ya480","created":1571035878465,"text":"数组","note":"* 数组（Array）\n\n1. 是一种线性表 数据结构\n    * 数据排成像一条线一样的结构\n    \n    * 每个线性表上的数据，最多只有前后两个方向\n        * 链表、队列、栈等也是线性表结构\n        * 二叉树、堆、图等是非线性表\n        \n2. 它用一组连续的内存空间，来存储一组具有相同类型的数据。\n\n\n\n","expandState":"collapse"},"children":[{"data":{"id":"bxp20zls5pc0","created":1571036689428,"text":"读","note":"* 数组读比较快\n\t* 是因为数据可以进行随机访问\n    \t* 基于连续的内存空间和相同的数据类型\n        * 首地址 + 角标 * 每个元素大小\n    * 数组支持随机访问，根据下标随机访问的时间复杂度为O(1)\n    \t* 注意不能说数组查找时间复杂度为O(1)\n        \n        \n* 根据寻址公式，所以下标从0开始 更加简便，无需做减1操作\n\t* 角标相当于一个偏移量\n    * 而java等语言沿袭了C语言中的设计，所以下标从0开始\n    \n    \n* 关于二维数组\n\t* 猜测地址是从先按照列，再到行\n    \n    * 对于 m * n 的数组，a [i][j] (i < m,j < n)的地址为：\n\n\t* address = base_address + ( i * n + j) * type_size\n    "},"children":[]},{"data":{"id":"bxp2121igjk0","created":1571036694732,"text":"插入、删除","note":"* 插入\n\t* 随机插进一个数组的一个位置，并且该位置后的元素都往后移动一位\n    \t* 时间复杂度为O(n)\n        * 如插入一个有序结构\n        \n    * 不需要移动\n    \t* 如直接将该位置元素放到最后\n        * 时间复杂度：O(1) \n        * 快排中有使用到这种思想\n        \n        \n        \n* 删除\n\t* 删除数组中随机一个元素，并移动其他元素，保证内存的连续性\n    \t* 时间复杂度：O(n)\n        \n    * 如果不需要保证内存连续性\n    \t* 如只标记为已删除，待空间不够再真正删除\n        * 时间复杂度猜测为O(1)\n        * 与JVM的标记清除算法思想类似"},"children":[]},{"data":{"id":"bxp2lkd4gbs0","created":1571038301900,"text":"与容器比较","note":"* 容器，如ArrayList等\n\t* 优点\n    \t* 可以将很多数组操作的细节封装起来，如：支持动态扩容\n        * ArrayList是按照1.5倍数动态扩容\n        \t* 扩容操作涉及内存申请和数据搬移，如果事先能确定需要存储的数据大小，最好在创建ArrayList的时候事先指定数据大小。\n        \n        \n        \n      \n      \n      \n* 数组使用场景\n    * ArrayList无法存储基本类型，而自动装箱、拆箱有一定的性能消耗\n        * 所以特别关注性能，或者希望使用基本类型，就可以选用数组\n        \n    * 数据大小事先已知，且要做的操作简单,用不到ArrayList的大部分方法\n    \t* 则可以使用数组\n    \n    * 另外多维数组时，Object[][]\n    \t* 看起来更加直观（个人喜好）\n\n\n\n\n* 一般使用容器就足够了，除非想把性能做到极致\n    \t\n        "},"children":[]}]},{"data":{"id":"bxp5b1zpio80","created":1571045941595,"text":"链表","note":"* 对比数组\n\t* 数组需要连续的内存\n\t\t* 链表不需要\n        \n    * 复杂度刚好相反\n    \t* 数组插入O(n),查询O(1)\n        * 链表插入O(1),查询O(n)\n        \n        \n        \n    * 数组实现上使用的是连续的内存空间\n    \t* 可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高\n        \n        * CPU每次从内存读取数据并不是只读取那个特定要访问的地址\n        \t* 而是读取一个数据块\n            * 下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取\n     \n     \n     \n    * 数组扩容，需要将数据重新复制一份。费时","expandState":"collapse"},"children":[{"data":{"id":"bxp5bct6r7s0","created":1571045965145,"text":"关于内存淘汰策略","note":"* 先进先出策略FIFO（First In，First Out）\n\n* 最少使用策略LFU（Least Frequently Used）\n\n* 最近最少使用策略LRU（Least Recently Used）\n\t* linkedHashMap就是利用双向链表来实现的LRU"},"children":[]},{"data":{"id":"bxp5f7ahxbs0","created":1571046266588,"text":"各种链表","note":"\n        \n        \n* 单链表\n\t* 每个链表节点\n    \t* 除存储数据外\n        * 还有一个后继指针next，记录下一个节点的地址\n        \n    * 头结点记录链表的基地址\n    * 尾结点指向空地址null\n    \n    * 删除操作的时间复杂度\n    \t* 不需移动其他节点，因此是O(1)\n    * 查找\n    \t* 无法使用寻址公式\n        * 因此时间复杂度：O(n)\n        \n        \n* 循环链表\n\t* 尾节点的next指向头节点\n    \n* 双向链表\n\t* 除了一个next后继节点\n    \t* 还有一个prev前驱节点\n        \n    * 占用更多空间，但是更加灵活\n    \t* 反正就是可以获取前面的节点，不需要从头开始遍历\n        \n* 双向循环链表\n\t* 如描述，双向，且是环状","layout_right_offset":{"x":8.499997317791099,"y":3.8333333002196355}},"children":[]},{"data":{"id":"bxpc76i3f140","created":1571065385726,"text":"单向链表判断是否回文","note":"1. 使用快慢指针定位链表中间节点\n\t* 快指针一次走两步\n    * 慢指针一次走一步\n    \t* 则快指针走完时，满指针走到中间\n        * 注意判断奇偶数，可根据快指针进行判断\n\n2. 慢指针走的时候，将前半部分反序\n\n3. 前后半部分比较，判断是否为回文\n\n4. 前半部分逆序复原\n\n5. 时间复杂度O(n), 空间复杂度O(1)\n\n* 代码见github"},"children":[]},{"data":{"id":"bxq4lvas1dc0","created":1571145527879,"text":"算法代码技巧","note":"1. 哨兵节点\n\t* 在链表的头部等边界地方，设置哨兵\n    \t* 可以将边界的特殊处理变成普通处理\n        \n    * 如带头链表的头即为哨兵 \n    \t* 则所有节点的插入删除逻辑都一样了\n        \n    * 又如：在数组中找值为x的节点\n    \t* 先判断数组尾部的的是否符合\n        * 不符合的话，替换尾部节点为目标x\n        \t* 符合直接返回\n        * 从头开始遍历数组，直至找到x\n        \t* x不是最后一个，则找到\n          \t* x是最后一个，表示数组中没这个元素\n        \n        * 优化点：无需判断数组是否角标越界。直接判断是否等于x即可\n        \t* 大数据量中，少一个判断也是很重要的\n            * 当然平时不要这样写。。。\n   \t"},"children":[]}]},{"data":{"id":"bxremdslruw0","created":1571275339195,"text":"栈","note":"* 栈是一种 操作受限 的线性表\n\t* 先进后出，后进先出\n    \t* 只允许一端插入、删除数据\n        \n        \n* 相比链表、数组，栈带来的只有限制\n\t* 但是在特定场景，栈能够满足需求\n    \t* 同时更加可控，不容易出错\n        \n      \n* 数组实现的栈\t\n\t* 叫：顺序栈\n\n* 链表实现的栈\t\n\t* 叫：链式栈\n    \n    \n    \n* 栈 操作的时间、空间复杂度\n\t* 时间复杂度\n    \t* O(1) 因为每次都只操作顶部的一个元素\n    \n    * 空间复杂度\n    \t* O(1) 只需一两个临时变量存储空间\n        * 动态扩容的顺序栈，到达某一临界大小时扩容，迁移数据到更大的内存\n        \t* 此时可用摊还分析法，将数据搬移工作均摊到每一次入栈操作\n            * 时间复杂度还是O(1)\n\n\n* 注意：\n\t* 存储的空间都是一个大小为n的数组或链表，不代表空间复杂度就是O(n)\n        * 因为这个空间是必须的，无法省掉\n        * 我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间","expandState":"collapse"},"children":[{"data":{"id":"bxrf07ku5pk0","created":1571276422763,"text":"栈的应用","note":"1. 函数调用栈\n\t* 每进入一个函数\n    \t* 就会将临时变量作为一个栈帧入栈\n    * 当被调用函数执行完成，返回之后\n    \t* 将这个函数对应的栈帧出栈\n        \n\n2. 表达式\n\t* 传入一串表达式，如:3+ 5* 4-2\n    * 编译器通过两个栈实现\n    \t* 一个保存操作数的栈\n        * 一个是保存运算符的栈\n        \n    * 从左往右遍历表达式\n    \t* 遇到数字，压入操作数栈\n        * 遇到运算符\n        \t* 与运算符栈顶的元素比较\n    * 比较规则\n    \t* 先确定所有的优先级\n        * 当前比运算符栈顶的元素优先级高，压入栈顶\n        * 等级更低或者等级相同，取出栈顶运算符先运算\n        \n        \n        \n3. 检查各种括号()[]{}等是否匹配\n    * 遍历表达式。遇到坐括号就压入栈中\n    \t* 遇到右括号就取出栈顶元素进行匹配\n        * 全部能匹配则通过\n        \n\n4. 浏览器的前进后退\n\t* 使用2个栈\n    \t* 点后退，就将左栈栈顶页面移到右栈\n        * 点前进，就将右栈栈顶元素移到左栈\n        * 点了其他页面，清空右栈\n    \n"},"children":[]}]},{"data":{"id":"bxrfxglglm00","created":1571279028409,"text":"队列","note":"* 也是操作受限的线性表数据结构\n\t* 先进先出\n    \n* 数组实现的队列\n\t* 顺序队列\n    \n* 链表实现的队列\n\t* 链式队列\n    \t* 实现逻辑相对简便\n    \n    \n","expandState":"collapse"},"children":[{"data":{"id":"bxsasicvh1s0","created":1571366094829,"text":"顺序队列实现","note":"* 顺序队列的实现\n\t1. 正常的数组，取出head元素后，head指针往前一步\n    \t* 会导致即使数组中有空位，也无法继续插入数据\n    \n    \n    2. 解决方案\n    \t* 当数据入队时已经满了，则触发数据搬移\n        * 每个元素一次性移动n位（直接赋值，n是角标是0与head的距离）\n        \n        \n    3. 循环队列解决方案\n    \t* 数组的最后元素指向head，则形成环状\n        * 无需数据搬移，性能更好\n        \n        * head 与 tail之间留一个空位不存数据\n        \t* 此处猜测是沿用非环状数组的用法，但是非环状数组会在满了之后，也利用最后一个空位\n        \n        * 队空的判断不变，都是head==tail\n\n\n\n* 队满的条件：(tail+1)%n=head\n\t* n是数组大小，tail是从零开始，因此tail+1不可能比n大。\n    \t* 相当于tail + 1 = head\n        \n    * 使用取模的意义在于临界点的判断\n    \t* tail是最大角标n-1，head是0，此时 n-1+1%n=0\n        * 即完成一圈就将数字重置为0，也就刚刚好是取模的用法\n        \n    * head、tail的移动都需要按照此公式"},"children":[]},{"data":{"id":"bxsb5yd7u940","created":1571367148413,"text":"关于阻塞队列","note":"* 一般无界的队列，就是链式队列实现的"},"children":[]}]}]},{"data":{"id":"bxsbdf7bmpk0","created":1571367733611,"text":"递归","note":"* 递归需要满足的三个条件\n\t* 一个问题的解可以分解为几个子问题的解\n    \n    * 这个问题与分解之后的子问题，除了数据规模不同\n    \t* 求解思路完全一样\n    \n    * 存在递归终止条件\n    \n    \n* 因此递归的关键在于\n\t* 写出递推公式\n    * 找到终止条件\n    \n    \n* 思维误区\n\t* 不要尝试去思考、层层分解每一层子问题，子子问题的关系\n    \t* 只考虑当前问题与子问题的关系即可（算出递推公式）\n      \n      \n\n","expandState":"collapse"},"children":[{"data":{"id":"bxsbnzamu600","created":1571368560988,"text":"递归的问题","note":"* 警惕栈溢出\n\t* 解决：定义全局变量，每执行一次+1，超过一定深度直接报错返回\n    \t* 估算的阈值不一定准确，因为跟当前线程剩余栈空间大小有关\n        * 因此不太实用，一般是深度较少的时候才使用\n        \n    * 因为递归调用一次就会在内存栈中保存一次现场数据\n    \t* 因此空间复杂度也会变大\n        * 如电影院的逐行递归，空间复杂度是O(n)\n        \n    \n        \n        \n        \n* 重复计算\n\t* 递归可能造成大量重复计算\n    \t* 如递归分成左右2子问题，子问题间可能会有重复\n        \n    * 解决\n    \t* 使用map存起来，递归调用前判断\n        \n* 递归的函数调用较多，数量大时时间效率较低"},"children":[]},{"data":{"id":"bxsbyvd6d6g0","created":1571369414441,"text":"优缺点","note":"* 优点\n\t* 递归代码的表达力很强，写起来非常简洁\n    \n* 缺点\n\t* 空间复杂度高、有堆栈溢出的风险\n    * 存在重复计算、过多的函数调用会耗时较多等问题\n    \n    \n    \n* 所有的递归代码都可以改为迭代循环的非递归写法\n\t* 因为递归本身就是借助栈来实现的\n    * 只不过我们使用的栈是系统或者虚拟机本身提供的\n    \t* 如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程\n        * 这样任何递归代码都可以改写成看上去不是递归代码的样子。\n\n\n* 一般思路是从底部结束条件往前迭代\n\n* 但是这种思路实际上是将递归改为了“手动”递归\n\t* 本质并没有变，\n\t* 而且也并没有解决前面讲到的某些问题，[徒增]了实现的复杂度。\n    \n    \n* 数据规模大，就要用这种方式了，避免递归导致栈溢出"},"children":[]},{"data":{"id":"bxsfkqgd6a00","created":1571379591091,"text":"递归断点","note":"* 对递归的值打日志\n\t* 并且结合 条件断点 使用\n    \t* idea断点->右键，设置条件"},"children":[]}]},{"data":{"id":"bxte8kh5enc0","created":1571477376555,"text":"排序","note":"* 有序度，逆序度\n\t* 对于一组数据\n    \t* 排列组合是 n！种\n        \n    * 有序元素\n    \t* 如果i<j ，则a[i] <= a[j]\n        \n    * 有序度\n    \t* 数组中具有有序关系的元素对的个数\n        * 完全有序的，又称满有序度\n        \t* 为：n*(n-1)/2\n            * (n-1)+(n-2)+...+1\n        * 完全逆序的，有序度就是0\n        \n    * 逆序度\n    \t* 逆序度=满有序度-有序度","expandState":"collapse"},"children":[{"data":{"id":"bxtemf23kk80","created":1571478461859,"text":"如何评价、分析一个排序算法","expandState":"collapse"},"children":[{"data":{"id":"bxtemp74gkw0","created":1571478483931,"text":"排序算法的执行效率","note":"* 排序算法的执行效率\n\n\t1. 最好情况、最坏情况、平均情况时间复杂度\n    \t* 要能说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的\n        \n    \t* 有些算法会区分三种复杂度，为了对比，所以最好都做一下区分\n        \n        * 不同有序度的数据，对执行时间肯定有影响\n        \t* 我们要知道排序算法在不同数据下的性能表现\n            \n            \n\t2. 时间复杂度反应的是数据规模n很大的时候的一个增长趋势\n    \t* 实际开发中。数据规模可能并不高\n    \t* 因此在对[同一阶]时间复杂度的排序算法性能对比的时候\n        \t* 我们就要把系数、常数、低阶也考虑进来。\n   \t\n    \n    3. 比较次数和交换（移动）次数\n    \t* 基于比较的排序算法会涉及两种操作\n        \t* 元素比较大小\n            * 元素交换或移动。"},"children":[]},{"data":{"id":"bxten0943ts0","created":1571478507995,"text":"排序算法的内存消耗","note":"* 算法的内存消耗可以通过空间复杂度来衡量\n\n* 原地排序（Sorted in place）\n\t* 特指空间复杂度是O(1)的排序算法"},"children":[]},{"data":{"id":"bxtenme8xaw0","created":1571478556195,"text":"排序算法的稳定性","note":"* 稳定性\n\t* 如果待排序的序列中存在值相等的元素\n    \t* 经过排序之后，相等元素之间原有的先后顺序不变。\n        \n        * 相等元素位置不变就是稳定的排序算法\n        \t* 否则就是不稳定的\n            \n  \n* 应用\n\t* 如实际开发中，先对金额进行排序，再按照创建时间进行排序\n    \t* 方案1 先按照金额排序，对于金额相同的，再进行排序\n        \t* 实现起来复杂\n        \n        * 方案2 先对时间进行排序，再对金额进行排序\n        \t* 使用稳定的排序算法可以非常简洁得实现"},"children":[]}]},{"data":{"id":"bxte9ajleqg0","created":1571477433299,"text":"O(n^2) :冒泡、插入、选择","note":"* 为什么插入排序要比冒泡排序更受欢迎\n\t* 从代码实现上来看\n    \t* 冒泡排序的数据交换要比插入排序的数据移动要复杂\n        * 冒泡排序需要3个赋值操作，而插入排序只需要1个\n        \n        \n* 所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是O(n2)\n\t* 但是如果我们希望把性能优化做到更好，那肯定首选插入排序。\n    * 即考虑算法的系数，低阶等因素","expandState":"collapse"},"children":[{"data":{"id":"bxtiufu5yvs0","created":1571490374909,"text":"冒泡","note":"* 冒泡\n\t* 是原地排序算法\n    \t* 空间复杂度为O(1)\n    \t* 只需要常量级的临时空间(交换时使用)\n        \n    * 是稳定的排序算法\n    \t* 只有交换才会改变前后顺序\n        * 相等并不交换\n        \n        \n    * 时间复杂度\n    \t* 最好O(n)\n        \t* 原本有序\n            \n        * 最坏O(n^2)\n        \t* 原数据逆序 \n            \n        * 平均\n        \t* O(n^2)\n        \t\n\n* 冒泡排序中\n\t* 交换的次数等于逆序度的次数\n    \t* 最坏 n*(n-1)/2,最好 0\n        * 中间值 n*(n-1)/4\n        \n    * 比较操作肯定要比交换操作多\n    \t* 上限O(n^2)\n        * 所以平均时间复杂度是O(n^2)\n        \n    * 这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用\n        \n\n"},"children":[]},{"data":{"id":"bxtjh7rrqhc0","created":1571492159726,"text":"插入","note":"* 元素的移动总次数，也是等于逆序度\n\t* 分成一个有序区间和一个无序区间\n    \t* 每次从无序区间中取出一个元素\n        * 比较找到自己的位置插入\n        \t* 找到自己的位置后，后面的元素全部往后移动一位\n        * 重复N次\n            \n            \n\n* 插入排序\n\t* 是原地排序算法\n    \n\t* 是稳定排序\n    \n    * 时间复杂度\n    \t* 最好情况，已经有序O(n)\n        * 最坏,逆序 O(n^2)\n        \n        * 平均也是O(n^2)\n        \t* 在一个有序的数组中插入一个数字是O(n)\n            * 所以n个就是O(n^2)"},"children":[]},{"data":{"id":"bxtlba2eo140","created":1571497336757,"text":"选择","note":"* 选择排序\n\t* 也分一个有序区间，一个无序区间\n    \n    * 每次从无序区间中选最小的，放进有序区间的末尾\n    \t* 每次通过比较获得未排序的最小元素\n        * 实际操作：将该最小元素与有序与无序的交界处的元素互换位置\n        \n    * 是原地排序\n    \n    * 时间复杂度\n    \t* 最好, 最坏，平均：O(n^2)\n        * 因为即使原本有序，也是每次拿出一个最小的，然后重复N次\n    \n    * [不是]稳定排序\n    \t* 如5，8，5，2，9\n        * 经过一次交换，原本在前的元素5变成后面\n        \t"},"children":[]}]},{"data":{"id":"bxteaa7rw2o0","created":1571477510948,"text":"O(nlogn)：快排、归并","note":"* 都用到了分治思想\n\t* 即将问题拆分成小问题处理\n    \n* 递归\n\t* 递归也是类似的处理方案\n    \n* 因此，分治算法一般都使用递归来实现\n\t* 分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突\n    \n    * 归并、快排的区别\n    \t* 归并是自下而上的排序\n        \t* 先处理子问题，再合并\n            \n        * 快排是自上而下\n        \t* 先分区，再处理子问题\n            \n\n* 使用二分（分治）比冒泡这些快的本质\n\t* 结论上，可通过数学公式的推导\n    * 理解上，冒泡等算法\n    \t* 每一遍的循环，只选出了一个元素，剩余的元素没任何变化\n        * 而两种分治法的排序，每次子问题的处理，或者父问题的处理，是带有一定的有序性\n        \t* 减少下一步的操作","expandState":"expand"},"children":[{"data":{"id":"bxu1ivqj9k00","created":1571543070236,"text":"快排","note":"* 也是使用分治思想\n\n* 数组中选择随机一个元素，例如最后一个\n\t* 作为一个分区点pivot\n    * 遍历p到r之间的数据\n    \t* 将小于pivot的放到左边\n        * 将大于pivot的放到右边\n        * 将pivot放到中间。\n\n* 不断将两边的区间按照此方式处理\n\t* 处理完就是有序的了\n    \n* 将数组分为2个区间\n\t1. 可以使用两个临时数组，实现方便\n    \t* 但是这样就不是原地排序了\n        \n    2. 从左到右遍历数组\n    \t* 标记遇到的第一个比pivot大的元素\n        * 后面遇到比此元素小的，则交换位置\n        \t* 标记元素的角标+1\n            * 新的标记元素肯定也是比pivot元素大的\n        * 最后将标记元素与pviot元素交换\n        \n* 性能分析\n\t* 是原地排序\n    * [不稳定]的算法\n    \t* 因为有交换\n        * 不移动其他元素，直接交换的，都是不稳定的，如选择排序\n        \n    * 时间复杂度\n    \t* 平均O(nlogn)\n        * 极端（原本有序），退化为O(n^2)\n        \t* 原本有序，每次取最后一个元素，则两区间每次都相差很大\n            * 每次都只能处理一个元素\n        * 推理。待树的那一节学习\n        \n    \t\n        \n","expandState":"expand"},"children":[{"data":{"id":"bxucelqvkao0","created":1571573767241,"text":"应用","note":"* 求无序数组的第K大元素\n\t* 使用快排，定一个中间点pivot\n    \t* 经过一轮比较，k比pivot位置大，则处理右边区间\n        \t* 否则处理左边区间\n        * 直到pivot等于k\n        \t* 则pivot的值就是第k大元素\n            \n    * 此算法复杂度是O(n)\n    \t* 第一次遍历n次\n        * 第二次遍历n/2次...\n        \n        * 等比数列求和\n        \t* 2n-1\n            * 即O(n)\n            \n            \n            \n* 注：等比数列求和公式\n\t* (a1-anq)/(1-q)\n    \t* a1为首项,an为第n项,d为公差,q 为等比"},"children":[]},{"data":{"id":"bxvwqh3awjk0","created":1571732679634,"text":"优化","note":"* 快排中，分区点的选择很重要\n\t* 如果每次选择的都是区间的末尾\n    \t* 则时间复杂度退化为O(n^2)\n        \n    * 分区点选择优化\n    * 三数取中法\n        * 从区间的首、尾、中间，分别取出一个数，比较大小，取中值\n        \n        * 数据量大可以是五数取中或者十数取中\n        \n    * 随机法\n    \t* 随机的概率较为平均"},"children":[]}]},{"data":{"id":"bxu1kf4ssrk0","created":1571543190822,"text":"归并","note":"* 将数组不断分为两半，直至只有一个元素\n\t* 将两半的元素合并，此时这两半数据，内部本身是有序的\n    \t* 对比俩个数组的第一个元素，谁小就取谁，放进临时数组\n        * 处理完后临时数据数据遍历赋值回原数组\n        \n        \n        * 哨兵方式：增加2个子数组的临时数组，大小比子数组大1，用于装哨兵\n        \t* 哨兵放Integer最大值\n            * 先在临时子数组暂存原来子数组数据\n            * 原数组接收对比后的数据\n            * 哨兵的存在可以不用判断是否已经到达边界\n       * 哨兵的代码量会略简洁\n       \n* 性能分析\n\t1. 是稳定的的算法\n    \t* 前提是合并的代码，左边<=右边时，选左边（等于号不能少）\n        \n    2. 时间复杂度\n    \t* 时间复杂度 O(nlogn)\n        * 最好情况、最坏情况，还是平均情况都一样，与数据的有序度无关。非常稳定\n     \n    3. 空间复杂度\n    \t* 需要临时数组\n        * O(n) ,[不是]原地排序","expandState":"expand"},"children":[{"data":{"id":"bxu8on584aw0","created":1571563269488,"text":"时间复杂度分析","note":"* T(n) = 2* T(n/2) + n； \n\t* n>1\n    \t* 后面的n表示合并操作\n        \n\t* T(1) = C；   \n    \t* n=1时，只需要常量级的执行时间，所以表示为C。\n        \n    * 一直推导\n    \t* 2^k * T(n/2^k) + k * n\n        \n        * 当T(n/2^k)=T(1)时\n        \t* n/2^k=1\n            * k=log2n \n        \n        * Cn + n* log2n\n        \t* 即O(nlogn)\n            \n            \n* 归并排序比较稳定 \n\t* 栈的深度是logn 非常小 所以不会堆栈溢出"},"children":[]}]}]},{"data":{"id":"bxteawan94o0","created":1571477559011,"text":"O(n) : 桶、基数、计数","note":"* 因为时间复杂度是O(n)\n\t* 因此也叫线性排序\n    \n    * 为什么能做到线性排序\n    \t* 不是基于比较的排序算法，不涉及元素之间的比较\n        \t* 基本只需要遍历一次就完成\n     \n    * 对排序的顺序有要求\n    \n    \n    \n* 空间复杂度\n\t* 这几种都不是原地排序\n    \n* 时间复杂度\n\t* 桶排序\n    \t* O(n)\n        * 桶的数量接近于数据数量时是O(n)\n        \t* 此处是指实际使用的桶的数量，实际使用的桶越多，桶内的元素越少，需要快排的数据越少\n            \n            * 注意代码中，桶的数量是数据范围决定的，不是数据量\n        * 即一个桶内元素不多，就是O(n)\n        \n    * 计数排序\n    \t* O(n+k)\n        * k是数据范围\n        \t* 因为需要遍历计数桶，将前面的元素数量加到当前的总数\n            * 范围大会导致时间复杂度变大\n        \n    * 基数排序\n    \t* O(dn)\n        * d是维度\n        \t* 维度不大的话近似于O(n)","expandState":"collapse"},"children":[{"data":{"id":"bxudeyc8xy80","created":1571576615765,"text":"桶排序","expandState":"collapse","note":"* 将数据分到几个有序的桶中\n\t* 即桶与桶之间是有序的\n    * 对每个桶里的元素进行排序\n    \t* 使用快排（要稳定dehua用归并）\n        * 最后按顺序取出，就是有序的了\n        \n* 时间复杂度\n\t* n个元素，分成m个桶\n    \t* 则每个桶的元素是k=n/m\n        * 每个桶的时间复杂度是O(klogk)\n        \n    * 总复杂度 O(m * klogk)\n    \t* O(n* log n/m)\n        * 当m接近于n时，就是O(n)\n        \n* 使用场景\n\t* 要排序的数据需要很容易就能划分成m个桶\n    * 桶与桶之间有着天然的大小顺序\n    \t* 不需要再次排序\n    * 数据需要均匀\n    \t* 极端情况下，退化为O(nlogn)\n    \n    * 适合用在外部排序中\n    \t* 即数据存储在外部磁盘中，无法将数据全部加载到有限的内存中\n        \n        \n* 例\n\t* 如10G订单信息按照金额排序\n    \t* 如果每个区间数量特别大，则可以对此区间再次细分\n        * 直到单个桶的数据可以全部加载到内存，进行快排\n        \n        * 注意要求稳定的算法的话，不要快排，用归并\n        \n        \n        \n        "},"children":[{"data":{"id":"bxud2o0czmw0","created":1571575652908,"text":"应用","note":"* 现在你有10个接口访问日志文件，每个日志文件大小约300MB\n\t* 每个文件里的日志都是按照时间戳从小到大排序的\n    \t* 你希望将这10个较小的日志文件，合并为1个日志文件\n        * 合并之后的日志仍然按照时间戳从小到大排列。\n        * 如果处理上述排序任务的机器内存只有1GB\n        \n        \n* 每次只读一个接口\n\t* 分到多个桶中\n    * 将桶中进行快排\n        \n    "},"children":[]},{"data":{"id":"bxvupz3cs600","created":1571726998235,"text":"实现","note":"* 注\n\t* 数组有多少元素，需要统计\n    \t* length是数组长度，不是真实数据\n        * （没填数据）默认就是0\n        \n    * 二维数组中 arr[i][j]\n    \t* arr[i] 表示第几个一维数组数组\n    \t* 一维数组长度可以不一致\n        \n        \n* 实现\n\t* 桶的大小作为参数传入\n    \t* 使用桶的大小决定桶的数量\n    \t* 传入的数据范围未知\n        * 桶的数量越接近数据范围，时间复杂度越接近O(n)\n        \n    * 桶的有序性\n    \t* 遍历数组\n        * 当前元素/桶大小作为桶角标\n        \n    * 使用二维数组存储所有桶\n    \t* 每个一维数组支持扩容\n        \n    * 使用一个数组记录每个桶大小"},"children":[]}]},{"data":{"id":"bxufav466oo0","created":1571581937509,"text":"计数排序","note":"* 计数排序其实是桶排序的一种特殊情况\n\t* 要排序的n个数据，所处的范围并不大的时候，比如最大值是k\n    \t* 就分成k+1个桶(元素是0或正整数)\n        * 桶内元素相等\n        \n        * 例如50w的考生，分数范围是0~900，则分为901个桶\n        \n        \n* 实现\n\t* 创建一个数组C，记录每种元素的个数\n    \t* 再改造为：记录小于等于当前元素的总个数\n        \n    * 创建临时数组R，用于放有序少数据\n    \n    * 从后往前遍历原数组A\n    \t* 根据当前元素的大小，匹配到计数数组C的下标\n        * 则该数组对应的值就是在有序数组中的位置\n        \t* 有序角标 =  value-1\n     \n    * 将临时数组赋值回原数组\n    \n    \n* 使用场景\n\t* 计数排序只能用在数据范围不大的场景中\n    \t* 如果数据范围k比要排序的数据n大很多，就不适合用计数排序了。\n        \n        * 而且，计数排序只能给非负整数排序\n        \t* 如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数\n            * 负数：同时+n\n            * 小数：同时乘以100等"},"children":[]},{"data":{"id":"bxuzsbx8oag0","created":1571639728484,"text":"基数排序","note":"* 对于手机号排序这种\n\t* 范围太大，不适宜桶排序，计数排序\n\n\n* 思路\n\t* 如果数据a在高位比数据b大\n\t\t* 那么a就肯定比b大\n    \n* 基数排序\n\t* 通过稳定排序的思想\n    \t* 先比较低位，再比较高位\n        \n    * 则比较完的就是有序的了\n    \n    \n    * 每一位的排序可以是桶排序，或者计数排序\n\n\n* 时间复杂度\n\t* k位的排序，每一位使用O(n)的桶排序\n    \t* O(k * n)\n        * k不大时，近似O(n)\n        \n        \n* 场景\n\t* 如果数据不等长，如英文字典\n    \t* 以前面的为准的，则后面补0\n        * 0 SACII码 比字母小，不影响排序\n    \n    * 应用场景\n    \t* 排序的数据可以分割出独立的“位”来比较\n        * 位之间有递进的关系\n        \t* 如果a数据的高位比b数据大，那剩下的低位就不用比较了\n            \n        * 每一位的[数据范围]不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到O(n)了。"},"children":[]}]},{"data":{"id":"bxvwhwzv30o0","created":1571732008977,"text":"实现通用的、高性能的排序函数","note":"* O(n) 的排序算法\n\t* 适用场景比较特殊\n    \t* 通用的排序函数中不能使用\n        \n* O(n2) 的算法\n\t* 适合数据规模小的\n    \n* O(nlogn) 适合数据规模大的\n\n\n* 因此\n\t* 一般都使用O(nlogn)时间复杂度的算法\n    \t* Java语言采用堆排序\n        * C语言使用快速排序\n        \n    * 归并排序用的较少，因为不是原地排序\n    \n    \n* C语言中的qsort()函数\n\n\t* 优先使用[归并排序]来排序输入数据\n    \t* 小数据量的排序，占用空间不大，且时间复杂度稳定\n        \n    * 超过100M时\n    \t* 会改为用[快速排序]算法来排序\n        * 使用三数取中法定位分区点\n        \n    * 递归的堆栈溢出优化\n    \t* 通过自己实现一个堆上的栈，手动模拟递归来解决\n        \n    * 插入排序\n    \t* 在快速排序的过程中，当要排序的区间中，元素的个数小于等于4时，qsort()就退化为插入排序\n        * 不再继续用递归来做快速排序\n    \n    * 使用哨兵减少判断次数\n     "},"children":[{"data":{"id":"bxvxe81dxaw0","created":1571734540667,"text":"关于算法实际运行时间","note":"* 在小规模数据面前\n\t* O(n^2)时间复杂度的算法并不一定比O(nlogn)的算法执行时间长。\n    \n    * 因为时间复杂度表示的是执行时间与数据规模的增长趋势、\n    \t* 在大O复杂度表示法中，我们会省略低阶、系数和常数\n        * O(nlogn)在没有省略低阶、系数、常数之前可能是O(knlogn + c)，而且k和c有可能还是一个比较大的数。\n        \n        * 因此n^2的值可能实际上比knlogn+c还要小"},"children":[]}]}]},{"data":{"id":"bxw490supew0","created":1571753880862,"text":"二分查找","note":"* 二分查找\n\t* 针对的是一个有序的数据集合\n    \n\t* 查找思想有点类似分治思想。\n    \t* 每次都通过跟区间的中间元素对比\n        * 将待查找的区间缩小为之前的一半\n        \t* 直到找到要查找的元素，或者区间被缩小为0。\n            \n            \n* 算法时间复杂度\n\t* O(logn) \n    \t* 对数时间复杂度\n        \n    * 是一种极其高效的时间复杂度\n    \t* 有时候甚至比时间复杂度是常量级O(1)的算法还要高效。\n        \t* 数据规模很大时，对数时间复杂度还是很小\n            * 对于一些忽略了常数、系数和低阶的O(1)算法，其实际可能表示的是一个非常大的值","expandState":"collapse"},"children":[{"data":{"id":"bxw4g05ckug0","created":1571754427990,"text":"局限性","note":"* 二分查找依赖的是顺序表结构，即数组\n\t* 链表这种是不可以的\n    \t* 因为数组按照下标随机访问数据的时间复杂度是O(1)\n        \n        * 链表随机访问的时间复杂度是O(n)，查找效率就会变得很低\n        \n\n* 数据需要是有序数据\n\t* 数据表必须是有序的，如果无序需要先排序\n    \t* 排序一般最低O(nlogn)\n        \n        * 不是频繁插入删除的话，排序的时间成本会被均摊，相对较少\n        \n        * 频繁插入删除，则成本变高，则不再适用\n        \t* 每次插入删除时确保有序，或者二分前排序\n            \n            \n            \n* 数据量太少\n\t* 太少直接遍历即可\n    \t* 数据比较耗时的话，还是用二分，减少比较次数\n        \n        \n* 数据量太大\n\t* 二分查找的底层需要依赖数组这种数据结构\n    \t* 数组需要空间连续\n        * 数据太多放不下"},"children":[]},{"data":{"id":"bxwpn9no9mw0","created":1571814240546,"text":"代码注意的点","note":"* 使用while循环的话\n\t* 条件是low<=high\n    \t* 不是low< high\n        * 因为每次比较的只是中间值，可能low（即此时的high）就是想要的值\n        \n        \n        \n    * mid 取值\n    \t* low + ((high-low)>>1)\n        \n        * 不能加起来再除以2，因为可能超过int的大小\n        * 使用位运算是为了更快\n    \n    * mid的判断\n    \t* 每次判断mid值是否命中\n        \t* 命中则返回\n            \n        * 不命中，对应新区间是\n        \t* low = mid +1；\n            * 或 high = mid -1\n            \n            * 因为mid本身已经判断，所以直接加1.另外假如直接取mid值，在mid等于最大或最小值时，会导致死循环\n              \n                     "},"children":[]},{"data":{"id":"bxwrmnqt0fc0","created":1571819835066,"text":"应用","note":"* 假设我们有1000万个整数数据，每个数据占8个字节\n\t* 如何设计数据结构和算法，快速判断某个整数是否出现在这1000万数据中？\n    \n    * 1000万 * 8byte = 80M\n    \t* 全部数据放进内存，排序\n        * 二分查找\n        \n        \n    * 散列表和二叉树理论上可以解决\n    \t* 但是他们需要更多内存，本题目中用不上\n        \n        \n        \n        \n* 根据IP查找地区\n\t* IP的一个范围对应一个地区\n    \t* 根据ip找到对应的一个的地区\n    \n    * 先将ip数据转化为long类型\n    \t* ip数据不经常变化\n        * 将数据排序\n    \n    * 问题转化为：\n    \t* 查找最后一个小于等于给定ip值的\n        \t* 找到一个起始区间小于等于当前值的\n            \n        * 或 第一个大于等于给定ip值的\n    * 找到后再判断区间内是否包含当前值"},"children":[]},{"data":{"id":"bxwsg4oh5o80","created":1571822144491,"text":"二分查找变体","note":"1. 数据中存在重复数据\n\t* 需要找第一个（或最后一个）数据"},"children":[]}]},{"data":{"id":"bxxj9izdw3k0","created":1571897797044,"text":"跳表","note":"* 是一种各方面性能都比较优秀的动态数据结构\n\t* 可以支持快速的插入、删除、查找操作\n    \t* 写起来也不复杂，甚至可以替代红黑树\n        \n        \n        \n* 实际应用\n\t* redis 的 Sorted Set使用跳表实现。\n    \n    * sorted set支持以下操作\n    \t* 插入、删除、查找、按照区间查找数据、以及迭代输出有序序列\n        \n        \t* 插入、删除、查找以及迭代输出有序序列这几个操作红黑树也可以完成\n            * 时间复杂度跟跳表是一样的\n        \n        * 对于按照区间查找数据这个操作\n        \t* 跳表可以做到O(logn)的时间复杂度\n            * 定位区间的起点，然后在原始链表中顺序往后遍历就可以了。\n    \n    * 准确的说：双hashmap构成的字典和跳跃表实现\n    \n    \n* 跳表也不能完全替代红黑树\n\t* 因为红黑树比跳表的出现要早\n    * 有现成的可以直接拿来用\n    \n    * 跳表要自己实现","expandState":"collapse"},"children":[{"data":{"id":"bxxjsstfqag0","created":1571899307371,"text":"跳表原理","note":"* 对原始的有序单链表\n\t* 每隔两个节点（或者N个节点），提取一个节点到上一级\n    \t* 这一级的链表叫索引或索引层\n        \n    * 索引链表每个节点有一个down指针\n    \t* 指向下一级的链表对应的节点\n        \n    * 靠近原始链表的一层索引\n    \t* 叫 第一级索引\n        * 在第一级索引基础上可以继续建立第二、第三级索引\n        \n        * 直至最顶部索引只有2个节点\n        \t* 或3个\n        \n        \n\n\t"},"children":[]},{"data":{"id":"bxxkpq7yc0o0","created":1571901887736,"text":"时间、空间复杂度","note":"* 时间复杂度\n\t* 假设是每两个节点抽取一个节点\n    \n    * 最多有k级索引\n    \t* 第一层是 n/2 个元素\n        * 第二层是 n/4 个元素\n        * 第k层是 n/2^k 个元素\n        \n        * 最顶层是2个元素\n        \t* n/2^k = 2\n            * k = log2n -1(2是底)\n        \n    * 或者直观点：k为log2n（2为底）\n    \t* 每层只有下一层的一半\n    \n    * 遍历的时候，从一个节点往前，遇到前面的节点比要找的节点大\n    \t* 则往下一层\n        * 每一层，包括遇到比自己大的节点\n        * 共3个节点\n    \n    * 所以时间复杂度是\n    \t* O(3* (log2n -1))\n        * 即：O(logn)\n        \n        \n* 空间复杂度\n\t* 一级索引节点数 ：n/2\n    \t* 二级：n/4\n    \t* ...\n    \t* 即：O(n)\n    \n    * 也可以隔3个、5个节点抽取一个节点\n    \t* 空间复杂度虽然还是O(n)\n        * 但实际减少很多\n    \n    * 实际开发中\n    \t* 原始链表中存储的有可能是很大的对象\n        * 而索引结点只需要存储关键值和几个指针\n        * 所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了"},"children":[]},{"data":{"id":"bxxkwfq5klk0","created":1571902413441,"text":"动态插入与删除","note":"* 插入删除的时间复杂度\n\t* O(logn)\n    * 通过O(logn)的查找，并通过O(1)的插入或删除即可\n    \t* 对于删除，如果被删除的节点在索引中也出现，则需要一并删除\n        \n        * 通常是遍历的时候已经知道，因为上层出现的下层一定有\n        \t* 此时先将该节点的前驱节点保存，以便后面的删除\n            \n\n* 插入删除的时间复杂度不高\n\t* 但是索引需要动态更新\n    \t* 否则极端情况下，跳表还会退化成单链表。\n        \n        \n    * 通过随机函数来维护“平衡性”。\n    \t* 当往跳表中插入数据的时候\n        * 可以选择同时将这个数据插入到部分索引层中。\n        \n        * 比如随机函数生成了值K\n        * 那我们就将这个结点添加到[第一级到第K级]这K级索引中。"},"children":[]},{"data":{"id":"bxy84wr5kpk0","created":1571967962948,"text":"实现","note":"* 随机函数\n\t* 如果跳表是每两个节点抽一个索引出来\n    \t* 则下层有50%的概率会被抽到\n        * 每上升一层都是50%\n        \n    * 因此使用随机函数从0~1中获取\n    \t* 每次超过0.5就加一层，直至断了退出while循环\n        \n  \n* 只有插入使用了随机函数\n\t* 删除没有\n    * 插入可视为已经均匀的插入索引\n    \t* 删除的元素也是随机的，随机的删除一个均匀的链表节点，结果理论上还是均匀的\n        \n        \n"},"children":[]}]},{"data":{"id":"bxyeqd87su80","created":1571986571115,"text":"散列表","expandState":"collapse"},"children":[{"data":{"id":"bxyequyqbsw0","created":1571986609724,"text":"散列思想","note":"* 散列表用的是数组支持按照下标随机访问数据的特性\n\t* 所以散列表其实就是数组的一种扩展，由数组演化而来。\n    \t* 可以说，如果没有数组，就没有散列表。\n        \n    \n* 散列思想\n\t* key（关键字）与数组下标形成一一映射\n    \t* 通过下标对数组进行随机访问，时间复杂度O(1)\n    \n\n\n    "},"children":[]},{"data":{"id":"bxz5ezqda740","created":1572061849700,"text":"散列函数","note":"* 散列函数（Hash函数）\n\t* 将key转化为数组下标的函数\n    \n    * 散列值（Hash值）\n    \t* 通过散列函数计算得到的值\n        \n    * 散列函数的设计要求\n    \t1. 散列函数计算得到的散列值是一个非负整数；\n        \n        2. 如果key1 = key2，那hash(key1) == hash(key2)；\n        \n        3. 如果key1 ≠ key2，那hash(key1) ≠ hash(key2)。\n        \n\n* 散列冲突\n\t* 散列函数设计要求的第三点\n    \t* 比较难实现\n        \n        * 即便像业界著名的MD5、SHA、CRC等哈希算法\n        \t* 也无法完全避免这种散列冲突。\n        * 而且，因为数组的存储空间有限，也会加大散列冲突的概率。"},"children":[]},{"data":{"id":"bxyf7zopiu80","created":1571987952192,"text":"散列冲突","note":"* 装载因子（load factor）\n\t* 用于保证散列表中有一定比例的空闲槽位，否则hash冲突的概率变得很大，查找的时间复杂度甚至退化为O(n)\n    \t* 超过一定值就扩容\n        * 散列表的装载因子=填入表中的元素个数/散列表的长度","expandState":"collapse"},"children":[{"data":{"id":"bxyfc90wo000","created":1571988285978,"text":"开放寻址法","note":"1. 开放寻址法\n\t* 如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。\n    \n* 线性探测\n\t* 当存储位置已经被占用了\n    \t* 我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。\n        * ThreadLocalMap使用的就是这种\n        \n    * 由于查找数据时，找不到继续下一个\n    \t* 所以删除一般不能直接置空。可以标记deleted或者移动其他元素进行填充等\n        \n* 其他开放寻址法\n\t* 二次探测\n    \t* 线性探测是每次加一步\n        \t* hash(key)+0\n            * hash(key)+1\n            * hash(key)+2……\n        \n        * 二次探测是原来的二次方\n        \t* hash(key)+0\n            * hash(key)+1^2\n            * hash(key)+2^2……\n            \n    * 双重散列\n    \t* 不仅仅使用一个散列函数，而是使用一组散列函数\n        * hash1(key)，hash2(key)，hash3(key)……\n        * 先用第一个散列函数，如果计算得到的存储位置已经被占用\n        \t* 再用第二个散列函数，依次类推，直到找到空闲的存储位置。\n \n"},"children":[{"data":{"id":"bxz5kcjhmgo0","created":1572062269403,"text":"优点","note":"* 优点：开放寻址法不像链表法需要拉很多链表。\n\t* 散列表中的数据都存储在数组中\n    \t* 可以有效地利用CPU缓存加快查询速度。\n        \n    * 这种方法实现的散列表，序列化起来比较简单。\n    \t* 链表法包含指针，序列化起来就没那么容易。\n        \n \n* 缺点\n\t* hash冲突的代价更高\n    \t* 如删除的时候需要标记或移动其他元素\n        * 查找的时候找不到要一直往后找\n    * 因为冲突代价高\n    \t* 因此[装载因子的上限]不能太大\n        * 这也导致这种方法比链表法更浪费内存空间。\n        \n        \n* 总结\n\t* 当数据量比较小、装载因子小的时候，适合采用开放寻址法。\n    * 实际应用： ThreadLocalMap"},"children":[]}]},{"data":{"id":"bxyfmgknao00","created":1571989086050,"text":"链表法","note":"* 链表法\n\t* 散列值相同的元素\n    \t* 都放到相同的槽位对应的链表中\n        * 一个数组位置就是一个槽，槽中放链表头元素\n    \n* 查找元素时间复杂度\n\t* O(k)\n    \t* k是链表长度\n        * 均匀的话k = n/m（m是槽的个数）"},"children":[{"data":{"id":"bxz5opsgar40","created":1572062611699,"text":"优点","note":"* 优点\n* 链表法对内存的利用率比开放寻址法要高。\n\t* 因为链表结点可以在需要的时候再创建\n    \t* 并不需要像开放寻址法那样事先申请好。\n        \n        \n* 对大装载因子的容忍度更高。\n\t* 只要散列函数的值随机均匀，即便装载因子变成10，也就是链表的长度变长了而已\n    \t* 虽然查找效率有所下降，但是比起顺序查找还是快很多。\n        \n \n \n \n* 缺点\n* 链表由于需要存储指针\n\t* 当存储的对象比较小时，指针相对比较耗内存\n    \t* 甚至可能使内存翻倍\n    \n    * 存储大对象时，则可以忽略\n    \n \n* 链表的节点是零散的分布在内存中\n\t* 不是连续的，所以对CPU缓存是不友好的\n    \t* 对于执行效率也有一定的影响\n\n\n* 优化\n\t* 链表可以改进为跳表、红黑树\n    * 进入同一个桶后，查找时间复杂度为O(logn)\n    \n* 应用\n\t* 比较适合存储大对象，大数据量的散列表\n    * 更加灵活，支持更多的优化策略\n    \t* 比如用红黑树代替链表。"},"children":[]}]}]},{"data":{"id":"bxz4tvi2po80","created":1572060194844,"text":"打造工业级散列表","note":"* 散列函数的设计\n\t* 散列函数的设计不能太复杂\n    \t* 过于复杂的散列函数，会消耗很多计算时间，也就间接的影响到散列表的性能\n    \n    * 散列函数生成的值要尽可能随机并且均匀分布\n    \t* 减少散列冲突，或者即使冲突，每个操中也分配均匀\n        \n        \n* 数据分析法\n\t* 分析数据的特点，进而设计散列函数\n    \t* 但动态(频繁插入和删除)的数据不一定能预知其特点 \n    \n    \n* 避免低效扩容\n\t* 插入的时间复杂度为O(1),装在因子达到阈值后触发扩容\n    \t* 使用摊还分析法，时间复杂度还是O(1)\n        \n        * 但是如果我们的业务代码直接服务于用户\n        \t* 某一个插入操作特别慢，用户体验就不好\n   \n    * 解决一次性扩容耗时过多\n    \t* 当装载于因子到达阈值\n        \n        * 只申请空间，不迁移全部数据\n        \n        * 当有数据插入时，新数据插入新的散列表，并且从老的散列表中拿出一个数据放入到新散列表。\n        \t* 经过多次插入操作后，老散列表中的数据就一点点全部搬移到新散列表中\n            * 查找的时候先从新的散列表查，再去旧的\n            \n        * 当然插入的时候也看看旧的有没数据，有的话需要做删除旧数据处理","expandState":"collapse"},"children":[{"data":{"id":"bxzade9sizk0","created":1572075830170,"text":"总结","note":"* 工业级的散列表\n\t* 支持快速的查询、插入、删除操作；\n    * 内存占用合理，不能浪费过多的内存空间；\n    \n    * 性能稳定\n    \t* 极端情况下，散列表的性能也不会退化到无法接受的情况。\n        \n        \n* 实现\n\t* 设计一个合适的散列函数；\n    * 定义装载因子阈值，并且设计动态扩容策略；\n    * 选择合适的散列冲突解决方法。"},"children":[]}]},{"data":{"id":"bxzbg6yq26g0","created":1572078870466,"text":"散列表与链表的组合使用","note":"* 散列表这种数据结构\n\t* 虽然支持非常高效的数据插入、删除、查找操作\n\t* 但是散列表中的数据都是通过散列函数打乱之后无规律存储的。\n    \t* 它无法支持按照某种顺序快速地遍历数据。\n        \n    * 没有链表的话，想要按顺序输出数据\n    \t* 则需要全部排序一次\n        * 频繁插入删除的数据则时间复杂度非常大\n        \n\n* 因此通过链表的组合使用来实现排序\n\t* 散列表负责快速定位\n    * 链表实现排序，并可以优化为跳表等","expandState":"collapse"},"children":[{"data":{"id":"bxzbnho6avs0","created":1572079442322,"text":"LRU缓存淘汰算法","note":"* 基于链表的缓存的操作\n\t* 添加一个数据\n    * 删除一个数据\n    * 查找一个数据\n    \n    * 都需要先在链表中查找这个元素\n    \t* 即使是添加，也要看看是否已经存在于链表中\n        \n    * 单纯的链表查找时间复杂度是O(n)\n    \n\n* 结合散列表\n\t* 可以将复杂度降低到O(1)\n    \n    * 如LinkedHashMap的实现（linked说的说就是使用双向链表）\n\t\t* 即 使用双向链表结构存储节点的排序\n        * 另外节点也放在散列表每个桶的单链表中\n        \n    * 查找数据\n    \t* 先通过散列表定位元素：O(1)\n        * 取出数据后，再将访问的节点移到双向链表的尾部\n        \n    * 删除元素\n    \t* 同查找数据的步骤：O(1)\n        * 双向链表可以直接获取前驱节点\n        \t* 因此删除也是O(1)\n            \n    * 添加元素\n    \t* 先通过散列表看是否已经存在此数据\n        * 已经存在的\n        \t* 覆盖且移动到双向链表末尾\n        * 不存在的\n        \t* 看链表（缓存）是否满了，已满则删除链表头元素，当前元素放进链表尾\n            * 未满则放链表尾部元素"},"children":[]},{"data":{"id":"bxzc289vljk0","created":1572080597329,"text":"Redis有序集合","note":"* 在有序集合中，每个成员对象有两个重要的属性\n\t* key（键值）和score（分值）\n    * 可以通过score来查找数据，还会通过key来查找数据。\n    \n \n* 按照分值的排序，以及分值的按区间查找\n\t* 可以根据跳表来实现\n    \n*  根据键值的查找删除\n\t* 使用跳表就需要O(n)了\n    \t* 因为跳表是根据分值来排的\n    * 因此根据键值构造一个散列表\n    \n    * 此处猜测是每一个key对应一个跳表"},"children":[]}]}]},{"data":{"id":"bxzhknwl8cw0","created":1572096147458,"text":"哈希算法","note":"* 注\n\t* hash就是哈希，也叫散列\n\n\n\n* 哈希算法\n\t* 将[任意长度]的二进制值串 映射为固定长度的二进制值串\n    \t* 这个映射的规则就是哈希算法\n        \n    * 哈希值\n    \t* 通过原始数据映射之后得到的二进制值串\n        \n    * 著名Hash算法\n    \t* 比如MD5、SHA等\n        \n        \n        \n        \n        \n* 设计一个优秀的哈希算法\n\t1. 从哈希值不能反向推导出原始数据\n    \t* 所以哈希算法也叫单向哈希算法\n        * 暴力破解的意思是尝试各种情况，通过毫无规律的穷举实现。也不算是反向推导\n        \n    2.  对输入数据非常敏感\n    \t* 哪怕原始数据只修改了一个Bit，最后得到的哈希值也大不相同\n        \n    3.  散列冲突的概率要很小\n    \t* 对于不同的原始数据，哈希值相同的概率非常小；\n     \n    4. 哈希算法的执行效率要尽量高效\n    \t* 针对较长的文本，也能快速地计算出哈希值。","expandState":"collapse"},"children":[{"data":{"id":"bxzi0tr07qo0","created":1572097414008,"text":"安全加密","note":"* 常用的加密的哈希算法（或数字签名）\n\t* MD5\n    \t* MD5 Message-Digest Algorithm\n        * MD5消息摘要算法  \n    * SHA\n    \t* Secure Hash Algorithm\n        * 安全散列算\n        \n\n* 哈希算法四点要求中，用于加密时，有两点格外重要\n\t1. 很难根据哈希值反向推导出原始数据\n    \t* 加密的目的就是防止原始数据泄露\n    \n    2. 散列冲突的概率要很小\n    \t* 理论上是没办法做到完全不冲突的\n        \t* 因为哈希算法产生的哈希值的长度是固定且有限的\n        \n        * 如MD5，哈希值是固定的128位二进制串，能表示的数据是有限的\n        \t* 最多能表示2^128个数据\n            \n        * 如果我们对2^128+1个数据求哈希值\n        \t* 就必然会存在哈希值相同的情况。\n            \n            \n    * 不过，虽然希算法存在散列冲突的情况\n    \t* 但是因为哈希值的范围很大，冲突的概率极低，因此较难破解\n        \n        * 找到真实的值，或者冲突的值就能破解（作为密码的话），要获取真实信息则与此无关\n        \n* 此外，没有绝对安全的加密\n\t* 只是越复杂、越难破解的加密算法，需要的计算时间也越长。","expandState":"collapse"},"children":[{"data":{"id":"bxzixnwmpoo0","created":1572099987305,"text":"加密补充","note":"* SHA 目前比MD5更安全\n\t* MD5已经号称被破解了\n    \n\n* 字典攻击\n\t*  对于一些过于简单的密码\n    \t* 维护一个常用密码的字典表，并记录其hash值\n        * 跟脱库密文（被别人拿到了）对比，即可拿到密码\n        \n    * 解决\n    \t* 加盐salt\n        * 加系统定义的一组固定的随机字符串，与用户的密码组合在一起，增加密码的复杂度\n        \n        * 拿组合之后的字符串来做哈希算法加密，将它存储到数据库中\n        \t* 进一步增加破解的难度。\n            \n         * 加盐的方式有很多种，可以是在头部加，可以在尾部加，还可在内容中间加，甚至加的盐还可以是随机的。\n            \n            \n            \n* 越是复杂哈希算法越难破解，但同样计算时间也就越长。\n\t* 所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。\n        "},"children":[]}]},{"data":{"id":"bxzihczlpq80","created":1572098709713,"text":"唯一标识","note":"* 如在海量的图库中，搜索一张图是否存在\n\t* 整张图的二进制码对比，但是文件太大\n    \t* 转化成二进制十分耗时\n        \n    * 每一个图片取一个唯一标识\n    \t* 或叫信息摘要\n        \n        * 从图片的二进制码串开头取100个字节，从中间取100个字节，从最后再取100个字节\n        \t* 然后将这300个字节放到一块，通过哈希算法（比如MD5），得到一个哈希字符串\n            * 用它作为图片的唯一标识。\n            \n \n* 结合散列表使用\n\t* 将唯一标识与图片地址存进散列表\n    \t* 先对比唯一标识\n        \n    * 唯一标志相同的，散列表中取出图片路径\n    \t* 取出图片做全量对比\n        "},"children":[]},{"data":{"id":"bxzimp3qe0w0","created":1572099127905,"text":"数据校验","note":"* 校验获取的文件是否安全、正确、完整\n\t* 如BT文件下载，会将文件切割成多个文件块\n    \t* 当下载完各个文件块后\n        * 对其做哈希运算，获取哈希值\n        \n        * 与种子文件中的保存的hash值比对\n        \t* 不一致则说明被篡改过"},"children":[]},{"data":{"id":"bxzivgczso00","created":1572099814152,"text":"散列函数","note":"* 散列函数是设计一个散列表的关键。\n\t* 直接决定了散列冲突的概率和散列表的性能。\n    \n* 散列函数中\n\t* 对散列冲突的要求较低\n    \t* 即便出现个别散列冲突，只要不是过于严重\n        \t* 都可以通过开放寻址法或者链表法解决。\n            \n            \n    * 散列值是否能反向解密也并不关心\n    \n    * 更加关注散列后的值是否能平均分布\n    \t* 以及算法执行效率"},"children":[]},{"data":{"id":"bxzjta9taw00","created":1572102465280,"text":"分布式相关应用","expandState":"collapse"},"children":[{"data":{"id":"bxzjqwusk5k0","created":1572102279345,"text":"负载均衡","note":"* 如实现一个会话粘滞（session sticky）的负载均衡算法\n\t* 即一个会话的所有请求都打到一台机器上\n    \t* 建立映射表，会浪费空间（客户端多的话）\n        * 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大；\n        \n    * 使用hash算法\n    \t* 对客户端IP地址或者会话ID计算哈希值\n        * 将取得的哈希值与服务器列表的大小进行取模运算，获得服务器编号。"},"children":[]},{"data":{"id":"bxzjt4l4z540","created":1572102452904,"text":"数据分片","note":"* 统计搜索关键词出现的次数\n\t* 假如有有1T的日志文件\n    \t* 没办法放到一台机器的内存中\n        * 即使放进去也很慢\n    \n    * 先对数据进行分片\n    \t* 多台机器同时处理\n        \n        * 读到搜索关键字时，计算hash值，再跟机器数n取模\n        \t* 获取对应处理的机器数\n            \n        * 最后合并数据\n        \t* 此处没有单独存自己的关键字跟次数\n            * 猜测是不用存储太多相同的key。且合并时不需做加法\n            \n     * 这里的处理过程也是MapReduce的基本设计思想。\n     \n     \n* 判断图片是否在图库中\n\t* 图片很多，如一亿张时\n    \t* 单台机器的内存装不下此散列表\n    * 同样是数据分片处理\n    \t* 每台机器只维护某一部分图片对应的散列表。\n        * 计算hash值，对n取模等\n        \n    * 断一个图片是否在图库中的时候\n    \t* 同样方法找到对应的机器的散列表去查找"},"children":[]},{"data":{"id":"bxzk8qjjib40","created":1572103676159,"text":"分布式存储","note":"* 如分布式缓存\n\t* 海量的数据需要缓存，一个缓存机器肯定是不够的。\n    \t* 于是，我们就需要将数据分布在多台机器上。\n    \n    * 同理多台机器处理\n    \t* 通过哈希算法对数据取哈希值，然后对机器个数取模\n        \t* 这个最终值就是应该存储的缓存机器编号。\n            \n   \n   \n* 一致性hash\n\t* 增加机器时，一般的hash方式需要重新hash\n    \n    * 假设我们有k个机器，数据的哈希值的范围是[0, MAX]。\n    \t* 我们将整个范围划分成m个小区间（m远大于k）\n        * 每个机器负责m/k个小区间。\n        \n        * 当有新机器加入的时候\n        \t* 我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。\n        \n        * 这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。\n        \n        * 实际应用中，还会借助虚拟的环和虚拟的节点"},"children":[]}]}]},{"data":{"id":"bxzy9k5xotc0","created":1572143236179,"text":"树","note":"* 树是一种非线性表结构\n\t* 高度Height\n    \t* 节点的高度 = 节点到叶子节点的最长路径（边数）\n        \n        * 树的高度 = 根节点的高度\n        \n        * 高度是[从下往上]算的\n    \n    * 深度Depth\n    \t* 节点的深度 = 根节点到这个节点的边的个数\n        * 由上往下\n    \n    * 层Level\n    \t* 节点的层数 = 节点的深度 +1\n        * 从1开始，由上往下","expandState":"expand"},"children":[{"data":{"id":"bxzy9u9nfog0","created":1572143258171,"text":"二叉树","note":"*  二叉树\n\t* 每个节点最多两个分叉\n    \n* 满二叉树\n\t* 叶子节点全都在最底层\n    * 除了叶子节点之外，每个节点都有左右两个子节点\n    \n    \n* 完全二叉树\n\t* 叶子节点都在最底下两层\n    * 最后一层的叶子节点都靠左排列\n    \t* 并且除了最后一层，其他层的节点个数都要达到最大\n        \n    * 满二叉树又是完全二叉树的一种特殊情况\n        \n        \n","expandState":"collapse"},"children":[{"data":{"id":"bxzzdru2bbk0","created":1572146387442,"text":"二叉树的存储","note":"* 链式存储法\n\t* 基于指针或者引用的二叉链式存储法\n    \n* 顺序存触法\n\t* 基于数组\n    \n    * 根节点存储在下标为1的位置\n    \t* 左子节点：2 * i\n        * 右子节点: 2 * i + 1\n        \n        * 反之：当前节点是i，则父节点是 i/2(右子节点也成立)\n        * 公式适合全部节点\n     \n    * 根节点存储在1的位置是为了方便计算子节点\n    \t* 完全二叉树中，会浪费一个下标为0的位置\n        \n        * 非完全二叉树则会浪费更多的数组存储空间\n    \n    \n* 所以\n\t* 如果某棵二叉树是一棵完全二叉树\n    \t* 那用数组存储无疑是最节省内存的一种方式。\n    \t* 因为数组不需要存储额外的左右子树的指针\n        \n        \n    * 这也是为什么完全二叉树要求最后一层的子节点都靠左的原因。\n        "},"children":[]},{"data":{"id":"bxzzf0jpaa00","created":1572146484770,"text":"二叉树的遍历","note":"* 前序遍历\n\t* 对于树中的任意节点来说\n    \t* 先打印这个节点\n        * 然后再打印它的左子树\n        * 最后打印它的右子树。\n\n* 中序遍历\n\t* 对于树中的任意节点来说\n    \t* 先打印它的左子树\n        * 然后再打印它本身\n        * 最后打印它的右子树\n        \n        \n* 后序遍历\n\t* 对于树中的任意节点来说\n    \t* 先打印它的左子树\n        * 然后再打印它的右子树\n        * 最后打印这个节点本身。\n        \n* 即：\n\t* 当前节点在最前就是前序\n    * 中间就是中序\n    * 最后就是后序\n    \n \n* 二叉树遍历的时间复杂度\n\t* 每个节点最多会被访问两次\n    * 所以遍历操作的时间复杂度，跟节点的个数n成正比\n    \t* O(n)"},"children":[]}]},{"data":{"id":"by05mtow4a80","created":1572164023420,"text":"二叉查找树","note":"* 二叉查找树（Binary Search Tree）\n\t* 也叫二叉搜索树\n    \t* 顾名思义，二叉查找树是为了实现快速查找而生的\n    \n    * 支持快速查找、快速插入、删除一个数据。\n    \n\n* 结构\n\t* 在树中的任意一个节点\n    \t* 其左子树中的每个节点的值，都要小于这个节点的值\n        * 而右子树节点的值都大于这个节点的值。\n        \n   \n* 查找\n\t* 根节点开始遍历\n    \t* 当前节点等于查找值，返回当前\n        * 比当前节点小，找左子树，否则找右子树\n     \n     \n* 插入\n\t* 类似查找，直到找到一个空的叶子节点\n    \n* 删除\n\t* 删除的节点没有子节点\n    \t* 将父节点中的指向当前节点的指针置为空\n        \n    * 有一个子节点\n    \t* 父节点的指针指向删除节点的子节点\n    * 有2个子节点\n    \t* 将右子树的最小子节点与删除节点换位置，删除该最小节点\n        * 此处，使用左子树的最大节点应该也是可以的","expandState":"collapse"},"children":[{"data":{"id":"by067v5tbqg0","created":1572165672267,"text":"查找树扩展","note":"* [中序遍历]二叉查找树\n\t* 可以输出有序的数据序列\n    \t* 时间复杂度是O(n)，非常高效。\n        \n        \n        \n* 支持重复数据的二叉查找树\n\t* 实际开发中，在二叉查找树中存储的，是一个包含很多字段的对象。\n    \t* 利用对象的某个字段作为键值（key）来构建二叉查找树\n        * 其他字段叫作卫星数据。\n    \n    * 方法1\n    \t* 二叉查找树每一个节点不仅存储一个数据\n        * 通过链表和支持动态扩容的数组等数据结构\n        \t* 把值相同的数据都存储在同一个节点上。\n            \n    * 方法2\n    \t* 每个节点仍然只存储一个数据\n        \n        * 如果碰到一个节点的值，与要插入数据的值相同\n        \t* 我们就将这个要插入的数据放到这个节点的右子树\n            * 即把这个新插入的数据当作大于这个节点的值来处理。\n            \n            \n        * 查找数据的时候，遇到值相同的节点，并不停止查找操作\n        \t* 继续在右子树中查找，直到遇到叶子节点\n            * 这样就可以把键值等于要查找值的所有节点都找出来。\n         \n         \n        * 删除同理"},"children":[]},{"data":{"id":"by06cum1vnk0","created":1572166062893,"text":"时间复杂度","note":"* 一般的二叉查找树\n\t* 不保证平衡性\n    \n    * 因此极端情况下退化为链表\n        * 查找时间复杂度：O(n)\n    \n    * 最理想，是一棵完全二叉树\n    \t* 插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是O(height)\n        * 即每层判断一个数据\n    \n    * 完全二叉树，时间复杂度等于O(height)\n    \t* 高度为log2n（2为底）\n        * 所以：O(logn)\n        \n        * 推导公式见下\n        * 理解：每一层的数据以指数增长\n        \t* 则查找就是以对数式的增长\n    \n"},"children":[{"data":{"id":"by0ao8j284o0","created":1572178239632,"text":"树的高度","note":"* 完全二叉树的高度\n\t* 可转为为层数便于运算\n    \n\t* 第一层1个元素\n    * 第二层2个元素\n    * 第三层4个元素\n    ...\n    * 第L层 2^(L-1)个元素\n    \n    * 完全 二叉树最后一层不一定是满的，即介于两者之间\n    \t* n >= 1+2+4+8+...+2^(L-2)+1\n        \t* 完全二叉树最坏的情况，最后一排是1个节点\n        \t* L <= log2n +1\n            \n\t\t* n <= 1+2+4+8+...+2^(L-2)+2^(L-1)\n        \t* L>=log2(n+1)\n            \n        * 层数小于等于log2n +1\n        * 高度小于等于log2n（2是底）\n        \n \n* 代码求树的真实高度\n\t* 递归每个子树\n    \t* 当前节点高度等于左右子树最大的一个 再加1。"},"children":[]}]},{"data":{"id":"by078wmll140","created":1572168574933,"text":"对比散列表","note":"1. 有序性\n\t* 散列表中的数据是无序存储的\n    \t* 如果要输出有序的数据，需要先进行排序。\n        \n    * 而对于二叉查找树来说，我们只需要[中序遍历]\n    \t* 就可以在O(n)的时间复杂度内，输出有序的数据序列。\n \n \n2. 扩容、散列冲突\n\t* 散列表扩容耗时很多\n    * 而且当遇到散列冲突时，性能不稳定\n    * 尽管二叉查找树的性能不稳定\n    \t* 但工程中，最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在O(logn)\n        \n  \n3. 尽管散列表的查找等操作的时间复杂度是常量级的\n\t* 但因为哈希冲突的存在，这个常量不一定比logn小\n    \t* 所以实际的查找速度可能不一定比O(logn)快\n        \n    * 加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。\n    \n4. 散列表的构造比二叉查找树要复杂\n\t* 散列函数的设计、冲突解决办法、扩容、缩容等。\n    \n    * 平衡二叉查找树只需要考虑平衡性这一个问题\n    \t* 而且这个问题的解决方案比较成熟、固定。\n        \n5. 最后，为了避免过多的散列冲突，散列表装载因子不能太大\n\t* 特别是基于开放寻址法解决冲突的散列表\n    \t* 太小则会浪费一定的存储空间。"},"children":[]}]},{"data":{"id":"by0r0v7dm2g0","created":1572224367120,"text":"平衡二叉查找树","note":"* 平衡二叉树(注意此处没有查找二字)\n\t* 严格定义\n    \t* 二叉树中任意一个节点的左右子树的高度相差不能大于1\n        \n    * 按照严格的定义\n    \t* 完全二叉树、满二叉树其实都是平衡二叉树\n        * 非完全二叉树也[有可能]是平衡二叉树。\n        \n   \n* 平衡二叉查找树\n\t* 不仅满足上面平衡二叉树的定义\n    * 还满足二叉[查找]树的特点\n    \n    * 最先被发明的平衡二叉查找树是AVL树\n    \t* 它严格符合我刚讲到的平衡二叉查找树的定义\n        * 即任何节点的左右子树高度相差不超过1\n        \t* 是一种高度平衡的二叉查找树。\n            \n       \n* 很多平衡二叉查找树其实并没有严格符合上面的定义\n\t* 即：树中任意一个节点的左右子树的高度相差不能大于1\n    \n    * 比如我们下面要讲的红黑树\n    \t* 它从根节点到各个叶子节点的最长路径，有可能会比最短路径大一倍。\n    \n    \n* 平衡\n\t* 初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。\n    \t* 只要树的高度不比log2n大很多（比如树的高度仍然是对数量级的）\n        \n        * 尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。"},"children":[{"data":{"id":"by0rb9wtqpk0","created":1572225182776,"text":"红黑树","note":"* 红黑树\n\t* 它是一种不严格的平衡二叉查找树\n\n\n* 特点\n\t1. 根节点是黑色的\n    \n    2. 每个叶子节点都是黑色的空节点（NIL）\n    \t* 即：叶子节点不存储数据\n        \n    3. 任何相邻的节点都不能同时为红色\n    \t* 即：红色节点是被黑色节点隔开的；\n        \n    4. 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点\n    \n    \n"},"children":[{"data":{"id":"by0rr3wlmgo0","created":1572226423528,"text":"红黑树的近似平衡","note":"* 红黑树的近似平衡\n\t* 平衡二叉查找树的初衷\n    \t* 是为了解决二叉查找树因为动态更新导致的性能退化问题\n        \n    * “平衡”的意思可以等价为性能不退化。\n    \t* “近似平衡”就等价为性能不会退化的太严重\n    \n    * 只要树的高度稳定趋近于log2n(2为底)\n    \t* 则树就是近似平衡的\n  \n  \n* 近似平衡证明\n\t1. 将红黑树的红色节点拿掉、\n    \t* 黑色节点连上黑色的祖父节点\n        * 二叉树变成四叉树\n        \n    2. 由于黑色节点在每条路径的节点数一样\n    \t* 四叉树中取出部分节点放到叶子节点位置\n        * 四叉树就变成了完全二叉树\n        \t* 节点相同，所以四叉树的底是平的\n            * 取出四叉中的两叉，是满二叉树\n            * 取出的二叉放到叶子节点（自己找位置），是完全二叉树\n        \n    3. 因此，调整前的黑色四叉树，高度小于完全二叉树的高度log2n(2为底)\n    \n    4. 红色节点放回去\n    \t* 红色节点不能连续\n        * 所以红黑树高度最大\n        \t* 小于 2* log2n\n        \n    5. 所以，红黑树的高度只比高度平衡的AVL树的高度（log2n）最多仅仅大了一倍\n    \t* 在性能上，下降得并不多\n        * 实际上红黑树的性能比当前推导的更好。"},"children":[]}]},{"data":{"id":"by0s2makcpc0","created":1572227325560,"text":"其他平衡二叉查找树","note":"* Splay Tree（伸展树）、Treap（树堆）等\n\t* 绝大部分情况下，它们操作的效率都很高\n    \t* 但是也无法避免极端情况下时间复杂度的退化。\n        \t* 猜测，到达某一阈值才会触发平衡机制\n            * 导致某次操作时间过长\n            \n            \n        * 尽管这种情况出现的概率不大，但是对于[单次操作时间]非常敏感的场景来说，它们并不适用。\n        \n        \n        \n* AVL树是一种高度平衡的二叉树\n\t* 所以查找的效率非常高\n    \n    * 但是，AVL树为了维持这种高度的平衡，就要付出更多的代价。\n    \t* 每次插入、删除都要做调整，就比较复杂、耗时。\n        \n     * 所以，对于有频繁的插入、删除操作的数据集合，使用AVL树的代价就有点高了。\n     \n     \n* 红黑树只是做到了近似平衡\n\t* 并不是严格的平衡\n    \t* 所以在维护平衡的成本上，要比AVL树要低。\n        \n    * 红黑树的插入、删除、查找各种操作性能都比较稳定。"},"children":[]}]}]}]},"template":"right","theme":"fresh-green","version":"1.4.43"}