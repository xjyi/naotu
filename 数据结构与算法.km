{"root":{"data":{"id":"bxcash4fucw0","created":1569741132836,"text":"数据结构与算法","note":"1. 广义\n\t* 数据结构\n    \t* 指一组数据的存储结构\n    * 算法\n    \t* 操作数据的一组方法\n\n2. 狭义\n\t* 某些著名的数据结构和算法\n    \t* 比如队列、栈、堆、二分查找、动态规划\n        * 这些都是前人智慧的结晶"},"children":[{"data":{"id":"bxxkv0b7e6g0","created":1571902301521,"text":"基础数据结构与算法","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxp1qn1ya480","created":1571035878465,"text":"数组","note":"* 数组（Array）\n\n1. 是一种线性表 数据结构\n    * 数据排成像一条线一样的结构\n    \n    * 每个线性表上的数据，最多只有前后两个方向\n        * 链表、队列、栈等也是线性表结构\n        * 二叉树、堆、图等是非线性表\n        \n2. 它用一组连续的内存空间，来存储一组具有相同类型的数据。\n\n\n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxp20zls5pc0","created":1571036689428,"text":"读","note":"* 数组读比较快\n\t* 是因为数据可以进行随机访问\n    \t* 基于连续的内存空间和相同的数据类型\n        * 首地址 + 角标 * 每个元素大小\n    * 数组支持随机访问，根据下标随机访问的时间复杂度为O(1)\n    \t* 注意不能说数组查找时间复杂度为O(1)\n        \n        \n* 根据寻址公式，所以下标从0开始 更加简便，无需做减1操作\n\t* 角标相当于一个偏移量\n    * 而java等语言沿袭了C语言中的设计，所以下标从0开始\n    \n    \n* 关于二维数组\n\t* 猜测地址是从先按照列，再到行\n    \n    * 对于 m * n 的数组，a [i][j] (i < m,j < n)的地址为：\n\n\t* address = base_address + ( i * n + j) * type_size\n    ","layout":null},"children":[]},{"data":{"id":"bxp2121igjk0","created":1571036694732,"text":"插入、删除","note":"* 插入\n\t* 随机插进一个数组的一个位置，并且该位置后的元素都往后移动一位\n    \t* 时间复杂度为O(n)\n        * 如插入一个有序结构\n        \n    * 不需要移动\n    \t* 如直接将该位置元素放到最后\n        * 时间复杂度：O(1) \n        * 快排中有使用到这种思想\n        \n        \n        \n* 删除\n\t* 删除数组中随机一个元素，并移动其他元素，保证内存的连续性\n    \t* 时间复杂度：O(n)\n        \n    * 如果不需要保证内存连续性\n    \t* 如只标记为已删除，待空间不够再真正删除\n        * 时间复杂度猜测为O(1)\n        * 与JVM的标记清除算法思想类似","layout":null},"children":[]},{"data":{"id":"bxp2lkd4gbs0","created":1571038301900,"text":"与容器比较","note":"* 容器，如ArrayList等\n\t* 优点\n    \t* 可以将很多数组操作的细节封装起来，如：支持动态扩容\n        * ArrayList是按照1.5倍数动态扩容\n        \t* 扩容操作涉及内存申请和数据搬移，如果事先能确定需要存储的数据大小，最好在创建ArrayList的时候事先指定数据大小。\n        \n        \n        \n      \n      \n      \n* 数组使用场景\n    * ArrayList无法存储基本类型，而自动装箱、拆箱有一定的性能消耗\n        * 所以特别关注性能，或者希望使用基本类型，就可以选用数组\n        \n    * 数据大小事先已知，且要做的操作简单,用不到ArrayList的大部分方法\n    \t* 则可以使用数组\n    \n    * 另外多维数组时，Object[][]\n    \t* 看起来更加直观（个人喜好）\n\n\n\n\n* 一般使用容器就足够了，除非想把性能做到极致\n    \t\n        ","layout":null},"children":[]}]},{"data":{"id":"bxp5b1zpio80","created":1571045941595,"text":"链表","note":"* 对比数组\n\t* 数组需要连续的内存\n\t\t* 链表不需要\n        \n    * 复杂度刚好相反\n    \t* 数组插入O(n),查询O(1)\n        * 链表插入O(1),查询O(n)\n        \n        \n        \n    * 数组实现上使用的是连续的内存空间\n    \t* 可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高\n        \n        * CPU每次从内存读取数据并不是只读取那个特定要访问的地址\n        \t* 而是读取一个数据块\n            * 下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取\n     \n     \n     \n    * 数组扩容，需要将数据重新复制一份。费时","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxp5bct6r7s0","created":1571045965145,"text":"关于内存淘汰策略","note":"* 先进先出策略FIFO（First In，First Out）\n\n* 最少使用策略LFU（Least Frequently Used）\n\n* 最近最少使用策略LRU（Least Recently Used）\n\t* linkedHashMap就是利用双向链表来实现的LRU","layout":null},"children":[]},{"data":{"id":"bxp5f7ahxbs0","created":1571046266588,"text":"各种链表","note":"\n        \n        \n* 单链表\n\t* 每个链表节点\n    \t* 除存储数据外\n        * 还有一个后继指针next，记录下一个节点的地址\n        \n    * 头结点记录链表的基地址\n    * 尾结点指向空地址null\n    \n    * 删除操作的时间复杂度\n    \t* 不需移动其他节点，因此是O(1)\n    * 查找\n    \t* 无法使用寻址公式\n        * 因此时间复杂度：O(n)\n        \n        \n* 循环链表\n\t* 尾节点的next指向头节点\n    \n* 双向链表\n\t* 除了一个next后继节点\n    \t* 还有一个prev前驱节点\n        \n    * 占用更多空间，但是更加灵活\n    \t* 反正就是可以获取前面的节点，不需要从头开始遍历\n        \n* 双向循环链表\n\t* 如描述，双向，且是环状","layout":null},"children":[]},{"data":{"id":"bxpc76i3f140","created":1571065385726,"text":"单向链表判断是否回文","note":"1. 使用快慢指针定位链表中间节点\n\t* 快指针一次走两步\n    * 慢指针一次走一步\n    \t* 则快指针走完时，满指针走到中间\n        * 注意判断奇偶数，可根据快指针进行判断\n\n2. 慢指针走的时候，将前半部分反序\n\n3. 前后半部分比较，判断是否为回文\n\n4. 前半部分逆序复原\n\n5. 时间复杂度O(n), 空间复杂度O(1)\n\n* 代码见github","layout":null},"children":[]},{"data":{"id":"bxq4lvas1dc0","created":1571145527879,"text":"算法代码技巧","note":"1. 哨兵节点\n\t* 在链表的头部等边界地方，设置哨兵\n    \t* 可以将边界的特殊处理变成普通处理\n        \n    * 如带头链表的头即为哨兵 \n    \t* 则所有节点的插入删除逻辑都一样了\n        \n    * 又如：在数组中找值为x的节点\n    \t* 先判断数组尾部的的是否符合\n        * 不符合的话，替换尾部节点为目标x\n        \t* 符合直接返回\n        * 从头开始遍历数组，直至找到x\n        \t* x不是最后一个，则找到\n          \t* x是最后一个，表示数组中没这个元素\n        \n        * 优化点：无需判断数组是否角标越界。直接判断是否等于x即可\n        \t* 大数据量中，少一个判断也是很重要的\n            * 当然平时不要这样写。。。\n   \t","layout":null},"children":[]}]},{"data":{"id":"bxremdslruw0","created":1571275339195,"text":"栈","note":"* 栈是一种 操作受限 的线性表\n\t* 先进后出，后进先出\n    \t* 只允许一端插入、删除数据\n        \n        \n* 相比链表、数组，栈带来的只有限制\n\t* 但是在特定场景，栈能够满足需求\n    \t* 同时更加可控，不容易出错\n        \n      \n* 数组实现的栈\t\n\t* 叫：顺序栈\n\n* 链表实现的栈\t\n\t* 叫：链式栈\n    \n    \n    \n* 栈 操作的时间、空间复杂度\n\t* 时间复杂度\n    \t* O(1) 因为每次都只操作顶部的一个元素\n    \n    * 空间复杂度\n    \t* O(1) 只需一两个临时变量存储空间\n        * 动态扩容的顺序栈，到达某一临界大小时扩容，迁移数据到更大的内存\n        \t* 此时可用摊还分析法，将数据搬移工作均摊到每一次入栈操作\n            * 时间复杂度还是O(1)\n\n\n* 注意：\n\t* 存储的空间都是一个大小为n的数组或链表，不代表空间复杂度就是O(n)\n        * 因为这个空间是必须的，无法省掉\n        * 我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxrf07ku5pk0","created":1571276422763,"text":"栈的应用","note":"1. 函数调用栈\n\t* 每进入一个函数\n    \t* 就会将临时变量作为一个栈帧入栈\n    * 当被调用函数执行完成，返回之后\n    \t* 将这个函数对应的栈帧出栈\n        \n\n2. 表达式\n\t* 传入一串表达式，如:3+ 5* 4-2\n    * 编译器通过两个栈实现\n    \t* 一个保存操作数的栈\n        * 一个是保存运算符的栈\n        \n    * 从左往右遍历表达式\n    \t* 遇到数字，压入操作数栈\n        * 遇到运算符\n        \t* 与运算符栈顶的元素比较\n    * 比较规则\n    \t* 先确定所有的优先级\n        * 当前比运算符栈顶的元素优先级高，压入栈顶\n        * 等级更低或者等级相同，取出栈顶运算符先运算\n        \n        \n        \n3. 检查各种括号()[]{}等是否匹配\n    * 遍历表达式。遇到坐括号就压入栈中\n    \t* 遇到右括号就取出栈顶元素进行匹配\n        * 全部能匹配则通过\n        \n\n4. 浏览器的前进后退\n\t* 使用2个栈\n    \t* 点后退，就将左栈栈顶页面移到右栈\n        * 点前进，就将右栈栈顶元素移到左栈\n        * 点了其他页面，清空右栈\n    \n","layout":null},"children":[]}]},{"data":{"id":"bxrfxglglm00","created":1571279028409,"text":"队列","note":"* 也是操作受限的线性表数据结构\n\t* 先进先出\n    \n* 数组实现的队列\n\t* 顺序队列\n    \n* 链表实现的队列\n\t* 链式队列\n    \t* 实现逻辑相对简便\n    \n    \n","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxsasicvh1s0","created":1571366094829,"text":"顺序队列实现","note":"* 顺序队列的实现\n\t1. 正常的数组，取出head元素后，head指针往前一步\n    \t* 会导致即使数组中有空位，也无法继续插入数据\n    \n    \n    2. 解决方案\n    \t* 当数据入队时已经满了，则触发数据搬移\n        * 每个元素一次性移动n位（直接赋值，n是角标是0与head的距离）\n        \n        \n    3. 循环队列解决方案\n    \t* 数组的最后元素指向head，则形成环状\n        * 无需数据搬移，性能更好\n        \n        * head 与 tail之间留一个空位不存数据\n        \t* 此处猜测是沿用非环状数组的用法，但是非环状数组会在满了之后，也利用最后一个空位\n        \n        * 队空的判断不变，都是head==tail\n\n\n\n* 队满的条件：(tail+1)%n=head\n\t* n是数组大小，tail是从零开始，因此tail+1不可能比n大。\n    \t* 相当于tail + 1 = head\n        \n    * 使用取模的意义在于临界点的判断\n    \t* tail是最大角标n-1，head是0，此时 n-1+1%n=0\n        * 即完成一圈就将数字重置为0，也就刚刚好是取模的用法\n        \n    * head、tail的移动都需要按照此公式","layout":null},"children":[]},{"data":{"id":"bxsb5yd7u940","created":1571367148413,"text":"关于阻塞队列","note":"* 一般无界的队列，就是链式队列实现的","layout":null},"children":[]}]},{"data":{"id":"bxocsqnvixs0","created":1570965515302,"text":"复杂度分析","note":"1. 通过统计，监控等获取算法的执行时间及占用内存\n\t* 叫事后统计法\n    \t* 依赖测试环境，如硬件。还有数据规模等\n    * 因此不准确","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxoctcl8pe00","created":1570965563032,"text":"时间复杂度","note":"* 大O时间复杂度\n\t* 表示代码执行时间随数据规模增长的变化趋势\n    * 也叫渐进时间复杂度\n    \t* 简称时间复杂度\n        \n        \n* 时间复杂度分析\n\t* 大O的复杂度表示方法，表示的是一种变化趋势\n    \t* 因此一般会忽略常量、低阶，系数等\n        * 只记录最大阶的量级即可\n        \t* 如循环执行次数最多的一段代码\n            \n        \n        \n* 加法法则\n    * 总复杂度等于量级最大的那段代码的复杂度\n        * 常量级别的代码（与n五无关的），即使执行上万遍，复杂度都是 O(1)\n        * 因为对变化趋势没有影响\n        \n\t* 如代码的复杂度里有O(n),O(n^2)， 则只看最大的O(n^2)\n        \n  \n* 乘法法则\n    * 嵌套代码的复杂度等于嵌套内外代码复杂度的乘积\n    \t* 如O(n) 与 O(n^2) 的乘积是O(n^3)\n        \n        \n* 多个数据规模, 即数据规模不止一个\n\t* 如 m n\n    \t* 由于两个数字都是变化的\n        * 因此复杂度根据两数据规模一起决定\n        \n        * 如O(m+n) , O(m* n) 等\n   ","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"bxodo0eh8a00","created":1570967965791,"text":"复杂度量级","note":"* 多项式量级(按数量级递增)\n\t* 常数阶\n    \t* O(1)\n        * 执行了很多行，只要跟数据规模没关系，就是O(1)\n        \n    * 对数阶\n    \t* O(logn)\n        \n    * 线性阶\n    \t* O(n)\n        \n    * 线性对数阶\n    \t* O(nlogn)\n        \n    * 平方阶，立方阶...k次方阶等\n    \t* O(n^2) O(n^3)\n\n\n\n\n\n\n* 非多项式量级\n\t* O(2^n)和O(n!)\n    \t* 指数阶，阶乘阶\n\n        \n    * 是非常低效的算法\n    \t* 数据规模越来越大时，非多项式量级算法的执行时间会急剧增加","layout":null},"children":[]},{"data":{"id":"bxoe1xbbt5s0","created":1570969056168,"text":"对数阶","note":"* O(logn)\n\t* 对数阶的复杂度表示方法中\n    \t* 忽略对数的底\n     \n\t* 因为任何底数的对数\n    \t* 都可以转化为：其他底数的对数 乘以 一个常数\n        * 见有道笔记，此处不好表示底数\n        \n        \n        \n* 对于线性对数阶O(nlogn)\n\t* 相当于O(logn)的代码循环执行了n遍\n    \t* 乘法法则\n        \n    * 归并排序、快速排序的时间复杂度都是O(nlogn)","layout":null},"children":[]},{"data":{"id":"bxofdyorb8g0","created":1570972820637,"text":"最好、最坏情况时间复杂度","note":"* 最好\n\t* 在最理想的情况下，执行这段代码的时间复杂度\n \n* 最坏\n\t* 在最糟糕的情况下，执行这段代码的时间复杂度","layout":null},"children":[]},{"data":{"id":"bxofe9y9uuw0","created":1570972845157,"text":"平均情况时间复杂度","note":"* 通常是将每种情况的耗时相加\n\t* 除以N种情况的个数\n    \t* 得到一个平均值\n        \n        \n* 但是，不算上各种情况的概率是不准确的\n\t* 因此，应该每种情况单独算上出现的概率\n    \t* 得到加权平均值\n        \n\n* 因此，也叫\n\t* 权平均时间复杂度\n    \t* 或者期望时间复杂度\n   \n   \n   \n* 注\n\t* 一般使用一个复杂度就可以了\n    \n    * 只有同一块代码在不同的情况下，时间复杂度有[量级的差距]\n    \t* 我们才会使用这三种复杂度表示法来区分\n\t","layout":null},"children":[]},{"data":{"id":"bxofedd85ls0","created":1570972852591,"text":"均摊时间复杂度","note":"* 相比前面的复杂度分析，使用场景更加特殊、更加有限。\n\n\n* 摊还分析法\n\t* 通过摊还分析得到的时间复杂度\n    \t* 叫均摊时间复杂度\n    \n    * 能够通过摊还分析法处理，就不用使用加权平均去算，会更加简便\n     \n     \n     \n* 使用场景：对于一些特殊的复杂度分析\n\t* 例如，经过n个的O(1) 操作之后\n    \t* 有一个 O(n) 的操作\n    * 并且是有规律的，有一定时序关系的\n    \n    * 则可以将该次O(n) 的操作，均摊到每个O(1)操作\n    \t* 总体复杂度就是O(1)\n      \n      \n      \n* 在能够应用均摊时间复杂度分析的场合\n\t* 一般均摊时间复杂度就等于最好情况时间复杂度\n    \n    * 可以认为\n    \t* 均摊时间复杂度就是一种特殊的平均时间复杂度","layout":null},"children":[]}]},{"data":{"id":"bxoctj3oixk0","created":1570965577208,"text":"空间复杂度","note":"* 渐进空间复杂度\n\t* 表示算法的存储空间与数据规模之间的增长关系\n    \n* 常见的空间复杂度就是O(1)、O(n)、O(n^2)  \n\t* 如对于传入的n个参数\n    \t* new对应大小的数组\n        * 则空间复杂度就是O(n)\n    ","layout":null},"children":[]}]},{"data":{"id":"bxsbdf7bmpk0","created":1571367733611,"text":"递归","note":"* 递归需要满足的三个条件\n\t* 一个问题的解可以分解为几个子问题的解\n    \n    * 这个问题与分解之后的子问题，除了数据规模不同\n    \t* 求解思路完全一样\n    \n    * 存在递归终止条件\n    \n    \n* 因此递归的关键在于\n\t* 写出递推公式\n    * 找到终止条件\n    \n    \n* 思维误区\n\t* 不要尝试去思考、层层分解每一层子问题，子子问题的关系\n    \t* 只考虑当前问题与子问题的关系即可（算出递推公式）\n      \n      \n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxsbnzamu600","created":1571368560988,"text":"递归的问题","note":"* 警惕栈溢出\n\t* 解决：定义全局变量，每执行一次+1，超过一定深度直接报错返回\n    \t* 估算的阈值不一定准确，因为跟当前线程剩余栈空间大小有关\n        * 因此不太实用，一般是深度较少的时候才使用\n        \n    * 因为递归调用一次就会在内存栈中保存一次现场数据\n    \t* 因此空间复杂度也会变大\n        * 如电影院的逐行递归，空间复杂度是O(n)\n        \n    \n        \n        \n        \n* 重复计算\n\t* 递归可能造成大量重复计算\n    \t* 如递归分成左右2子问题，子问题间可能会有重复\n        \n    * 解决\n    \t* 使用map存起来，递归调用前判断\n        \n* 递归的函数调用较多，数量大时时间效率较低","layout":null},"children":[]},{"data":{"id":"bxsbyvd6d6g0","created":1571369414441,"text":"优缺点","note":"* 优点\n\t* 递归代码的表达力很强，写起来非常简洁\n    \n* 缺点\n\t* 空间复杂度高、有堆栈溢出的风险\n    * 存在重复计算、过多的函数调用会耗时较多等问题\n    \n    \n    \n* 所有的递归代码都可以改为迭代循环的非递归写法\n\t* 因为递归本身就是借助栈来实现的\n    * 只不过我们使用的栈是系统或者虚拟机本身提供的\n    \t* 如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程\n        * 这样任何递归代码都可以改写成看上去不是递归代码的样子。\n\n\n* 一般思路是从底部结束条件往前迭代\n\n* 但是这种思路实际上是将递归改为了“手动”递归\n\t* 本质并没有变，\n\t* 而且也并没有解决前面讲到的某些问题，[徒增]了实现的复杂度。\n    \n    \n* 数据规模大，就要用这种方式了，避免递归导致栈溢出","layout":null},"children":[]},{"data":{"id":"bxsfkqgd6a00","created":1571379591091,"text":"递归断点","note":"* 对递归的值打日志\n\t* 并且结合 条件断点 使用\n    \t* idea断点->右键，设置条件","layout":null},"children":[]}]}]},{"data":{"id":"bxte8kh5enc0","created":1571477376555,"text":"排序","note":"* 有序度，逆序度\n\t* 对于一组数据\n    \t* 排列组合是 n！种\n        \n    * 有序元素\n    \t* 如果i<j ，则a[i] <= a[j]\n        \n    * 有序度\n    \t* 数组中具有有序关系的元素对的个数\n        * 完全有序的，又称满有序度\n        \t* 为：n*(n-1)/2\n            * (n-1)+(n-2)+...+1\n        * 完全逆序的，有序度就是0\n        \n    * 逆序度\n    \t* 逆序度=满有序度-有序度","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxtemf23kk80","created":1571478461859,"text":"如何评价、分析一个排序算法","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxtemp74gkw0","created":1571478483931,"text":"排序算法的执行效率","note":"* 排序算法的执行效率\n\n\t1. 最好情况、最坏情况、平均情况时间复杂度\n    \t* 要能说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的\n        \n    \t* 有些算法会区分三种复杂度，为了对比，所以最好都做一下区分\n        \n        * 不同有序度的数据，对执行时间肯定有影响\n        \t* 我们要知道排序算法在不同数据下的性能表现\n            \n            \n\t2. 时间复杂度反应的是数据规模n很大的时候的一个增长趋势\n    \t* 实际开发中。数据规模可能并不高\n    \t* 因此在对[同一阶]时间复杂度的排序算法性能对比的时候\n        \t* 我们就要把系数、常数、低阶也考虑进来。\n   \t\n    \n    3. 比较次数和交换（移动）次数\n    \t* 基于比较的排序算法会涉及两种操作\n        \t* 元素比较大小\n            * 元素交换或移动。","layout":null},"children":[]},{"data":{"id":"bxten0943ts0","created":1571478507995,"text":"排序算法的内存消耗","note":"* 算法的内存消耗可以通过空间复杂度来衡量\n\n* 原地排序（Sorted in place）\n\t* 特指空间复杂度是O(1)的排序算法","layout":null},"children":[]},{"data":{"id":"bxtenme8xaw0","created":1571478556195,"text":"排序算法的稳定性","note":"* 稳定性\n\t* 如果待排序的序列中存在值相等的元素\n    \t* 经过排序之后，相等元素之间原有的先后顺序不变。\n        \n        * 相等元素位置不变就是稳定的排序算法\n        \t* 否则就是不稳定的\n            \n  \n* 应用\n\t* 如实际开发中，先对金额进行排序，再按照创建时间进行排序\n    \t* 方案1 先按照金额排序，对于金额相同的，再进行排序\n        \t* 实现起来复杂\n        \n        * 方案2 先对时间进行排序，再对金额进行排序\n        \t* 使用稳定的排序算法可以非常简洁得实现","layout":null},"children":[]}]},{"data":{"id":"bxte9ajleqg0","created":1571477433299,"text":"O(n^2) :冒泡、插入、选择","note":"* 为什么插入排序要比冒泡排序更受欢迎\n\t* 从代码实现上来看\n    \t* 冒泡排序的数据交换要比插入排序的数据移动要复杂\n        * 冒泡排序需要3个赋值操作，而插入排序只需要1个\n        \n        \n* 所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是O(n2)\n\t* 但是如果我们希望把性能优化做到更好，那肯定首选插入排序。\n    * 即考虑算法的系数，低阶等因素","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxtiufu5yvs0","created":1571490374909,"text":"冒泡","note":"* 冒泡\n\t* 是原地排序算法\n    \t* 空间复杂度为O(1)\n    \t* 只需要常量级的临时空间(交换时使用)\n        \n    * 是稳定的排序算法\n    \t* 只有交换才会改变前后顺序\n        * 相等并不交换\n        \n        \n    * 时间复杂度\n    \t* 最好O(n)\n        \t* 原本有序\n            \n        * 最坏O(n^2)\n        \t* 原数据逆序 \n            \n        * 平均\n        \t* O(n^2)\n        \t\n\n* 冒泡排序中\n\t* 交换的次数等于逆序度的次数\n    \t* 最坏 n*(n-1)/2,最好 0\n        * 中间值 n*(n-1)/4\n        \n    * 比较操作肯定要比交换操作多\n    \t* 上限O(n^2)\n        * 所以平均时间复杂度是O(n^2)\n        \n    * 这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用\n        \n\n","layout":null},"children":[]},{"data":{"id":"bxtjh7rrqhc0","created":1571492159726,"text":"插入","note":"* 元素的移动总次数，也是等于逆序度\n\t* 分成一个有序区间和一个无序区间\n    \t* 每次从无序区间中取出一个元素\n        * 比较找到自己的位置插入\n        \t* 找到自己的位置后，后面的元素全部往后移动一位\n        * 重复N次\n            \n            \n\n* 插入排序\n\t* 是原地排序算法\n    \n\t* 是稳定排序\n    \n    * 时间复杂度\n    \t* 最好情况，已经有序O(n)\n        * 最坏,逆序 O(n^2)\n        \n        * 平均也是O(n^2)\n        \t* 在一个有序的数组中插入一个数字是O(n)\n            * 所以n个就是O(n^2)","layout":null},"children":[]},{"data":{"id":"bxtlba2eo140","created":1571497336757,"text":"选择","note":"* 选择排序\n\t* 也分一个有序区间，一个无序区间\n    \n    * 每次从无序区间中选最小的，放进有序区间的末尾\n    \t* 每次通过比较获得未排序的最小元素\n        * 实际操作：将该最小元素与有序与无序的交界处的元素互换位置\n        \n    * 是原地排序\n    \n    * 时间复杂度\n    \t* 最好, 最坏，平均：O(n^2)\n        * 因为即使原本有序，也是每次拿出一个最小的，然后重复N次\n    \n    * [不是]稳定排序\n    \t* 如5，8，5，2，9\n        * 经过一次交换，原本在前的元素5变成后面\n        \t","layout":null},"children":[]}]},{"data":{"id":"bxteaa7rw2o0","created":1571477510948,"text":"O(nlogn)：快排、归并","note":"* 都用到了分治思想\n\t* 即将问题拆分成小问题处理\n    \n* 递归\n\t* 递归也是类似的处理方案\n    \n* 因此，分治算法一般都使用递归来实现\n\t* 分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突\n    \n    * 归并、快排的区别\n    \t* 归并是自下而上的排序\n        \t* 先处理子问题，再合并\n            \n        * 快排是自上而下\n        \t* 先分区，再处理子问题\n            \n\n* 使用二分（分治）比冒泡这些快的本质\n\t* 结论上，可通过数学公式的推导\n    * 理解上，冒泡等算法\n    \t* 每一遍的循环，只选出了一个元素，剩余的元素没任何变化\n        * 而两种分治法的排序，每次子问题的处理，或者父问题的处理，是带有一定的有序性\n        \t* 减少下一步的操作","expandState":"expand","layout":null},"children":[{"data":{"id":"bxu1ivqj9k00","created":1571543070236,"text":"快排","note":"* 也是使用分治思想\n\n* 数组中选择随机一个元素，例如最后一个\n\t* 作为一个分区点pivot\n    * 遍历p到r之间的数据\n    \t* 将小于pivot的放到左边\n        * 将大于pivot的放到右边\n        * 将pivot放到中间。\n\n* 不断将两边的区间按照此方式处理\n\t* 处理完就是有序的了\n    \n* 将数组分为2个区间\n\t1. 可以使用两个临时数组，实现方便\n    \t* 但是这样就不是原地排序了\n        \n    2. 从左到右遍历数组\n    \t* 标记遇到的第一个比pivot大的元素\n        * 后面遇到比此元素小的，则交换位置\n        \t* 标记元素的角标+1\n            * 新的标记元素肯定也是比pivot元素大的\n        * 最后将标记元素与pviot元素交换\n        \n* 性能分析\n\t* 是原地排序\n    * [不稳定]的算法\n    \t* 因为有交换\n        * 不移动其他元素，直接交换的，都是不稳定的，如选择排序\n        \n    * 时间复杂度\n    \t* 平均O(nlogn)\n        * 极端（原本有序），退化为O(n^2)\n        \t* 原本有序，每次取最后一个元素，则两区间每次都相差很大\n            * 每次都只能处理一个元素\n        * 推理。待树的那一节学习\n        \n    \t\n        \n","expandState":"expand","layout":null},"children":[{"data":{"id":"bxucelqvkao0","created":1571573767241,"text":"应用","note":"* 求无序数组的第K大元素\n\t* 使用快排，定一个中间点pivot\n    \t* 经过一轮比较，k比pivot位置大，则处理右边区间\n        \t* 否则处理左边区间\n        * 直到pivot等于k\n        \t* 则pivot的值就是第k大元素\n            \n    * 此算法复杂度是O(n)\n    \t* 第一次遍历n次\n        * 第二次遍历n/2次...\n        \n        * 等比数列求和\n        \t* 2n-1\n            * 即O(n)\n            \n            \n            \n* 注：等比数列求和公式\n\t* (a1-anq)/(1-q)\n    \t* a1为首项,an为第n项,d为公差,q 为等比","layout":null},"children":[]},{"data":{"id":"bxvwqh3awjk0","created":1571732679634,"text":"优化","note":"* 快排中，分区点的选择很重要\n\t* 如果每次选择的都是区间的末尾\n    \t* 则时间复杂度退化为O(n^2)\n        \n    * 分区点选择优化\n    * 三数取中法\n        * 从区间的首、尾、中间，分别取出一个数，比较大小，取中值\n        \n        * 数据量大可以是五数取中或者十数取中\n        \n    * 随机法\n    \t* 随机的概率较为平均","layout":null},"children":[]}]},{"data":{"id":"bxu1kf4ssrk0","created":1571543190822,"text":"归并","note":"* 将数组不断分为两半，直至只有一个元素\n\t* 将两半的元素合并，此时这两半数据，内部本身是有序的\n    \t* 对比俩个数组的第一个元素，谁小就取谁，放进临时数组\n        * 处理完后临时数据数据遍历赋值回原数组\n        \n        \n        * 哨兵方式：增加2个子数组的临时数组，大小比子数组大1，用于装哨兵\n        \t* 哨兵放Integer最大值\n            * 先在临时子数组暂存原来子数组数据\n            * 原数组接收对比后的数据\n            * 哨兵的存在可以不用判断是否已经到达边界\n       * 哨兵的代码量会略简洁\n       \n* 性能分析\n\t1. 是稳定的的算法\n    \t* 前提是合并的代码，左边<=右边时，选左边（等于号不能少）\n        \n    2. 时间复杂度\n    \t* 时间复杂度 O(nlogn)\n        * 最好情况、最坏情况，还是平均情况都一样，与数据的有序度无关。非常稳定\n     \n    3. 空间复杂度\n    \t* 需要临时数组\n        * O(n) ,[不是]原地排序","expandState":"expand","layout":null},"children":[{"data":{"id":"bxu8on584aw0","created":1571563269488,"text":"时间复杂度分析","note":"* T(n) = 2* T(n/2) + n； \n\t* n>1\n    \t* 后面的n表示合并操作\n        \n\t* T(1) = C；   \n    \t* n=1时，只需要常量级的执行时间，所以表示为C。\n        \n    * 一直推导\n    \t* 2^k * T(n/2^k) + k * n\n        \n        * 当T(n/2^k)=T(1)时\n        \t* n/2^k=1\n            * k=log2n \n        \n        * Cn + n* log2n\n        \t* 即O(nlogn)\n            \n            \n* 归并排序比较稳定 \n\t* 栈的深度是logn 非常小 所以不会堆栈溢出","layout":null},"children":[]},{"data":{"id":"bxud2o0czmw0","created":1571575652908,"text":"类似算法应用","note":"* 现在你有10个接口访问日志文件，每个日志文件大小约300MB\n\t* 每个文件里的日志都是按照时间戳从小到大排序的\n    \t* 你希望将这10个较小的日志文件，合并为1个日志文件\n        * 合并之后的日志仍然按照时间戳从小到大排列。\n        * 如果处理上述排序任务的机器内存只有1GB\n        \n        \n\n\n* 每次各个接口获取一定数据\n    * 一条或多条\n    * 比较选出最小的，写入新文件\n    \n    * 优化\n    \t* 使用堆，每次取出堆顶即可\n        \n* 功能有点像归并排序中的合并函数\n","layout":null},"children":[]}]}]},{"data":{"id":"bxteawan94o0","created":1571477559011,"text":"O(n) : 桶、基数、计数","note":"* 因为时间复杂度是O(n)\n\t* 因此也叫线性排序\n    \n    * 为什么能做到线性排序\n    \t* 不是基于比较的排序算法，不涉及元素之间的比较\n        \t* 基本只需要遍历一次就完成\n     \n    * 对排序的顺序有要求\n    \n    \n    \n* 空间复杂度\n\t* 这几种都不是原地排序\n    \n* 时间复杂度\n\t* 桶排序\n    \t* O(n)\n        * 桶的数量接近于数据数量时是O(n)\n        \t* 此处是指实际使用的桶的数量，实际使用的桶越多，桶内的元素越少，需要快排的数据越少\n            \n            * 注意代码中，桶的数量是数据范围决定的，不是数据量\n        * 即一个桶内元素不多，就是O(n)\n        \n    * 计数排序\n    \t* O(n+k)\n        * k是数据范围\n        \t* 因为需要遍历计数桶，将前面的元素数量加到当前的总数\n            * 范围大会导致时间复杂度变大\n        \n    * 基数排序\n    \t* O(dn)\n        * d是维度\n        \t* 维度不大的话近似于O(n)","expandState":"expand","layout":null},"children":[{"data":{"id":"bxudeyc8xy80","created":1571576615765,"text":"桶排序","expandState":"expand","note":"* 将数据分到几个有序的桶中\n\t* 即桶与桶之间是有序的\n    * 对每个桶里的元素进行排序\n    \t* 使用快排（要稳定dehua用归并）\n        * 最后按顺序取出，就是有序的了\n        \n* 时间复杂度\n\t* n个元素，分成m个桶\n    \t* 则每个桶的元素是k=n/m\n        * 每个桶的时间复杂度是O(klogk)\n        \n    * 总复杂度 O(m * klogk)\n    \t* O(n* log n/m)\n        * 当m接近于n时，就是O(n)\n        \n* 使用场景\n\t* 要排序的数据需要很容易就能划分成m个桶\n    * 桶与桶之间有着天然的大小顺序\n    \t* 不需要再次排序\n    * 数据需要均匀\n    \t* 极端情况下，退化为O(nlogn)\n    \n    * 适合用在外部排序中\n    \t* 即数据存储在外部磁盘中，无法将数据全部加载到有限的内存中\n        \n        \n* 例\n\t* 如10G订单信息按照金额排序\n    \t* 如果每个区间数量特别大，则可以对此区间再次细分\n        * 直到单个桶的数据可以全部加载到内存，进行快排\n        \n        * 注意要求稳定的算法的话，不要快排，用归并\n        \n        \n        \n        ","layout":null},"children":[{"data":{"id":"bxvupz3cs600","created":1571726998235,"text":"实现","note":"* 注\n\t* 数组有多少元素，需要统计\n    \t* length是数组长度，不是真实数据\n        * （没填数据）默认就是0\n        \n    * 二维数组中 arr[i][j]\n    \t* arr[i] 表示第几个一维数组数组\n    \t* 一维数组长度可以不一致\n        \n        \n* 实现\n\t* 桶的大小作为参数传入\n    \t* 使用桶的大小决定桶的数量\n    \t* 传入的数据范围未知\n        * 桶的数量越接近数据范围，时间复杂度越接近O(n)\n        \n    * 桶的有序性\n    \t* 遍历数组\n        * 当前元素/桶大小作为桶角标\n        \n    * 使用二维数组存储所有桶\n    \t* 每个一维数组支持扩容\n        \n    * 使用一个数组记录每个桶大小","layout":null},"children":[]}]},{"data":{"id":"bxufav466oo0","created":1571581937509,"text":"计数排序","note":"* 计数排序其实是桶排序的一种特殊情况\n\t* 要排序的n个数据，所处的范围并不大的时候，比如最大值是k\n    \t* 就分成k+1个桶(元素是0或正整数)\n        * 桶内元素相等\n        \n        * 例如50w的考生，分数范围是0~900，则分为901个桶\n        \n        \n* 实现\n\t* 创建一个数组C，记录每种元素的个数\n    \t* 再改造为：记录小于等于当前元素的总个数\n        \n    * 创建临时数组R，用于放有序少数据\n    \n    * 从后往前遍历原数组A\n    \t* 根据当前元素的大小，匹配到计数数组C的下标\n        * 则该数组对应的值就是在有序数组中的位置\n        \t* 有序角标 =  value-1\n     \n    * 将临时数组赋值回原数组\n    \n    \n* 使用场景\n\t* 计数排序只能用在数据范围不大的场景中\n    \t* 如果数据范围k比要排序的数据n大很多，就不适合用计数排序了。\n        \n        * 而且，计数排序只能给非负整数排序\n        \t* 如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数\n            * 负数：同时+n\n            * 小数：同时乘以100等","layout":null},"children":[]},{"data":{"id":"bxuzsbx8oag0","created":1571639728484,"text":"基数排序","note":"* 对于手机号排序这种\n\t* 范围太大，不适宜桶排序，计数排序\n\n\n* 思路\n\t* 如果数据a在高位比数据b大\n\t\t* 那么a就肯定比b大\n    \n* 基数排序\n\t* 通过稳定排序的思想\n    \t* 先比较低位，再比较高位\n        \n    * 则比较完的就是有序的了\n    \n    \n    * 每一位的排序可以是桶排序，或者计数排序\n\n\n* 时间复杂度\n\t* k位的排序，每一位使用O(n)的桶排序\n    \t* O(k * n)\n        * k不大时，近似O(n)\n        \n        \n* 场景\n\t* 如果数据不等长，如英文字典\n    \t* 以前面的为准的，则后面补0\n        * 0 SACII码 比字母小，不影响排序\n    \n    * 应用场景\n    \t* 排序的数据可以分割出独立的“位”来比较\n        * 位之间有递进的关系\n        \t* 如果a数据的高位比b数据大，那剩下的低位就不用比较了\n            \n        * 每一位的[数据范围]不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到O(n)了。","layout":null},"children":[]}]},{"data":{"id":"bxvwhwzv30o0","created":1571732008977,"text":"实现通用的、高性能的排序函数","note":"* O(n) 的排序算法\n\t* 适用场景比较特殊\n    \t* 通用的排序函数中不能使用\n        \n* O(n2) 的算法\n\t* 适合数据规模小的\n    \n* O(nlogn) 适合数据规模大的\n\n\n* 因此\n\t* 一般都使用O(nlogn)时间复杂度的算法\n    \t* Java语言采用堆排序\n        * C语言使用快速排序\n        \n    * 归并排序用的较少，因为不是原地排序\n    \n    \n* C语言中的qsort()函数\n\n\t* 优先使用[归并排序]来排序输入数据\n    \t* 小数据量的排序，占用空间不大，且时间复杂度稳定\n        \n    * 超过100M时\n    \t* 会改为用[快速排序]算法来排序\n        * 使用三数取中法定位分区点\n        \n    * 递归的堆栈溢出优化\n    \t* 通过自己实现一个堆上的栈，手动模拟递归来解决\n        \n    * 插入排序\n    \t* 在快速排序的过程中，当要排序的区间中，元素的个数小于等于4时，qsort()就退化为插入排序\n        * 不再继续用递归来做快速排序\n    \n    * 使用哨兵减少判断次数\n     ","layout":null},"children":[{"data":{"id":"bxvxe81dxaw0","created":1571734540667,"text":"关于算法实际运行时间","note":"* 在小规模数据面前\n\t* O(n^2)时间复杂度的算法并不一定比O(nlogn)的算法执行时间长。\n    \n    * 因为时间复杂度表示的是执行时间与数据规模的增长趋势、\n    \t* 在大O复杂度表示法中，我们会省略低阶、系数和常数\n        * O(nlogn)在没有省略低阶、系数、常数之前可能是O(knlogn + c)，而且k和c有可能还是一个比较大的数。\n        \n        * 因此n^2的值可能实际上比knlogn+c还要小","layout":null},"children":[]}]}]},{"data":{"id":"bxw490supew0","created":1571753880862,"text":"二分查找","note":"* 二分查找\n\t* 针对的是一个有序的数据集合\n    \n\t* 查找思想有点类似分治思想。\n    \t* 每次都通过跟区间的中间元素对比\n        * 将待查找的区间缩小为之前的一半\n        \t* 直到找到要查找的元素，或者区间被缩小为0。\n            \n            \n* 算法时间复杂度\n\t* O(logn) \n    \t* 对数时间复杂度\n        \n    * 是一种极其高效的时间复杂度\n    \t* 有时候甚至比时间复杂度是常量级O(1)的算法还要高效。\n        \t* 数据规模很大时，对数时间复杂度还是很小\n            * 对于一些忽略了常数、系数和低阶的O(1)算法，其实际可能表示的是一个非常大的值","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxw4g05ckug0","created":1571754427990,"text":"局限性","note":"* 二分查找依赖的是顺序表结构，即数组\n\t* 链表这种是不可以的\n    \t* 因为数组按照下标随机访问数据的时间复杂度是O(1)\n        \n        * 链表随机访问的时间复杂度是O(n)，查找效率就会变得很低\n        \n\n* 数据需要是有序数据\n\t* 数据表必须是有序的，如果无序需要先排序\n    \t* 排序一般最低O(nlogn)\n        \n        * 不是频繁插入删除的话，排序的时间成本会被均摊，相对较少\n        \n        * 频繁插入删除，则成本变高，则不再适用\n        \t* 每次插入删除时确保有序，或者二分前排序\n            \n            \n            \n* 数据量太少\n\t* 太少直接遍历即可\n    \t* 数据比较耗时的话，还是用二分，减少比较次数\n        \n        \n* 数据量太大\n\t* 二分查找的底层需要依赖数组这种数据结构\n    \t* 数组需要空间连续\n        * 数据太多放不下","layout":null},"children":[]},{"data":{"id":"bxwpn9no9mw0","created":1571814240546,"text":"代码注意的点","note":"* 使用while循环的话\n\t* 条件是low<=high\n    \t* 不是low< high\n        * 因为每次比较的只是中间值，可能low（即此时的high）就是想要的值\n        \n        \n        \n    * mid 取值\n    \t* low + ((high-low)>>1)\n        \n        * 不能加起来再除以2，因为可能超过int的大小\n        * 使用位运算是为了更快\n    \n    * mid的判断\n    \t* 每次判断mid值是否命中\n        \t* 命中则返回\n            \n        * 不命中，对应新区间是\n        \t* low = mid +1；\n            * 或 high = mid -1\n            \n            * 因为mid本身已经判断，所以直接加1.另外假如直接取mid值，在mid等于最大或最小值时，会导致死循环\n              \n                     ","layout":null},"children":[]},{"data":{"id":"bxwrmnqt0fc0","created":1571819835066,"text":"应用","note":"* 假设我们有1000万个整数数据，每个数据占8个字节\n\t* 如何设计数据结构和算法，快速判断某个整数是否出现在这1000万数据中？\n    \n    * 1000万 * 8byte = 80M\n    \t* 全部数据放进内存，排序\n        * 二分查找\n        \n        \n    * 散列表和二叉树理论上可以解决\n    \t* 但是他们需要更多内存，本题目中用不上\n        \n        \n        \n        \n* 根据IP查找地区\n\t* IP的一个范围对应一个地区\n    \t* 根据ip找到对应的一个的地区\n    \n    * 先将ip数据转化为long类型\n    \t* ip数据不经常变化\n        * 将数据排序\n    \n    * 问题转化为：\n    \t* 查找最后一个小于等于给定ip值的\n        \t* 找到一个起始区间小于等于当前值的\n            \n        * 或 第一个大于等于给定ip值的\n    * 找到后再判断区间内是否包含当前值","layout":null},"children":[]},{"data":{"id":"bxwsg4oh5o80","created":1571822144491,"text":"二分查找变体","note":"1. 数据中存在重复数据\n\t* 需要找第一个（或最后一个）数据","layout":null},"children":[]},{"data":{"id":"bxxj9izdw3k0","created":1571897797044,"text":"跳表","note":"* 是一种各方面性能都比较优秀的动态数据结构\n\t* 可以支持快速的插入、删除、查找操作\n    \t* 写起来也不复杂，甚至可以替代红黑树\n        \n        \n        \n* 实际应用\n\t* redis 的 Sorted Set使用跳表实现。\n    \n    * sorted set支持以下操作\n    \t* 插入、删除、查找、按照区间查找数据、以及迭代输出有序序列\n        \n        \t* 插入、删除、查找以及迭代输出有序序列这几个操作红黑树也可以完成\n            * 时间复杂度跟跳表是一样的\n        \n        * 对于按照区间查找数据这个操作\n        \t* 跳表可以做到O(logn)的时间复杂度\n            * 定位区间的起点，然后在原始链表中顺序往后遍历就可以了。\n    \n    * 准确的说：双hashmap构成的字典和跳跃表实现\n    \n    \n* 跳表也不能完全替代红黑树\n\t* 因为红黑树比跳表的出现要早\n    * 有现成的可以直接拿来用\n    \n    * 跳表要自己实现","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxxjsstfqag0","created":1571899307371,"text":"跳表原理","note":"* 对原始的有序单链表\n\t* 每隔两个节点（或者N个节点），提取一个节点到上一级\n    \t* 这一级的链表叫索引或索引层\n        \n    * 索引链表每个节点有一个down指针\n    \t* 指向下一级的链表对应的节点\n        \n    * 靠近原始链表的一层索引\n    \t* 叫 第一级索引\n        * 在第一级索引基础上可以继续建立第二、第三级索引\n        \n        * 直至最顶部索引只有2个节点\n        \t* 或3个\n        \n        \n\n\t","layout":null},"children":[]},{"data":{"id":"bxxkpq7yc0o0","created":1571901887736,"text":"时间、空间复杂度","note":"* 时间复杂度\n\t* 假设是每两个节点抽取一个节点\n    \n    * 最多有k级索引\n    \t* 第一层是 n/2 个元素\n        * 第二层是 n/4 个元素\n        * 第k层是 n/2^k 个元素\n        \n        * 最顶层是2个元素\n        \t* n/2^k = 2\n            * k = log2n -1(2是底)\n        \n    * 或者直观点：k为log2n（2为底）\n    \t* 每层只有下一层的一半\n    \n    * 遍历的时候，从一个节点往前，遇到前面的节点比要找的节点大\n    \t* 则往下一层\n        * 每一层，包括遇到比自己大的节点\n        * 共3个节点\n    \n    * 所以时间复杂度是\n    \t* O(3* (log2n -1))\n        * 即：O(logn)\n        \n        \n* 空间复杂度\n\t* 一级索引节点数 ：n/2\n    \t* 二级：n/4\n    \t* ...\n    \t* 即：O(n)\n    \n    * 也可以隔3个、5个节点抽取一个节点\n    \t* 空间复杂度虽然还是O(n)\n        * 但实际减少很多\n    \n    * 实际开发中\n    \t* 原始链表中存储的有可能是很大的对象\n        * 而索引结点只需要存储关键值和几个指针\n        * 所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了","layout":null},"children":[]},{"data":{"id":"bxxkwfq5klk0","created":1571902413441,"text":"动态插入与删除","note":"* 插入删除的时间复杂度\n\t* O(logn)\n    * 通过O(logn)的查找，并通过O(1)的插入或删除即可\n    \t* 对于删除，如果被删除的节点在索引中也出现，则需要一并删除\n        \n        * 通常是遍历的时候已经知道，因为上层出现的下层一定有\n        \t* 此时先将该节点的前驱节点保存，以便后面的删除\n            \n\n* 插入删除的时间复杂度不高\n\t* 但是索引需要动态更新\n    \t* 否则极端情况下，跳表还会退化成单链表。\n        \n        \n    * 通过随机函数来维护“平衡性”。\n    \t* 当往跳表中插入数据的时候\n        * 可以选择同时将这个数据插入到部分索引层中。\n        \n        * 比如随机函数生成了值K\n        * 那我们就将这个结点添加到[第一级到第K级]这K级索引中。","layout":null},"children":[]},{"data":{"id":"bxy84wr5kpk0","created":1571967962948,"text":"实现","note":"* 随机函数\n\t* 如果跳表是每两个节点抽一个索引出来\n    \t* 则下层有50%的概率会被抽到\n        * 每上升一层都是50%\n        \n    * 因此使用随机函数从0~1中获取\n    \t* 每次超过0.5就加一层，直至断了退出while循环\n        \n  \n* 只有插入使用了随机函数\n\t* 删除没有\n    * 插入可视为已经均匀的插入索引\n    \t* 删除的元素也是随机的，随机的删除一个均匀的链表节点，结果理论上还是均匀的\n        \n        \n","layout":null},"children":[]}]}]},{"data":{"id":"bxyeqd87su80","created":1571986571115,"text":"散列表","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxyequyqbsw0","created":1571986609724,"text":"散列思想","note":"* 散列表用的是数组支持按照下标随机访问数据的特性\n\t* 所以散列表其实就是数组的一种扩展，由数组演化而来。\n    \t* 可以说，如果没有数组，就没有散列表。\n        \n    \n* 散列思想\n\t* key（关键字）与数组下标形成一一映射\n    \t* 通过下标对数组进行随机访问，时间复杂度O(1)\n    \n\n\n    ","layout":null},"children":[]},{"data":{"id":"bxz5ezqda740","created":1572061849700,"text":"散列函数","note":"* 散列函数（Hash函数）\n\t* 将key转化为数组下标的函数\n    \n    * 散列值（Hash值）\n    \t* 通过散列函数计算得到的值\n        \n    * 散列函数的设计要求\n    \t1. 散列函数计算得到的散列值是一个非负整数；\n        \n        2. 如果key1 = key2，那hash(key1) == hash(key2)；\n        \n        3. 如果key1 ≠ key2，那hash(key1) ≠ hash(key2)。\n        \n\n* 散列冲突\n\t* 散列函数设计要求的第三点\n    \t* 比较难实现\n        \n        * 即便像业界著名的MD5、SHA、CRC等哈希算法\n        \t* 也无法完全避免这种散列冲突。\n        * 而且，因为数组的存储空间有限，也会加大散列冲突的概率。","layout":null},"children":[]},{"data":{"id":"bxyf7zopiu80","created":1571987952192,"text":"散列冲突","note":"* 装载因子（load factor）\n\t* 用于保证散列表中有一定比例的空闲槽位，否则hash冲突的概率变得很大，查找的时间复杂度甚至退化为O(n)\n    \t* 超过一定值就扩容\n        * 散列表的装载因子=填入表中的元素个数/散列表的长度","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxyfc90wo000","created":1571988285978,"text":"开放寻址法","note":"1. 开放寻址法\n\t* 如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。\n    \n* 线性探测\n\t* 当存储位置已经被占用了\n    \t* 我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。\n        * ThreadLocalMap使用的就是这种\n        \n    * 由于查找数据时，找不到继续下一个\n    \t* 所以删除一般不能直接置空。可以标记deleted或者移动其他元素进行填充等\n        \n* 其他开放寻址法\n\t* 二次探测\n    \t* 线性探测是每次加一步\n        \t* hash(key)+0\n            * hash(key)+1\n            * hash(key)+2……\n        \n        * 二次探测是原来的二次方\n        \t* hash(key)+0\n            * hash(key)+1^2\n            * hash(key)+2^2……\n            \n    * 双重散列\n    \t* 不仅仅使用一个散列函数，而是使用一组散列函数\n        * hash1(key)，hash2(key)，hash3(key)……\n        * 先用第一个散列函数，如果计算得到的存储位置已经被占用\n        \t* 再用第二个散列函数，依次类推，直到找到空闲的存储位置。\n \n","layout":null},"children":[{"data":{"id":"bxz5kcjhmgo0","created":1572062269403,"text":"优点","note":"* 优点：开放寻址法不像链表法需要拉很多链表。\n\t* 散列表中的数据都存储在数组中\n    \t* 可以有效地利用CPU缓存加快查询速度。\n        \n    * 这种方法实现的散列表，序列化起来比较简单。\n    \t* 链表法包含指针，序列化起来就没那么容易。\n        \n \n* 缺点\n\t* hash冲突的代价更高\n    \t* 如删除的时候需要标记或移动其他元素\n        * 查找的时候找不到要一直往后找\n    * 因为冲突代价高\n    \t* 因此[装载因子的上限]不能太大\n        * 这也导致这种方法比链表法更浪费内存空间。\n        \n        \n* 总结\n\t* 当数据量比较小、装载因子小的时候，适合采用开放寻址法。\n    * 实际应用： ThreadLocalMap","layout":null},"children":[]}]},{"data":{"id":"bxyfmgknao00","created":1571989086050,"text":"链表法","note":"* 链表法\n\t* 散列值相同的元素\n    \t* 都放到相同的槽位对应的链表中\n        * 一个数组位置就是一个槽，槽中放链表头元素\n    \n* 查找元素时间复杂度\n\t* O(k)\n    \t* k是链表长度\n        * 均匀的话k = n/m（m是槽的个数）","layout":null},"children":[{"data":{"id":"bxz5opsgar40","created":1572062611699,"text":"优点","note":"* 优点\n* 链表法对内存的利用率比开放寻址法要高。\n\t* 因为链表结点可以在需要的时候再创建\n    \t* 并不需要像开放寻址法那样事先申请好。\n        \n        \n* 对大装载因子的容忍度更高。\n\t* 只要散列函数的值随机均匀，即便装载因子变成10，也就是链表的长度变长了而已\n    \t* 虽然查找效率有所下降，但是比起顺序查找还是快很多。\n        \n \n \n \n* 缺点\n* 链表由于需要存储指针\n\t* 当存储的对象比较小时，指针相对比较耗内存\n    \t* 甚至可能使内存翻倍\n    \n    * 存储大对象时，则可以忽略\n    \n \n* 链表的节点是零散的分布在内存中\n\t* 不是连续的，所以对CPU缓存是不友好的\n    \t* 对于执行效率也有一定的影响\n\n\n* 优化\n\t* 链表可以改进为跳表、红黑树\n    * 进入同一个桶后，查找时间复杂度为O(logn)\n    \n* 应用\n\t* 比较适合存储大对象，大数据量的散列表\n    * 更加灵活，支持更多的优化策略\n    \t* 比如用红黑树代替链表。","layout":null},"children":[]}]}]},{"data":{"id":"bxz4tvi2po80","created":1572060194844,"text":"打造工业级散列表","note":"* 散列函数的设计\n\t* 散列函数的设计不能太复杂\n    \t* 过于复杂的散列函数，会消耗很多计算时间，也就间接的影响到散列表的性能\n    \n    * 散列函数生成的值要尽可能随机并且均匀分布\n    \t* 减少散列冲突，或者即使冲突，每个操中也分配均匀\n        \n        \n* 数据分析法\n\t* 分析数据的特点，进而设计散列函数\n    \t* 但动态(频繁插入和删除)的数据不一定能预知其特点 \n    \n    \n* 避免低效扩容\n\t* 插入的时间复杂度为O(1),装在因子达到阈值后触发扩容\n    \t* 使用摊还分析法，时间复杂度还是O(1)\n        \n        * 但是如果我们的业务代码直接服务于用户\n        \t* 某一个插入操作特别慢，用户体验就不好\n   \n    * 解决一次性扩容耗时过多\n    \t* 当装载于因子到达阈值\n        \n        * 只申请空间，不迁移全部数据\n        \n        * 当有数据插入时，新数据插入新的散列表，并且从老的散列表中拿出一个数据放入到新散列表。\n        \t* 经过多次插入操作后，老散列表中的数据就一点点全部搬移到新散列表中\n            * 查找的时候先从新的散列表查，再去旧的\n            \n        * 当然插入的时候也看看旧的有没数据，有的话需要做删除旧数据处理","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxzade9sizk0","created":1572075830170,"text":"总结","note":"* 工业级的散列表\n\t* 支持快速的查询、插入、删除操作；\n    * 内存占用合理，不能浪费过多的内存空间；\n    \n    * 性能稳定\n    \t* 极端情况下，散列表的性能也不会退化到无法接受的情况。\n        \n        \n* 实现\n\t* 设计一个合适的散列函数；\n    * 定义装载因子阈值，并且设计动态扩容策略；\n    * 选择合适的散列冲突解决方法。","layout":null},"children":[]}]},{"data":{"id":"bxzbg6yq26g0","created":1572078870466,"text":"散列表与链表的组合使用","note":"* 散列表这种数据结构\n\t* 虽然支持非常高效的数据插入、删除、查找操作\n\t* 但是散列表中的数据都是通过散列函数打乱之后无规律存储的。\n    \t* 它无法支持按照某种顺序快速地遍历数据。\n        \n    * 没有链表的话，想要按顺序输出数据\n    \t* 则需要全部排序一次\n        * 频繁插入删除的数据则时间复杂度非常大\n        \n\n* 因此通过链表的组合使用来实现排序\n\t* 散列表负责快速定位\n    * 链表实现排序，并可以优化为跳表等","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxzbnho6avs0","created":1572079442322,"text":"LRU缓存淘汰算法","note":"* 基于链表的缓存的操作\n\t* 添加一个数据\n    * 删除一个数据\n    * 查找一个数据\n    \n    * 都需要先在链表中查找这个元素\n    \t* 即使是添加，也要看看是否已经存在于链表中\n        \n    * 单纯的链表查找时间复杂度是O(n)\n    \n\n* 结合散列表\n\t* 可以将复杂度降低到O(1)\n    \n    * 如LinkedHashMap的实现（linked说的说就是使用双向链表）\n\t\t* 即 使用双向链表结构存储节点的排序\n        * 另外节点也放在散列表每个桶的单链表中\n        \n    * 查找数据\n    \t* 先通过散列表定位元素：O(1)\n        * 取出数据后，再将访问的节点移到双向链表的尾部\n        \n    * 删除元素\n    \t* 同查找数据的步骤：O(1)\n        * 双向链表可以直接获取前驱节点\n        \t* 因此删除也是O(1)\n            \n    * 添加元素\n    \t* 先通过散列表看是否已经存在此数据\n        * 已经存在的\n        \t* 覆盖且移动到双向链表末尾\n        * 不存在的\n        \t* 看链表（缓存）是否满了，已满则删除链表头元素，当前元素放进链表尾\n            * 未满则放链表尾部元素","layout":null},"children":[]},{"data":{"id":"bxzc289vljk0","created":1572080597329,"text":"Redis有序集合","note":"* 在有序集合中，每个成员对象有两个重要的属性\n\t* key（键值）和score（分值）\n    * 可以通过score来查找数据，还会通过key来查找数据。\n    \n \n* 按照分值的排序，以及分值的按区间查找\n\t* 可以根据跳表来实现\n    \n*  根据键值的查找删除\n\t* 使用跳表就需要O(n)了\n    \t* 因为跳表是根据分值来排的\n    * 因此根据键值构造一个散列表\n    \n    * 此处猜测是每一个key对应一个跳表","layout":null},"children":[]}]},{"data":{"id":"bxzhknwl8cw0","created":1572096147458,"text":"哈希算法","note":"* 注\n\t* hash就是哈希，也叫散列\n\n\n\n* 哈希算法\n\t* 将[任意长度]的二进制值串 映射为固定长度的二进制值串\n    \t* 这个映射的规则就是哈希算法\n        \n    * 哈希值\n    \t* 通过原始数据映射之后得到的二进制值串\n        \n    * 著名Hash算法\n    \t* 比如MD5、SHA等\n        \n        \n        \n        \n        \n* 设计一个优秀的哈希算法\n\t1. 从哈希值不能反向推导出原始数据\n    \t* 所以哈希算法也叫单向哈希算法\n        * 暴力破解的意思是尝试各种情况，通过毫无规律的穷举实现。也不算是反向推导\n        \n    2.  对输入数据非常敏感\n    \t* 哪怕原始数据只修改了一个Bit，最后得到的哈希值也大不相同\n        \n    3.  散列冲突的概率要很小\n    \t* 对于不同的原始数据，哈希值相同的概率非常小；\n     \n    4. 哈希算法的执行效率要尽量高效\n    \t* 针对较长的文本，也能快速地计算出哈希值。","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxzi0tr07qo0","created":1572097414008,"text":"安全加密","note":"* 常用的加密的哈希算法（或数字签名）\n\t* MD5\n    \t* MD5 Message-Digest Algorithm\n        * MD5消息摘要算法  \n    * SHA\n    \t* Secure Hash Algorithm\n        * 安全散列算\n        \n\n* 哈希算法四点要求中，用于加密时，有两点格外重要\n\t1. 很难根据哈希值反向推导出原始数据\n    \t* 加密的目的就是防止原始数据泄露\n    \n    2. 散列冲突的概率要很小\n    \t* 理论上是没办法做到完全不冲突的\n        \t* 因为哈希算法产生的哈希值的长度是固定且有限的\n        \n        * 如MD5，哈希值是固定的128位二进制串，能表示的数据是有限的\n        \t* 最多能表示2^128个数据\n            \n        * 如果我们对2^128+1个数据求哈希值\n        \t* 就必然会存在哈希值相同的情况。\n            \n            \n    * 不过，虽然希算法存在散列冲突的情况\n    \t* 但是因为哈希值的范围很大，冲突的概率极低，因此较难破解\n        \n        * 找到真实的值，或者冲突的值就能破解（作为密码的话），要获取真实信息则与此无关\n        \n* 此外，没有绝对安全的加密\n\t* 只是越复杂、越难破解的加密算法，需要的计算时间也越长。","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxzixnwmpoo0","created":1572099987305,"text":"加密补充","note":"* SHA 目前比MD5更安全\n\t* MD5已经号称被破解了\n    \n\n* 字典攻击\n\t*  对于一些过于简单的密码\n    \t* 维护一个常用密码的字典表，并记录其hash值\n        * 跟脱库密文（被别人拿到了）对比，即可拿到密码\n        \n    * 解决\n    \t* 加盐salt\n        * 加系统定义的一组固定的随机字符串，与用户的密码组合在一起，增加密码的复杂度\n        \n        * 拿组合之后的字符串来做哈希算法加密，将它存储到数据库中\n        \t* 进一步增加破解的难度。\n            \n         * 加盐的方式有很多种，可以是在头部加，可以在尾部加，还可在内容中间加等\n            \n            \n            \n* 越是复杂哈希算法越难破解，但同样计算时间也就越长。\n\t* 所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。\n        ","layout":null},"children":[]}]},{"data":{"id":"bxzihczlpq80","created":1572098709713,"text":"唯一标识","note":"* 如在海量的图库中，搜索一张图是否存在\n\t* 整张图的二进制码对比，但是文件太大\n    \t* 转化成二进制十分耗时\n        \n    * 每一个图片取一个唯一标识\n    \t* 或叫信息摘要\n        \n        * 从图片的二进制码串开头取100个字节，从中间取100个字节，从最后再取100个字节\n        \t* 然后将这300个字节放到一块，通过哈希算法（比如MD5），得到一个哈希字符串\n            * 用它作为图片的唯一标识。\n            \n \n* 结合散列表使用\n\t* 将唯一标识与图片地址存进散列表\n    \t* 先对比唯一标识\n        \n    * 唯一标志相同的，散列表中取出图片路径\n    \t* 取出图片做全量对比\n        ","layout":null},"children":[]},{"data":{"id":"bxzimp3qe0w0","created":1572099127905,"text":"数据校验","note":"* 校验获取的文件是否安全、正确、完整\n\t* 如BT文件下载，会将文件切割成多个文件块\n    \t* 当下载完各个文件块后\n        * 对其做哈希运算，获取哈希值\n        \n        * 与种子文件中的保存的hash值比对\n        \t* 不一致则说明被篡改过","layout":null},"children":[]},{"data":{"id":"bxzivgczso00","created":1572099814152,"text":"散列函数","note":"* 散列函数是设计一个散列表的关键。\n\t* 直接决定了散列冲突的概率和散列表的性能。\n    \n* 散列函数中\n\t* 对散列冲突的要求较低\n    \t* 即便出现个别散列冲突，只要不是过于严重\n        \t* 都可以通过开放寻址法或者链表法解决。\n            \n            \n    * 散列值是否能反向解密也并不关心\n    \n    * 更加关注散列后的值是否能平均分布\n    \t* 以及算法执行效率","layout":null},"children":[]},{"data":{"id":"bxzjta9taw00","created":1572102465280,"text":"分布式相关应用","expandState":"expand","layout":null},"children":[{"data":{"id":"bxzjqwusk5k0","created":1572102279345,"text":"负载均衡","note":"* 如实现一个会话粘滞（session sticky）的负载均衡算法\n\t* 即一个会话的所有请求都打到一台机器上\n    \t* 建立映射表，会浪费空间（客户端多的话）\n        * 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大；\n        \n    * 使用hash算法\n    \t* 对客户端IP地址或者会话ID计算哈希值\n        * 将取得的哈希值与服务器列表的大小进行取模运算，获得服务器编号。","layout":null},"children":[]},{"data":{"id":"bxzjt4l4z540","created":1572102452904,"text":"数据分片","note":"* 统计搜索关键词出现的次数\n\t* 假如有有1T的日志文件\n    \t* 没办法放到一台机器的内存中\n        * 即使放进去也很慢\n    \n    * 先对数据进行分片\n    \t* 多台机器同时处理\n        \n        * 读到搜索关键字时，计算hash值，再跟机器数n取模\n        \t* 获取对应处理的机器数\n            \n        * 最后合并数据\n        \t* 此处没有单独存自己的关键字跟次数\n            * 猜测是不用存储太多相同的key。且合并时不需做加法\n            \n     * 这里的处理过程也是MapReduce的基本设计思想。\n     \n     \n* 判断图片是否在图库中\n\t* 图片很多，如一亿张时\n    \t* 单台机器的内存装不下此散列表\n    * 同样是数据分片处理\n    \t* 每台机器只维护某一部分图片对应的散列表。\n        * 计算hash值，对n取模等\n        \n    * 断一个图片是否在图库中的时候\n    \t* 同样方法找到对应的机器的散列表去查找","layout":null},"children":[]},{"data":{"id":"bxzk8qjjib40","created":1572103676159,"text":"分布式存储","note":"* 如分布式缓存\n\t* 海量的数据需要缓存，一个缓存机器肯定是不够的。\n    \t* 于是，我们就需要将数据分布在多台机器上。\n    \n    * 同理多台机器处理\n    \t* 通过哈希算法对数据取哈希值，然后对机器个数取模\n        \t* 这个最终值就是应该存储的缓存机器编号。\n            \n   \n   \n* 一致性hash\n\t* 增加机器时，一般的hash方式需要重新hash\n    \n    * 假设我们有k个机器，数据的哈希值的范围是[0, MAX]。\n    \t* 我们将整个范围划分成m个小区间（m远大于k）\n        * 每个机器负责m/k个小区间。\n        \n        * 当有新机器加入的时候\n        \t* 我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。\n        \n        * 这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。\n        \n        * 实际应用中，还会借助虚拟的环和虚拟的节点\n        \n\n\t* 一致性\n    \t* 当服务器数量变化时，影响到的，数据—-服务器，对应关系变化不是全量的。\n        \t* 对于绝大多数数据来说，是前后一致的","layout":null},"children":[]}]}]}]},{"data":{"id":"bxzy9k5xotc0","created":1572143236179,"text":"树","note":"* 树是一种非线性表结构\n\t* 高度Height\n    \t* 节点的高度 = 节点到叶子节点的最长路径（边数）\n        \n        * 树的高度 = 根节点的高度\n        \n        * 高度是[从下往上]算的\n    \n    * 深度Depth\n    \t* 节点的深度 = 根节点到这个节点的边的个数\n        * 由上往下\n    \n    * 层Level\n    \t* 节点的层数 = 节点的深度 +1\n        * 从1开始，由上往下","expandState":"collapse","layout":null},"children":[{"data":{"id":"bxzy9u9nfog0","created":1572143258171,"text":"二叉树","note":"*  二叉树\n\t* 每个节点最多两个分叉\n    \n* 满二叉树\n\t* 叶子节点全都在最底层\n    * 除了叶子节点之外，每个节点都有左右两个子节点\n    \n    \n* 完全二叉树\n\t* 叶子节点都在最底下两层\n    * 最后一层的叶子节点都靠左排列\n    \t* 并且除了最后一层，其他层的节点个数都要达到最大\n        \n    * 满二叉树又是完全二叉树的一种特殊情况\n        \n        \n","expandState":"expand","layout":null},"children":[{"data":{"id":"bxzzdru2bbk0","created":1572146387442,"text":"二叉树的存储","note":"* 链式存储法\n\t* 基于指针或者引用的二叉链式存储法\n    \n* 顺序存触法\n\t* 基于数组\n    \n    * 根节点存储在下标为1的位置\n    \t* 左子节点：2 * i\n        * 右子节点: 2 * i + 1\n        \n        * 反之：当前节点是i，则父节点是 i/2(右子节点也成立)\n        * 公式适合全部节点\n     \n    * 根节点存储在1的位置是为了方便计算子节点\n    \t* 完全二叉树中，会浪费一个下标为0的位置\n        \n        * 非完全二叉树则会浪费更多的数组存储空间\n    \n    \n* 所以\n\t* 如果某棵二叉树是一棵完全二叉树\n    \t* 那用数组存储无疑是最节省内存的一种方式。\n    \t* 因为数组不需要存储额外的左右子树的指针\n        \n        \n    * 这也是为什么完全二叉树要求最后一层的子节点都靠左的原因。\n        ","layout":null},"children":[]},{"data":{"id":"bxzzf0jpaa00","created":1572146484770,"text":"二叉树的遍历","note":"* 前序遍历\n\t* 对于树中的任意节点来说\n    \t* 先打印这个节点\n        * 然后再打印它的左子树\n        * 最后打印它的右子树。\n\n* 中序遍历\n\t* 对于树中的任意节点来说\n    \t* 先打印它的左子树\n        * 然后再打印它本身\n        * 最后打印它的右子树\n        \n        \n* 后序遍历\n\t* 对于树中的任意节点来说\n    \t* 先打印它的左子树\n        * 然后再打印它的右子树\n        * 最后打印这个节点本身。\n        \n* 即：\n\t* 当前节点在最前就是前序\n    * 中间就是中序\n    * 最后就是后序\n    \n \n* 二叉树遍历的时间复杂度\n\t* 每个节点最多会被访问两次\n    * 所以遍历操作的时间复杂度，跟节点的个数n成正比\n    \t* O(n)\n        \n","layout":null},"children":[]},{"data":{"id":"bz27q9zikgw0","created":1576029214556,"text":"Morris遍历","note":"* Morris遍历二叉树是遍历二叉树的神级方法\n\t* 它的时间复杂度仅为O(n)，空间复杂度为O(1)\n    \t* 最大优势是：空间复杂度\n\n\t1. 拿到一个节点，如果该节点无左子树，那么节点指向它的右节点。\n\n\t2. 如果这个节点有左子树，找到左子树的最右节点。\n\t\t* 如果最右节点指向null，则让最右节点指向当前节点，并将该目标节点向左孩子移动。\n\n\t\t* 如果最右节点已经指向该节点，则让最右节点指向null，并将该目标节点向右孩子移动。\n\n\t* 一直按照上述两大步骤递归，直到遍历完所有节点"},"children":[]}]},{"data":{"id":"by05mtow4a80","created":1572164023420,"text":"二叉查找树","note":"* 二叉查找树（Binary Search Tree）\n\t* 也叫二叉搜索树\n    \t* 顾名思义，二叉查找树是为了实现快速查找而生的\n    \n    * 支持快速查找、快速插入、删除一个数据。\n    \n\n* 结构\n\t* 在树中的任意一个节点\n    \t* 其左子树中的每个节点的值，都要小于这个节点的值\n        * 而右子树节点的值都大于这个节点的值。\n        * `从左到右按照大小顺序，区别于堆，堆没有这种要求`\n        \n   \n* 查找\n\t* 根节点开始遍历\n    \t* 当前节点等于查找值，返回当前\n        * 比当前节点小，找左子树，否则找右子树\n     \n     \n* 插入\n\t* 类似查找，直到找到一个空的叶子节点\n    \n* 删除\n\t* 删除的节点没有子节点\n    \t* 将父节点中的指向当前节点的指针置为空\n        \n    * 有一个子节点\n    \t* 父节点的指针指向删除节点的子节点\n    * 有2个子节点\n    \t* 将右子树的最小子节点与删除节点换位置，删除该最小节点\n        * 此处，使用左子树的最大节点应该也是可以的","expandState":"collapse","layout":null},"children":[{"data":{"id":"by067v5tbqg0","created":1572165672267,"text":"查找树扩展","note":"* [中序遍历]二叉查找树\n\t* 可以输出有序的数据序列\n    \t* 时间复杂度是O(n)，非常高效。\n        \n        \n        \n* 支持重复数据的二叉查找树\n\t* 实际开发中，在二叉查找树中存储的，是一个包含很多字段的对象。\n    \t* 利用对象的某个字段作为键值（key）来构建二叉查找树\n        * 其他字段叫作卫星数据。\n    \n    * 方法1\n    \t* 二叉查找树每一个节点不仅存储一个数据\n        * 通过链表和支持动态扩容的数组等数据结构\n        \t* 把值相同的数据都存储在同一个节点上。\n            \n    * 方法2\n    \t* 每个节点仍然只存储一个数据\n        \n        * 如果碰到一个节点的值，与要插入数据的值相同\n        \t* 我们就将这个要插入的数据放到这个节点的右子树\n            * 即把这个新插入的数据当作大于这个节点的值来处理。\n            \n            \n        * 查找数据的时候，遇到值相同的节点，并不停止查找操作\n        \t* 继续在右子树中查找，直到遇到叶子节点\n            * 这样就可以把键值等于要查找值的所有节点都找出来。\n         \n         \n        * 删除同理","layout":null},"children":[]},{"data":{"id":"by06cum1vnk0","created":1572166062893,"text":"时间复杂度","note":"* 一般的二叉查找树\n\t* 不保证平衡性\n    \n    * 因此极端情况下退化为链表\n        * 查找时间复杂度：O(n)\n    \n    * 最理想，是一棵完全二叉树\n    \t* 插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是O(height)\n        * 即每层判断一个数据\n    \n    * 完全二叉树，时间复杂度等于O(height)\n    \t* 高度为log2n（2为底）\n        * 所以：O(logn)\n        \n        * 推导公式见下\n        * 理解：每一层的数据以指数增长\n        \t* 则查找就是以对数式的增长\n    \n","layout":null},"children":[{"data":{"id":"by0ao8j284o0","created":1572178239632,"text":"树的高度","note":"* 完全二叉树的高度\n\t* 可转为为层数便于运算\n    \n\t* 第一层1个元素\n    * 第二层2个元素\n    * 第三层4个元素\n    ...\n    * 第L层 2^(L-1)个元素\n    \n    * 完全 二叉树最后一层不一定是满的，即介于两者之间\n    \t* n >= 1+2+4+8+...+2^(L-2)+1\n        \t* 完全二叉树最坏的情况，最后一排是1个节点\n        \t* L <= log2n +1\n\n            \n\t\t* n <= 1+2+4+8+...+2^(L-2)+2^(L-1)\n        \t* L>=log2(n+1)\n            \n            \n        * 层数小于等于log2n +1\n        * 高度小于等于log2n（2是底）\n        \n\n* 完全二叉树最左一个节点\n\t* log2n +1\n* 满二叉树层数\n\t* log2(n+1)\n\n* 完全二叉树层数范围\n\t* [ log2(n+1),log2n +1 ]\n    * 高度是层数-1\n        \n \n* 代码求树的真实高度\n\t* 递归每个子树\n    \t* 当前节点高度等于左右子树最大的一个 再加1。","layout":null},"children":[]}]},{"data":{"id":"by078wmll140","created":1572168574933,"text":"对比散列表","note":"1. 有序性\n\t* 散列表中的数据是无序存储的\n    \t* 如果要输出有序的数据，需要先进行排序。\n        \n    * 而对于二叉查找树来说，我们只需要[中序遍历]\n    \t* 就可以在O(n)的时间复杂度内，输出有序的数据序列。\n \n \n2. 扩容、散列冲突\n\t* 散列表扩容耗时很多\n    * 而且当遇到散列冲突时，性能不稳定\n    * 尽管二叉查找树的性能不稳定\n    \t* 但工程中，最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在O(logn)\n        \n  \n3. 尽管散列表的查找等操作的时间复杂度是常量级的\n\t* 但因为哈希冲突的存在，这个常量不一定比logn小\n    \t* 所以实际的查找速度可能不一定比O(logn)快\n        \n    * 加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。\n    \n4. 散列表的构造比二叉查找树要复杂\n\t* 散列函数的设计、冲突解决办法、扩容、缩容等。\n    \n    * 平衡二叉查找树只需要考虑平衡性这一个问题\n    \t* 而且这个问题的解决方案比较成熟、固定。\n        \n5. 最后，为了避免过多的散列冲突，散列表装载因子不能太大\n\t* 特别是基于开放寻址法解决冲突的散列表\n    \t* 太小则会浪费一定的存储空间。","layout":null},"children":[]}]},{"data":{"id":"by0r0v7dm2g0","created":1572224367120,"text":"平衡二叉查找树","note":"* `平衡`二叉树(注意此处没有查找二字)\n\t* 严格定义\n    \t* 二叉树中任意一个节点的左右子树的[高度相差不能大于1]\n        \n    * 按照严格的定义\n    \t* 完全二叉树、满二叉树其实都是平衡二叉树\n        * 非完全二叉树也[有可能]是平衡二叉树。\n        \n   \n* 平衡二叉查找树\n\t* 不仅满足上面平衡二叉树的定义\n    * 还满足二叉[查找]树的特点\n    \n    * 最先被发明的平衡二叉查找树是AVL树\n    \t* 它严格符合我刚讲到的平衡二叉查找树的定义\n        * 即任何节点的左右子树高度相差不超过1\n        \t* 是一种高度平衡的二叉查找树。\n            \n       \n* 很多平衡二叉查找树其实并没有严格符合上面的定义\n\t* 即：树中任意一个节点的左右子树的高度相差不能大于1\n    \n    * 比如我们下面要讲的红黑树\n    \t* 它从根节点到各个叶子节点的最长路径，有可能会比最短路径大一倍。\n    \n    \n* 平衡\n\t* 初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。\n    \t* 只要树的高度不比log2n大很多（比如树的高度仍然是对数量级的）\n        \n        * 尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。","expandState":"collapse","layout":null},"children":[{"data":{"id":"by0rb9wtqpk0","created":1572225182776,"text":"红黑树","note":"* 红黑树\n\t* 它是一种不严格的平衡二叉查找树\n\n\n* 特点\n\t1. 根节点是黑色的\n    \n    2. 每个叶子节点都是黑色的空节点（NIL）\n    \t* 即：叶子节点不存储数据\n        \n    3. 任何相邻的节点都不能同时为红色\n    \t* 即：红色节点是被黑色节点隔开的；\n        \n    4. 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点\n    \n    \n* 实现\n\t* 见《基础》篇章，或原文\n    \n    \n","layout":null},"children":[{"data":{"id":"by0rr3wlmgo0","created":1572226423528,"text":"红黑树的近似平衡","note":"* 红黑树的近似平衡\n\t* 平衡二叉查找树的初衷\n    \t* 是为了解决二叉查找树因为动态更新导致的性能退化问题\n        \n    * “平衡”的意思可以等价为性能不退化。\n    \t* “近似平衡”就等价为性能不会退化的太严重\n    \n    * 只要树的高度稳定趋近于log2n(2为底)\n    \t* 则树就是近似平衡的\n  \n  \n* 近似平衡证明\n\t1. 将红黑树的红色节点拿掉、\n    \t* 黑色节点连上黑色的祖父节点\n        * 二叉树变成四叉树\n        \n    2. 由于黑色节点在每条路径的节点数一样\n    \t* 四叉树中取出部分节点放到叶子节点位置\n        * 四叉树就变成了完全二叉树\n        \t* 节点相同，所以四叉树的底是平的\n            * 取出四叉中的两叉，是满二叉树\n            * 取出的二叉放到叶子节点（自己找位置），是完全二叉树\n        \n    3. 因此，调整前的黑色四叉树，高度小于完全二叉树的高度log2n(2为底)\n    \n    4. 红色节点放回去\n    \t* 红色节点不能连续\n        * 所以红黑树高度最大\n        \t* 小于 2* log2n\n        \n    5. 所以，红黑树的高度只比高度平衡的AVL树的高度（log2n）最多仅仅大了一倍\n    \t* 在性能上，下降得并不多\n        * 实际上红黑树的性能比当前推导的更好。","layout":null},"children":[]}]},{"data":{"id":"by0s2makcpc0","created":1572227325560,"text":"其他平衡二叉查找树","note":"* Splay Tree（伸展树）、Treap（树堆）等\n\t* 绝大部分情况下，它们操作的效率都很高\n    \t* 但是也无法避免极端情况下时间复杂度的退化。\n        \t* 猜测，到达某一阈值才会触发平衡机制\n            * 导致某次操作时间过长\n            \n            \n        * 尽管这种情况出现的概率不大，但是对于[单次操作时间]非常敏感的场景来说，它们并不适用。\n        \n        \n        \n* AVL树是一种高度平衡的二叉树\n\t* 所以查找的效率非常高\n    \n    * 但是，AVL树为了维持这种高度的平衡，就要付出更多的代价。\n    \t* 每次插入、删除都要做调整，就比较复杂、耗时。\n        \n     * 所以，对于有频繁的插入、删除操作的数据集合，使用AVL树的代价就有点高了。\n     \n     \n* 红黑树只是做到了近似平衡\n\t* 并不是严格的平衡\n    \t* 所以在维护平衡的成本上，要比AVL树要低。\n        \n    * 红黑树的插入、删除、查找各种操作性能都比较稳定。","layout":null},"children":[]}]},{"data":{"id":"by0y9f2uuaw0","created":1572244785065,"text":"递归树","note":"* 针对递归代码，分析时间复杂度\n\t1. 使用递推公式\n    \t* 如归并排序、快排的最好时间复杂度\n        \t* 都可以通过递推公式获取时间复杂度\n            \n    2. 有些情况下使用递推公式，会涉及非常复杂的数学推导。\n    \t* 如快排，每次两个区间的比例是1:9 等\n        \n    3. 此时可以使用递归树方式求时间复杂度\n    \n    \n* 递归树\n\t* 将递归的过程画成图，就是一棵树\n    \n    \n\n* 对于[递归代码]，需要使用递归树进行分析时间复杂度\n    * 简单的，不容易出错的，可以使用递推公式\n        \n    * 递推公式\n    \t* 总的时间复杂度 = 下一层时间复杂度 + 一些运算\n        \t* 一直推导到没有下一层\n        * 递归特有的\n        \n        \n    * (区别于)像二叉树的查找、二分查找\n    \t* 每一层需要多少时间复杂度是多少，是确定的\n        \t* 只需要计算树的高度\n            \n        * 这种一般观察每一层的数据特点\n        \t* 符合等比数列特征的就用等比数列求和公式","expandState":"collapse","layout":null},"children":[{"data":{"id":"by0yn0s6t3k0","created":1572245851044,"text":"归并排序","note":"* 每次将区间一分为二\n\t* 分解的操作代价较低\n    \t* 时间消耗量可记为 1\n        \n    * 主要耗时的是合并操作\n    \t* 2个子数组合并为一个\n        * 时间消耗量每层都是n\n        \t* 每层总的元素量不变\n            \n    * 因此时间复杂度就是\n    \t* O(h* n)\n        \t* h为树的高度\n            \n        * 归并排序中，此递归树是满二叉树\n        \t* 高度为log2n(2为底)\n            * 此高度的计算，其实也（可以）是递推，注意不是等比数列求和\n            * 实际为：log2(n+1) -1，约等于log2n（比log2n小）\n            \n        * O(nlogn)  ","layout":null},"children":[]},{"data":{"id":"by10gxv5q080","created":1572251016728,"text":"快排","note":"* 对于快排中，两区间分割的比例不相同时\n\t* 如1:9\n    \n    * 每一层需要操作的节点数还是n\n    \t* 分区点pivot一般来说也是需要移动的\n    * 则时间复杂度依旧是  O(h * n)\n    \n    * 由于区间不是均匀的\n    \t* 每次分为n/10，以及9n/10\n        * 则树最大的高度，等于每次都是9n/10的分支\n        \n    * 高度计算\n    \t* 第零层 n\n        * 第1层 9n/10\n        * 第2层 n* (9^2/10^2)\n        * ..\n        * 第k层 n * (9/10)^k = 1\n        \t* 1表示剩余元素\n            * 则k = log(10/9)n\n            * 其中 10/9是底数\n            \n            \n    * 根据大O标记法，忽略底数\n    \t* 因此快排时间复杂度就是O(nlogn)\n        * 此处默认整棵树的每一层都是n，实际上区间较小的会更早结束\n        \t* 但是即使按照最高的高度，还是O(nlogn)\n        \n        \n    * 区间即使变得更加不均匀\n    \t* 改变的也是底数，底数为常数\n        * 所以时间复杂度不变","layout":null},"children":[]},{"data":{"id":"by11j9noa8o0","created":1572254020235,"text":"斐波拉契递归","note":"* 斐波拉契数列\n\t* 计算第n个斐波拉契数字的大小\n    \t* 每个数字等于前面2个数字之和\n        * 第一个是1 ，第二个是2\n        \n    * 由于每次都分成两部分\n    \t* 一个是数据规模-1\n        \t* 每次都是-1的是最长路径\n            \n        * 另一个是数据规模-2\n        \t* 每次都是-2的是最短路径\n\n\n* 每次分成2区间是1个时间消耗量\n    * 每一层是上一层的2倍\n     \n     \n     \n* 时间复杂度介于\n    * 每次都是-1的满二叉树\n    \t* 最长路径就是n\n        * 2^0 + 2^1 +...+2^n-1\n        \n    * 每次都是-2的满二叉树\n    \t* 最长路径就是n/2\n        * 2^0 +2^1 +...+2^(n/2-1)\n        \n    * 所以时间复杂度就是以上两者之间\n    \t* 第一个总时间复杂度\n        \t* 2^n-1\n        * 第二个\n        \t* 2^n/2 -1\n            \n            \n* 得到的结果还不够精确，只是一个范围\n\t* 但是我们也基知道了上面算法的时间复杂度是指数级的，非常高。\n    \n    \n    \n","layout":null},"children":[]},{"data":{"id":"by124wz16e80","created":1572255716635,"text":"全排列","note":"* 全排列\n\t* 如何把n个数据的所有排列都找出来\n    \t* 即排列组合\n        \n        \n        \n* 注意这里是计算时间复杂度\n\t* 所以不是看有多少种组合\n    \t* 而是有多少种操作（耗多少的单位时间）\n\n\n\n* 可以从后往前组合\n\t* 当然也可以从前往后\n    \n    * 定位最后一个位置\n    \t* 每个元素都轮流替换到该位置\n        * 每个元素替换完成后\n        \t* 递归，定位位置 -1 \n            * 继续将剩余元素轮流放倒数第二位置\n            * 直至只剩一个元素\n            \n    \t* 每次替换的元素需要替换回来\n        \t* 例：1位置元素换到2位置\n            * 当需要2位置原来的元素的时候，已经拿不到了\n            \n       \n","layout":null,"expandState":"expand","layout_right_offset":{"x":1,"y":1}},"children":[{"data":{"id":"by1opzeg8p40","created":1572319431986,"text":"递归树分析","note":"* 递归树分析      \n    * 最后一个位置，每个元素都去坐一下\n    \t* n个时间消耗（单个时间消耗其实是6+1个，与n成正比，此处省略为n）\n    * 即第一层 n\n    * 第二层 n *  n-1\n    * 第三层 n * n-1 * n-2\n    * ...\n    * n * n-1 * ...* 1\n    \t* 时间复杂度大于O(n!)，小于O(n * n!)\n        \n    * 虽然没法知道非常精确的时间复杂度\n    \t* 但是这样一个范围已经让我们知道，全排列的时间复杂度是非常高的。\n        \n\n* 为什么不是直接最后一项？\n\t* 注意递归跟循环不一样\n    * 递归相当于在循环中，嵌套一个小的子循环\n    \t* 有的语句在当前循环中，有的在子循环中\n\n\n","layout":null},"children":[]}]}]},{"data":{"id":"by1uqyyuidk0","created":1572336436066,"text":"堆","note":"* 堆 heap\n\t* 是完全二叉树\n    \n    * 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。\n    \t* 都大于等于的叫大顶堆\n        * 都小于等于的叫小顶堆\n        \n        * `左右子节点的大小没有要求`\n        \n        \n* 由于堆是完全二叉树\n\t* 使用数组存储最省空间\n    \n \n","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"by1vbxqrmzk0","created":1572338079049,"text":"堆化","note":"* 堆化heapify\n    * 将堆的元素进行调整，让其重新满足堆的特性\n    \t* 从下往上\n        * 从上往下\n        \n        \n* 堆中插入一个元素\n\t* 为了满足完全二叉树的特点\n    \t* 新元素插到数组的最后\n\n    * 对插入的元素进行堆化调整\n    \t* 采用从下往上的方式\n        \n        * 跟父节点比较，比父节点大的交换位置（大顶堆），直至比父节点小\n        \n        \n* 堆中删除元素\n\t* 如果删除后，使用子节点替换\n    \t* 数组可能出现空洞（不再是完全二叉树）\n        \n    * 因此，无论删除哪个元素，都跟最后一个元素交换位置\n    \t* 最后一位元素填充被删除的位置\n        * 再堆化\n        \n    * 交换的元素，可能比父节点大，也有可能是比子节点小\n    \t* 根据不同情况，选择自下而上，还是自上而下\n        \n        * 一般就是删除顶部元素\n        \t* 此时就是自上而下\n        * `只有操作堆顶元素才有意义`\n    \n* 完全二叉树高度\n\t* 小于log2n\n    \n    * 插入元素、删除堆顶元素都是\n    \t* O(logn)\n        \n        ","layout":null},"children":[]},{"data":{"id":"by1who0o90g0","created":1572341349175,"text":"堆排序","note":"* 方式1\n\t* 假设数组中只有一个元素\n    \t* 后续元素逐个增加，并做堆化\n    * 直至最后一个\n    \n* 方式2\n\t* 先整体构建一个堆\n    \t* 每次删除堆顶元素\n        \t* 最大值移到数组已排序的交界处\n            \n            \n\n1. 建堆\n\t* 由于完全二叉树的叶子节点是\n    \t* n/2 +1\n        * 指的是下标，此处下标1开始\n        \n    * 第一个非叶子节点是 n/2\n    \t* 从n/2 到1，开始堆化\n        * `最后一个叶子节点的父节点，就是最后一个非叶子节点 根据完全二叉树公式，推导结果一样`\n        * 跟删除顶部节点代码一样\n        \t* 往下比较，遇到第一个不需要调整的就结束\n            \n        * 当前节点可能不只移动一次\n        \t* 当前节点可能比孙子节点还小 \n            \n2. 建好堆后\n\t* 每次取走堆顶元素，并重新调整\n    \t* 即删除元素\n    * 直至只剩一个元素\n         \n         \n* 根节点如果是0开始\n\t* 如果节点的下标是i\n    \t* 那左子节点的下标是 2i+1\n        * 右子节点的下标就是 2i+2\n        * 父节点的下标是  (i-1)/2\n         \n","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"by1yx5fwxcw0","created":1572348204784,"text":"完全二叉树的叶子节点起点","note":"* 证明：满二叉树的 n/2 + 1 开始是叶子节点\n\t* 2^0 + 2^1+...+2^k = n\n    \t* 则k = log2(n+1) -1\n        \n        * 最后一排2^k = (n+1)/2\n        * 所以最后一排第一个节点就是n/2 + 1\n       \n       \n* 对于完全二叉树\n    * 满二叉树基础上增加x个节点\n    \n    * x是双数\n        * 非叶子节点少了 x/2 个\n        * n/2 + 1-x/2 \n        \t*  =(n+x)/2 +1\n            * 等式成立\n    * x是单数\n    \t* 会导致 n 也变成单数\n        \t* 依旧成立","layout":null},"children":[]},{"data":{"id":"by2j8ehz3jk0","created":1572405508703,"text":"时间复杂度","note":"* 堆化时间复杂度\n    * 粗算：每个节点可能从某个高度移到顶部（或靠近顶部）\n        * 最多就是O(logn)\n        * n个节点就是O(nlogn)\n        * 不够准确的地方。主要是高度误差\n       \n       \n    * 更准确计算：\n    \t* 一个节点跟左右子节点的比较并交换的时间消耗量记为1\n    \n    * 第一层 \n        * 最多经过h（高度）次交换\n        * 第一层只有一个元素\n        * 2^0 * h\n    * 第二层： 2^1 * (h-1)\n    * ...\n    * 倒数第二层：2^(h-1)  * 1\n    \n    * 加起来就是时间复杂度\n\n* 2^0 * h \n+ 2^1 * (h-1)\n+ 2^2 * (h-2)\n+ ...\n+ 2^(h-1)  * 1\n\t* 当前式子记为S1\n    \n    * S2 = S1 *2  \n    * S1 = S2 -S1\n    \t* 错项相减(详见原文)\n        * S1 = 2^(h+1) -h-2\n        \t* h=log2n\n\n* 建堆的时间复杂度就是\n\t* O(n)\n* 建堆完毕后，逐个删除最大节点\n\t* 时间复杂度：O(nlogn)\n    \n* 所以总体时间复杂度\n\t* O(nlogn)","layout":null},"children":[]},{"data":{"id":"by2myqdz1u80","created":1572416035381,"text":"堆排序性能分析","note":"* 时间复杂度\n\t* O(nlogn)\n    \t* 见之前推导\n   \n* 空间复杂度\n\t* 需要极个别临时存储空间，所以堆排序 是 原地排序算法\n    \n* 稳定性\n\t* 不是稳定的排序算法\n    \t* 因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作\n        * 所以就有可能改变值相同数据的原始相对顺序。\n        \n        \n* 堆排序的优势\n\t* 时间复杂度比较稳定\n        \n        \n","layout":null},"children":[]},{"data":{"id":"by2nupkb1p40","created":1572418541240,"text":"对比快排","note":"* 对比快排\n\t* 快速排序 要 比堆排序性能 好\n\n    \n    \n1. 堆排序数据访问的方式没有快速排序友好\n    * 快速排序中，数据是顺序访问的\n        \n    * 堆排序中，数据是跳着访问的\n        * 这样对CPU缓存是不友好\n          \n          \n2. 对于同样的数据，在排序过程中\n    * 堆排序算法的数据交换次数要多于快速排序\n        * 堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。\n        \t* 对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了。\n        \n    * 快速排序数据交换的次数不会比逆序度多。\n        * 对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换（或移动）。\n        \n        \n \n","layout":null},"children":[]}]},{"data":{"id":"by2pcz1wlwo0","created":1572422793560,"text":"优先级队列","note":"* 队列最大的特性就是先进先出\n\t* 在优先级队列中\n    \t* 优先级最高的，最先出队\n        \n\n* 实现\n\t* 用堆来实现是最直接、最高效的\n    \t* 其实一个堆就是一个优先级队列\n        * 很多时候，它们只是概念上的区分而已","expandState":"expand","layout":null},"children":[{"data":{"id":"by2phhghzp40","created":1572423147081,"text":"合并有序小文件","note":"* 有100个小文件，每个文件的大小是100MB，每个文件中存储的都是有序的字符串\n\t* 希望将这些100个小文件合并成一个有序的大文件\n    \t* 这里就会用到优先级队列\n        \n        \n        \n        \n* 类似于归并排序的合并函数\n\t* 我们从这100个文件中，各取第一个字符串\n    \n    * 比较大小\n    \t* 如：放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组中删除。\n        \n        \n    * 但是取走一个元素后，对应文件的下一个元素，需要跟剩余的99个子文件再对比一次\n    \t* 效率不高\n        \n    * 此时可以采用优先级队列\n    \t* 即堆\n        \n \n* 优化点\n\t* 从小文件中取出来的字符串放入到小顶堆中\n    \t* 那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串\n    \n    * 将堆顶这个字符串放入到大文件中，并将其从堆中删除\n    \t* 然后再从小文件中取出下一个字符串，放入到堆中。\n        \n    * 删除堆顶数据和往堆中插入数据的时间复杂度都是[O(logn)]\n    \t* 更高效","layout":null},"children":[]},{"data":{"id":"by2pmoiyz8w0","created":1572423554289,"text":"高性能定时器","note":"* java线程池中\n\t* ScheduledThreadPoolExecutor使用的队列：DelayedWorkQueue\n    \t* 使用优先级队列\n    \n    * 注： 是一个支持延时、周期性任务的线程池\n    \n    \n    \n* 好处\n\t* 不用每次都去轮询一下各个任务的执行时间\n    \t* 可能是每秒一次等\n        \n    * 按照执行时间构建一个小顶堆\n    \t* 拿堆顶元素，算出下一次执行时间\n        * 则到了这个时间再去拿即可\n        \n        * 继续判断下一个任务（堆重新堆化，按删除堆顶元素操作）","layout":null},"children":[]}]},{"data":{"id":"by2puvancl40","created":1572424195937,"text":"求TOP K","note":"* 针对静态数据集合\n\t* 数据集合事先确定，不会再变\n    \n* 求静态数据的top K\n    * 维护一个小顶堆\n    \t* 注意不是大顶堆\n        \n    * 小顶堆的大小是k\n    \t* 则堆顶元素就是最小的\n        * 新来一个元素，就跟堆顶元素比较，比堆顶元素大的，替换堆顶元素\n        \t* 再堆化\n            \n    * 遍历数组需要O(n)\n    \t* 一次堆化操作需要O(logK)\n        * 最坏情况下，n个元素都入堆一次，所以时间复杂度就是O(nlogK)。\n    \n\n\n\n\n* 动态数据集合\n\t* 数据集合事先并不确定，有数据动态地加入到集合中\n    \n    \n* 动态数据的topK\n\t* 类似静态数据的操作即可\n    \t* 每次插入都保留前k个，并堆化\n    \n    * 当然也可以插入不堆化\n    \t* 查询的时候再全部排序\n        * 此时时间复杂度：O(nlogK)\n        \t* n是当前数据大小","layout":null},"children":[{"data":{"id":"by2r9wb2k3c0","created":1572428194711,"text":"例","note":"* 假设现在我们有一个包含10亿个搜索关键词的日志文件\n\t* 如何快速获取到Top 10最热门的搜索关键词\n    \n \n* 解决\n\t* 高级的解决方法，比如使用MapReduce等\n    \n    * 如果将处理的场景限定为单机，可以使用的内存为1GB。那这个问题该如何解决？\n    \t* 要有快速计算的能力\n        * 1G 约等于 9个0 的byte\n        \t* 一亿是 8个0 九位数\n            * 所以50byte * 一亿是5G\n        * 同理，一百万是 6个0，跟1m大约相同\n        \n    \n\n* 注意内存的限制，限制的是什么操作\n\t* 所有的计算、排序等，都需要内存\n    \n* 所以\n\t1. 先将大的日志文件遍历\n    \t* hash算法，根据hash值分到10个小的日志文件\n        * 文件的读入，写出是一条一条，内存肯定够\n        \n    2. hash值相同的数据，肯定在同一个小文件\n    \t* 除以10后，每个文件含有关键字大约一千万，一个算50byte\n        * 即使不算不重复数据\n        \n        * 则维护单个文件对应hash表\n        \t* 500m\n        * 取前十\n        \n    3. 每个文件取前十\n    \t* 再一起对比，取全部的前十","layout":null},"children":[]}]},{"data":{"id":"by2q5hsaiwg0","created":1572425028534,"text":"求中位数","note":"* 求动态数据集合中的中位数\n\t* 如果是静态数据，则先排序\n    \t* 再返回 n/2 即可\n        * 偶数的n有两个中位数\n        \t* n/2 \n            * n/2 +1\n            \n            \n* 动态数据的求中位数实现\n\t* 维护两个堆\n    \t* 一个大顶堆\n        * 一个小顶堆\n        \n    * 先排序\n    \t* 大顶堆中存储[前n/2]的数据\n    \t* 小顶堆中存储[后n/2]的数据\n    \n    * 则大顶堆的元素，就是中位数\n    \t* 同理，奇数的后一位中位数，则大顶堆存 n/2 +1 个数据\n        \n        * 总之：确保大顶堆堆顶的元素就是想要的元素\n        \n        \n    * 新增数据\n    \t* 新数据 <= 大顶堆堆顶元素\n        \t* 将新数据插入到大顶堆\n            \n        * 新数据 >= 小顶堆堆顶元素\n        \t* 将新数据插入到小顶堆\n            \n    * 新增数据后，大小顶堆的数据大小不符合前面的约定\n    \t* 大顶堆的多了一个，则堆顶元素往小顶堆移动一个\n        * 同理小顶堆\n        \n        * 大顶堆的最大，是比小顶堆的最小 小一位","expandState":"expand","layout":null},"children":[{"data":{"id":"by2r1cob2io0","created":1572427525063,"text":"时间复杂度","note":"* 插入数据因为需要涉及堆化\n\t* 所以时间复杂度变成了O(logn)\n    \n    * 但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，\n    \t* 所以时间复杂度就是O(1)。","layout":null},"children":[]},{"data":{"id":"by2r1w6zqhk0","created":1572427567551,"text":"其他类似问题","note":"* 如：如何快速求接口的99%响应时间\n\t* 99% 响应时间\n    \t* 如果将一组数据从小到大排列\n        \n        * 这个99百分位数就是大于等于前面99%数据的那个数据\n        \t* 1，2，3，……，100，那99百分位数就是99\n            \n            \n    * n个数据\n    \t* 将数据从小到大排列之后\n        * 99百分位数大约就是第n* 99%个数据\n        \n    * 实现\n    \t* 维护两个堆，一个大顶堆，一个小顶堆\n        \t* 大顶堆中保存n* 99%个数据\n            * 小顶堆中保存n* 1%个数据\n            \n        * 大顶堆堆顶的数据就是我们要找的99%响应时间。\n   ","layout":null},"children":[]}]}]},{"data":{"id":"byi8dchp3a80","created":1573999823251,"text":"B+树","note":"* B+树发明于1972年\n\t* 跳表发明于1989年\n    \t* 跳表的作者有可能就是受了B+树的启发，才发明出跳表来的~~\n    \n    \n* 通过存储在磁盘的多叉树结构，做到了时间、空间的平衡\n\t* 既保证了执行效率，又节省了内存\n    \n* B+树特点\n\t1. 每个节点中子节点的个数不能超过m，也不能小于m/2；\n\t\t* 根节点的子节点个数可以小于m/2，这是一个例外；\n\n\t2. m叉树只存储索引，并不真正存储数据\n    \t* 这个有点儿类似跳表\n        * 此处是指非叶子节点（叶子节点记录数据地址）\n\n\t3. 通过链表将叶子节点串联在一起，这样可以方便按区间查找；\n\n\t* 一般情况，根节点会被加载存储在内存中，其他节点存储在磁盘中\n    \n* B-树\n\t* -是横杠，不是减\n    \n    * B树实际上是低级版的B+树\n    \t* 或者说B+树是B树的改进版\n        \n    * 区别\n    \t* B+树中的节点不存储数据，只是索引，而B树中的节点存储数据\n        \n        * B树中的叶子节点并不需要链表来串联。\n        \n     * B树只是一个每个节点的子节点个数不能小于m/2的m叉树","expandState":"expand","layout":null},"children":[{"data":{"id":"byi9fd05gd40","created":1574002802206,"text":"数据库的功能性需求与非功能性需求","note":"* 功能性需求\n\t1. 根据值查找数据\n    2. 根据区间值查找数据\n    \n* 非功能性需求\n\t* 如性能\n    \t* 时间复杂度\n        * 空间复杂度\n        \n\n* 其他数据结构\n\t* 散列表\n    \t* 不支持区间查找\n    \n    * 平衡二叉查找树\n    \t* 通过中序遍历，可以获取从小到大的有序数据序列\n        \t* 但这仍然不足以支持按照区间快速查找数据\n            \n    * 跳表\n    \t* 支持快速地插入、查找、删除数据\n        \t* 时间复杂度都是：O(logn)\n        * 支持按照区间快速地查找数据\n        \n        * 基本满足要求\n        \t* 但是由于索引存在硬盘，需要减少IO次数，因此还是B+树合适\n            \n            * 理论上讲，对跳表稍加改造，也可以替代B+树，作为数据库的索引实现的。\n        \n        \n","layout":null},"children":[]},{"data":{"id":"byi9k24kfds0","created":1574003170349,"text":"B+树的发展","note":"* B+树是由二叉查找树演化过来的\n\t* 为了让二叉查找树支持按照区间来查找数据，对它进行改造\n    \n    \t* 树中的节点并不存储数据本身，而是只是作为索引\n        \n        * 每个叶子节点串在一条链表上，链表中的数据是从小到大有序的\n        \n        \n    * 数据库表中存储数据非常多\n    \t* 存储在内存不合适，况且还得持久化\n        * 所以索引需要存在硬盘\n        \t* 通常内存的访问速度是纳秒级别的\n            * 而磁盘访问的速度是毫秒级别的\n         \n        * 所以需要尽量减少IO操作\n        \n    * 减少IO操作就要减少树的高度\n    \t* 只有获取了一个节点，才能根据此节点的指针获取下一次节点，这部分需要在内存完成\n        \n        * 减少树的高度，可以构建成m叉树","layout":null},"children":[]},{"data":{"id":"byi9y7za3v40","created":1574004280188,"text":"m叉树中 m的确定","note":"* 不管是内存中的数据，还是磁盘的数据\n\t* 操作系统都是按页来读取的，一次会读一页的数据\n    \t* 一页大小通常是4KB\n        * 这个值可以通过getconfig PAGE_SIZE命令查看\n    \n    * 如果要读取的数据量超过一页的大小\n    \t* 就会触发多次IO操作。\n    \n    \n    * 所以，在选择m大小的时候，要尽量让每个节点的大小等于一个页的大小\n    \t* 读取一个节点，只需要一次磁盘IO操作\n        \n        \n","layout":null},"children":[{"data":{"id":"byio0n5j6m80","created":1574043965485,"text":"树节点数据","note":"* 非叶子节点：\n\t* PAGE_SIZE = (m-1) * 4[keywordss大小] + m * 8[children大小]\n\t\n    * 分成m个区间，需要m-1个key\n    \t* key是int类型的话，占4个字节，所以乘以4\n        \n    * 同时需要存储m个子节点指针\n        * 指针本身就是一个地址，64位占8个，32位占4个\n        * 32,64的判定不取决于你的计算机，而取决于你编译代码的平台。\n        * 你在编译的时候，可以选择32,64.\n  \n  \n  \n* 叶子节点\n\t* 叶子节点虽然不决定m大小，但是一个叶子节点大小也尽量等于一页的大小\n    \t* 即决定一个节点装多少索引数据\n    * PAGE_SIZE = k*4[key大小]+k*8[dataAd大小]+8[prev大小]+8[next大小]\n    \t* k为数据量大小\n        * 当前节点会存储\n        \t* 业务数据的指针地址\n            * key值\n            * 前后树节点指针\n        \n        \n","layout":null},"children":[]}]},{"data":{"id":"byia16ug4dk0","created":1574004512812,"text":"节点的分裂、合并","note":"* 索引可以加快查询的速度\n\t* 但是也会使数据写入效率下降\n    \t* 当然数据本身就是以索引的形式存在\n        * 此处是指额外增加其他索引的情况\n        \n\n* 对于一个B+树来说，m值是根据页的大小事先计算好的\n\t* 往数据库中写入数据的过程中\n    \t* 这样就有可能使索引中某些节点的子节点个数超过m\n        * 这个节点的大小超过了一个页的大小\n        \n    * 不处理的话，读取这样一个节点，就会导致多次磁盘IO操作\n    \n    * 节点分裂（感觉准确的说法不是页分裂）\n    \t* 把超过m个子节点的节点，分裂成两个节点\n        * 节点分裂之后，其上层父节点的子节点个数就有可能超过m个。\n        \t* 我们可以用同样的方法，将父节点也分裂成两个节点。\n            * 这种级联反应会从下往上，一直影响到根节点\n            \n            \n* 叶子节点与非叶子节点\n\t* 叶子节点则是根据数据大小的k决定，与m逻辑类似\n    \n    * 所以，插入数据，由于节点分裂，会导致写入速度降低","layout":null},"children":[{"data":{"id":"byiomc97w6g0","created":1574045665775,"text":"节点合并","note":"* 删除数据时\n\t* 会产生空洞，查询效率降低\n    * 因此子节点数降低达到阈值时\n    \t* 需要合并节点\n        * B+树的阈值就是 m/2\n        \n        * m/2是取整，合并后可能实际大小大于m\n        \t* 此时可以再按照插入节点方式分裂","layout":null},"children":[]}]}]}]},{"data":{"id":"by3c3vonikw0","created":1572486966479,"text":"图","note":"* 图(Graph)\n\t* 一种非线性表数据结构\n    \n    * 顶点\n    \t* 图中的元素\n    \n    * 边 \n    \t* 一个顶点与任意其他顶点建立的连接关系\n        \n    * 度\n    \t* 就是跟顶点相连接的边的条数\n        \n        \n    * 无向图\n    \t* 边没有方向的图\n        \n    * 有向图\n    \t* 边有方向的图\n        \n    * 有向图中，我们把度分为\n    \t* 入度（In-degree）\n        \t* 顶点的入度，表示有多少条边指向这个顶点\n            \n        * 出度（Out-degree）\n        \t* 顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点\n            \n    * 带权图 \n    \t* 带权图中，每条边都有一个权重（weight）\n        ","expandState":"collapse","layout":null},"children":[{"data":{"id":"by3c9ufo6eg0","created":1572487433944,"text":"邻接矩阵存储","note":"* 邻接矩阵\n\t* 图最直观的一种存储方法\n    * 底层是一个二维数组\n    \n    * 优点\n    \t* 简单、直观\n        * 基于数组，所以在获取两个顶点的关系时比较高效\n        \n        * 方便计算\n        \t* 用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算。\n    \n    \n* 对于无向图\n\t* 如果顶点i与顶点j之间有边\n    \t* 我们就将A[i][j]和A[j][i]标记为1\n      \n* 对于有向图\n    * 如果顶点i到顶点j之间，有一条箭头从顶点i指向顶点j的边\n    \t* 那我们就将A[i][j]标记为1\n        \n    * 同理，如果有一条箭头从顶点j指向顶点i的边\n    \t* 我们就将A[j][i]标记为1\n      \n    \n* 对于带权图\n\t* 数组中就存储相应的权重。\n    \n    \n* 比较浪费存储空间\n    * 对于无向图来说，如果A[i][j]等于1，那A[j][i]也肯定等于1\n        * 我们只需要存储一个就可以了\n        \n    * 如果我们存储的是稀疏图\n    \t* 顶点很多，但每个顶点的边并不多\n        * 则邻接矩阵的存储方法就更加浪费空间了（存储的就是边）","layout":null},"children":[]},{"data":{"id":"by3dvrkf71c0","created":1572491972822,"text":"邻接表存储","note":"* 邻接表\n\t* 长得与散列表相似\n    * 每个顶点对应一个链表\n    \t* 链表中存储的是与这个顶点相连接的其他顶点\n        \n        * 有向图：当前顶点指向的顶点\n        * 无向图：跟这个顶点有边相连的顶点\n        \n        \n        \n* 特点\n\t* 邻接表存储起来比较节省空间\n    \t* 但是使用起来就比较耗时间\n        \t* 链表的存储方式对缓存不友好\n            \n            \n        * 遍历顶点对应的链表\n        \t* 看是否有指向\n            \n        * 优化\n        \t* 可以将链表优化为红黑树，跳表、散列表等","layout":null},"children":[]},{"data":{"id":"by3e7kco5000","created":1572492897486,"text":"应用：微博","note":"* 微博\n\t* 微博是有向图\n    * 微信是无向图\n    \n* 社交网络是一张稀疏图\n\t* 使用邻接矩阵存储比较浪费存储空间。\n    \t* 采用邻接表来存储\n        \n        \n    * 用一个邻接表来存储这种有向图是不够的\n    \t* 如：要想知道某个用户都被哪些用户关注了（粉丝列表）\n        \n    * 使用[逆邻接表]\n    \t* 存储的是指向这个顶点的顶点。\n        \n\n* 邻接表改进\n\t* 基础的邻接表是链表\n    \n    * 根据具体需求将链表升级\n    \t* 如微博中粉丝按照首字母排序\n        \n    * 需要排序\n    \t* 需要分页显示（区间查找）\n        * 因此适合跳表\n   \n   \n","expandState":"expand","layout":null},"children":[{"data":{"id":"by3ics8jeag0","created":1572504590911,"text":"数据量大方案","note":"* 数据量大，解决方案\n\t* 可以通过哈希算法等数据分片方式\n    \t* 将邻接表存储在不同的机器的内存上\n        \n        * 将顶点分散到各个机器\n        \t* 根据顶点找的时候，也是先通过hash算法找到对应的机器\n            \n            \n    * 或者是利用外部存储\n    \t* 比如硬盘，数据库\n        * 外部存储的存储空间要比内存会宽裕很多\n        \n        * 数据库方式就是建两列，一列是顶点，一列是指向（或被指向）的顶点\n        \t* 同时建索引\n            \n            \n            \n            \n* 内存\n\t* 用邻接表\n    \n* 数据库\n\t* 持久化存储\n    \n* 专业的图数据库\n\t* 超大图 并且涉及大量图计算","layout":null},"children":[]}]},{"data":{"id":"by3lreaaozk0","created":1572514199335,"text":"深度和广度优先搜索","note":"* 思考一个问题， 实在想不到时，不妨暂时先不要求空间复杂度为O(1)\n\t* 先考虑O(n)的空间复杂度\n    \n\n* 深度优先搜索算法和广度优先搜索算法\n\t\n\t* 既可以用在无向图，也可以用在有向图上\n    \n    * 是简单的暴力搜索算法\n    \t* 仅适用于状态空间不大，也就是说图不大的搜索\n    \n\n\n    ","expandState":"expand","layout":null},"children":[{"data":{"id":"by4fc606tig0","created":1572597639144,"text":"广度优先","note":"* 广度优先遍历\n\t* 先遍历节点附近的\n    * Breadth-First-Search\n    \t* BFS\n    * 求得的路径就是最短路径\n    \n* 实现\n    * 使用一个数组记录每个顶点是否已经被访问\n        \n    * 使用队列存储一些顶点已经遍历\n        * 但是但相连的顶点还没有被访问的顶点\n        * k层已经遍历，保存用于k+1层的遍历\n        \n    * 使用一个数组记录指定当前顶点的前驱顶点\n       \n    * 例子是角标等于该值\n    \t* 其他情况需要转化为角标\n        \n        \n* 时间复杂度\n\t* O(V+E)\n    \t* V：顶点个数\n        * E：边数\n        * 为什么不直接是O(E)\n        \t* 因为代码上，是根据顶点对应的链表进行遍历，从一个顶点切换到另外一个顶点也是有时间消耗de\n        \n    * 简写为O(E)\n    \t* 连通图来说，一个图中的所有顶点都是连通的，E肯定要大于等于V-1\n        * 不连通也遍历不了了\n        \n\n* 空间复杂度\n\t* visited数组、queue队列、prev数组\n    \t* 这三个存储空间的大小都不会超过顶点的个数\n    * 所以空间复杂度是O(V)","layout":null},"children":[]},{"data":{"id":"by4fh15bcdk0","created":1572598020391,"text":"深度优先","note":"* 深度优先\n\t* DFS：Depth-First-Search\n    * 类比走迷宫\n    \n* 实现\n\t* 一个数组记录各个顶点是否已经遍历\n    * 另外一个prev数组记录前驱顶点\n    \n    * 使用递归的方式\n    \t* 一个顶点在判断自己不是寻找的目标顶点后\n        \t* 获取自己的邻接表，递归获取下一个顶点\n            \n            \n* 时间复杂度\n\t* O(E)\n    \t* E表示边的个数\n        * 每条边最多会被访问两次，一次是遍历，一次是回退\n        \t* 此处不太同意作者的的观点。因为回退不占时间。但是结论不影响。\n        \n        \n* 空间复杂度\n\t* O(V)\n    \t* visited、prev数组的大小跟顶点的个数V成正比\n    \t* 递归调用栈的最大深度不会超过顶点的个数\n       ","layout":null},"children":[]}]},{"data":{"id":"byfc2dr5h7k0","created":1573705568564,"text":"拓扑排序","note":"* 应用\n\t* 根据局部有序关系，构建一个全局有序的关系\n    \t* 例：如何确定代码源文件的编译依赖关系\n\n\n* 实现\n\t* 算法构建在具体的数据结构之上\n\t\t* 使用 图 这种数据结构\n    \n    * 每个源文件是图的一个顶点\n    \t* 源文件之间的依赖关系可以用边来表示\n        * 即：有向图\n        \n        * b依赖a，则a先执行\n        \t* 表示：a -> b\n            * 入度为0才轮到当前顶点\n            \n    * 有向无环图\n    \t* 拓扑排序本身就是基于有向无环图的一个算法\n        \t* 因为出现环之后拓扑排序无法工作\n            * 即不能循环依赖\n            \n            \n* 时间复杂度\n\t* 两种算法的都是O(V+E)\n    \t* V: 顶点个数\n        * E: 边的个数\n        \n    * 关于图的时间复杂度计算\n    \t* 要以邻接表（数组+链表）的结构去算\n        * 不能以抽象的图结构去思考\n        \t* 不准确且耗时\n     \n    * 图不一定是连通图\n    \t* E并不一定大于V，两者的大小关系不确定\n        * 所以两者不能简化","expandState":"collapse","layout":null},"children":[{"data":{"id":"byff9pme1zc0","created":1573714606276,"text":"Kahn算法","note":"*  邻接表\n\t* 记录出度的节点\n    * 当前节点处理完，才能处理指向的下一节点\n    \n* 实现\n\t1. 先遍历邻接表，统计每个顶点的入度\n    \n    2. 将入度为0的节点放进队列，逐个处理\n    \t* 每处理一个节点，就根据邻接表的出度记录\n        * 将对应节点的入度-1\n    \n    3. 每个节点的入度减少之后，判断是否为0\n    \t* 是的话，放进队列等待处理\n        \n        \n* 优先处理入度数为0的\n\t* 即每次处理当前最佳的节点\n    * 体现了贪心算法的思想\n    \n   \n\n    \n","layout":null},"children":[]},{"data":{"id":"byff9w30u6o0","created":1573714620343,"text":"DFS深度优先搜索算法","note":"* 准确的说\n\t* 此处叫深度优先遍历\n    \n1. 先根据邻接表构建逆连接表\n\t* 即指向的是比自己先处理的顶点\n        \n        \n2. 递归处理每个顶点\n\t* 先输出它可达的所有顶点\n    \t* 把它依赖的所有的顶点输出了\n        \n        * 然后再输出自己\n        \n    * 只要没有环状的边\n    \t* 通过深度的递归，终将找到一个不指向其他顶点的","layout":null},"children":[]},{"data":{"id":"byg9wv6jfds0","created":1573801054051,"text":"使用场景","note":"1. 通过局部顺序来推导全局顺序\n\t* 一般都能用拓扑排序来解决\n\n\n2.  两种算法都可以检测图中是否有环\n\t* kahn:最后输出的顶点个数，少于图中顶点个数\n\t\t* 图中还有入度不是0的顶点\n\t\t* 说明图中存在环\n    \n    * DFS\n    \t* (未实践) 单次深度遍历时，计算每个顶点访问次数\n        * 超过1就是有环\n        \n        \n3. 环检测应用\n\t* 如数据库中所有用户的推荐关系是否存在环\n    \t* 单个用户则不需使用拓扑排序来处理\n        \t* 直接逐个关系遍历，记录遇到用户的次数\n    \n    ","layout":null},"children":[]}]},{"data":{"id":"bygae8rttig0","created":1573802415828,"text":"最短路径算法","note":"* 最短路径算法\n\t* 准确说，是单源最短路径算法\n    * 最出名的：Dijkstra算法\n    \n","expandState":"expand","layout":null},"children":[{"data":{"id":"byh07v48d740","created":1573875264775,"text":"Dijkstra算法","note":"* Dijkstra算法\n\t1. 使用一个数组，记录每个顶点与[起始顶点]的距离\n\t\t* 初始值都为int的最大值\n        \n    2. 将遍历到的，还没处理的顶点放进优先级队列\n    \t* 还没处理，就是还没根据这个顶点，获取它下一步的顶点\n    \n    \n    3. 每次从优先级队列中获取距离起始最近的顶点\n    \t* 此处使用了动态规划的思想\n        * 把相同顶点的距离，合并留下最短的一个\n        \n    4. 通过此逻辑，每次都取离起始顶点的最近路径获取的下一个顶点\n    \t* 遇到顶点恰好是目标顶点，则此时经过的路径就是最短路径\n        \n        \n    5. 当然同时需要一个数组来记录路径\n    \t* 到达某顶点的路径假如有多条，修改距离的同时，也要修改记录来源的上一个顶点\n        \n    6. 另外还需要记录每个顶点是否处理过（进入过优先级队列）\n    \t*  防止重复处理","expandState":"collapse","layout":null},"children":[{"data":{"id":"byh750zisy80","created":1573894790228,"text":"时间复杂度","note":"1. 最外层while循环\n\t* 优先级队列中取数据，最多执行V次\n    \t* V：顶点数\n    \n    * 内部for循环，最多E次\n    \t* E：边数\n        * 这里的E不是一个顶点的边，而是全部顶点的边的和\n        \n        * 遍历E，时间复杂度就是O(E)\n        \t* 这里V是肯定小于E的\n            * 即使相等也是O(E)\n     \n    * 每个数据插入优先级队列\n    \t* 最多logV次操作\n        * O(logV)\n        \n \n\t* 所以时间复杂度\n    \t* O(ElogV)","layout":null},"children":[]}]},{"data":{"id":"byh727czukg0","created":1573894569011,"text":"地图上的最短路线","note":"* 每个岔路口就是一个顶点\n\t* 路的长度就是边的权重\n    \t* 单行道的话就是一个有向边\n \n \n* 地图范围不大的时候\n\t* 使用Dijkstra算法\n    \n    \n* 范围较大\n\t* 做工程不像做理论，一定要给出个最优解。\n    \t* 理论上算法再好，如果执行效率太低，也无法应用到实际的工程中\n     \n    * 因此，经常要根据问题的实际背景，对解决方案权衡取舍\n    \t* 很多时候，为了兼顾执行效率，我们只需要计算出一个可行的次优解就可以了\n        \n        \n    * 大地图优化\n    \t* 两点之间的最短路径或者说较好的出行路径，并不会很“发散”\n        * 只会出现在两点之间和两点附近的区块内\n        \n        * 我们可以在整个大地图上，划出一个小的区块，这个小区块恰好可以覆盖住两个点，但又不会很大\n        \t* 这个小区块内部运行Dijkstra算法\n            * 这样就可以避免遍历整个大图，也就大大提高了执行效率。\n            \n    * 俩个地点距离远\n    \t* 如北京某地点与上海某地点\n        \n        * 把北京、上海分别看作一个顶点\n        \t* 先规划大的出行路线，这些大的出行路线就是某些主干道等\n            * 然后再细化每个阶段的小路线\n        ","layout":null},"children":[]},{"data":{"id":"byh7nwg9zrs0","created":1573896269276,"text":"最短时间、最少红绿灯","note":"* 最短时间\n\t* 边的权重改为时间\n    \t* 时间会根据实时拥堵情况变化\n        * 但是，计算的某一时刻，拥堵情况是不变的\n    \n    * 获取路的拥堵情况\n    \t* 速度、路长等，计算转化时间\n        * （方案为个人思路）\n        \n* 最少红绿灯\n\t* 经过的每条边对应一个红绿灯\n    \t* 相当于忽略岔路口，直接用红绿灯作为顶点\n        * 有权图可以退化为无权图\n        \n        * 可以继续使用Dijkstra算法，或者使用广度遍历也能实现","layout":null},"children":[]},{"data":{"id":"byh80cz8ddc0","created":1573897245621,"text":"Dijkstra算法其他使用场景","note":"* Dijkstra算法的核心\n\t* 无疑是使用优先级队列，每次都先处理当前最优的子问题\n    \t* 处理最优，目的就是为了，保证当前最优的选择情况下，达到目的（即选择的路径是最优的）\n        \n   \n* 例子\n\t* 翻译句子需要翻译每个单词\n    \t* 每个单词系统会给几种翻译结果\n        * 每个结果有评分，且结果之间本身就有序\n    \n    * 求前k个高分的组合\n    \n    \n* 由于本身有序（假设从高到低）\n\t* 则全部单词的第一个进行组合\n    \t* 肯定是最高分\n        * 放进优先级队列\n    \n    * 次高分就肯定是其中一个单词换了自己的次高分\n    \t* 每个单词都尝试换一次 次高分的组合，就是第一个顶点连接的下一批顶点\n        \t* 全部放进队列\n    \n    \t* 在当前 最高的组合中选最高的进行下一步\n        \t* 每个顶点的下一批顶点，分数是肯定比当前顶点低的，但是不一定比当前顶点的同批顶点分数低\n            * 所以就用优先级队列自动对比\n        \n    * 则每次从队列中出列的都是剩下的最高分组合\n    \t* 最高分的下一批顶点，与已经在队列中的继续比较\n        \t* 自动会筛选最大的\n        *  获取下一批的逻辑，就是当前的单词每个轮流替换自己的下一个翻译结果","expandState":"collapse","layout":null},"children":[{"data":{"id":"byhallmk4zk0","created":1573904552313,"text":"时间复杂度","note":"* 需要前k个组合数据，单词个数为n\n\t* 出列一个数据，入列n个数据\n    \t* 共需出列k个\n        * 即：nk个入列\n        \n    * 队列中的数据最多nk个\n    \t* 单个入列：O(lognk)\n    \n    * 总时间复杂度\n    \t* O(nk * log(nk))","layout":null},"children":[]}]},{"data":{"id":"byipl6x56x40","created":1574048396907,"text":"A*算法","note":"* 如果图比较大\n\t* 使用Dijkstra最短路径算法执行耗时会很多\n    \n    * 真实软件开发中的问题\n    \t* 一般情况下，我们都不需要非得求最优解（也就是最短路径）\n        \n    \t* 在权衡路线规划质量和执行效率的情况下，我们只需要寻求一个次优解就足够了\n        \n\n* A* 算法是对Dijkstra算法的优化和改造\n\t* Dijkstra算法每次找到跟起点最近的顶点，往外扩张（有点类似BFS）\n    \t* 但是扩展的方向是盲目的（四周）\n        \n        * 前期遍历的方向甚至有可能离终点更远\n        \n    * 因此，不但要考虑与起点的距离，还要考虑与终点的距离\n    \t* 离终点距离无法知道，但是可以估计\n        \n  \n* A* 算法是一种启发式搜索算法（Heuristically Search Algorithm）\n\n\t* 启发式搜索算法利用估价函数，避免“跑偏”，贪心地朝着最有可能到达终点的方向前进。\n    \t* 这种算法找出的路线，并不是最短路线。\n        * 但是，实际的软件开发中的路线规划问题，我们往往并不需要非得找最短路线。\n        * 所以，鉴于启发式搜索算法能很好地平衡路线质量和执行效率，它在实际的软件开发中的应用更加广泛。\n        \n    \n   ","expandState":"collapse","layout":null},"children":[{"data":{"id":"byiq6bka4r40","created":1574050052660,"text":"距离估计","note":"* 启发函数（heuristic function）\n\t* 可以通过这个顶点跟终点之间的直线距离，即欧几里得距离\n\t\t* 来近似地估计这个顶点跟终点的路径长度\n    \t* 路径长度跟直线距离是两个概念\n   * 距离记作h(i)（i是顶点编号）\n   \n   \n   \n* 由于欧几里得距离的计算需要开根号\n\t* 涉及比较耗时的开根号计算\n    \n    * 一般通过更加简单的距离计算公式：曼哈顿距离（Manhattan distance）\n    \t* 两点之间横纵坐标的距离之和\n        \n        * 计算的过程只涉及加减法、符号位反转，比欧几里得距离更高效\n        \t* 符号位反转是指，单个坐标距离取绝对值\n            \n            \n* 估价函数（evaluation function）\n\t* f(i)=g(i)+h(i)\n    \t* g(i) 当前顶点与起始顶点的距离\n        * h(i) 与终点之间的估算距离","layout":null},"children":[]},{"data":{"id":"byivgjxa49c0","created":1574064960052,"text":"实现","note":"* 实现\n    * 基于Dijkstra算法，基本一致。改进了以下几点\n        \n    1. 每个顶点增加x,y两个坐标位置\n    \t* 实际开发时，对应就是经纬度\n        \n    2. 小顶堆中。排序的依据不再是距离起始顶点的距离\n    \t* 而是距离起始点距离+距离终点距离\n        * 距离终点距离根据x，y计算曼哈顿距离\n        \n    3. Dijkstra算法是处理的当前顶点的时候\n    \t* 判断是否为终点顶点\n        * 此时当前顶点已经是距离起始的最短路径节点\n        \n        * A* 算法中，是遍历边的时候就看边的另一头是否为终点\n        \t* 是的话直接返回\n            \n            \n            \n* A* 算法不是最短路径\n\t*  算法利用贪心算法的思路，每次考察f值最小的顶点\n    \t* 遍历该顶点边的时候，找到终点就返回了\n    * 没有遍历全部路径，所以所得路径可能不是最优\n    \n    * 同时曼哈顿距离并不代表真实的距离\n    \t* 两个曼哈顿距离短的，只能证明直线距离短，他们之间可能并没有路（边）\n        \n     \t\n        ","layout":null},"children":[]},{"data":{"id":"byivwlugihs0","created":1574066218061,"text":"游戏寻路问题","note":"* 游戏中的地图\n\t* 没有规划非常清晰的道路，更多的是宽阔的荒野、草坪等\n    \t* 没法把岔路口抽象成顶点，把道路抽象成边\n        \n    * 把整个地图分割成一个个的小方块\n\t\t* 在某一个方块上的人物，只能往上下左右四个方向的方块上移动\n        \n        * 可以把每个方块看作一个顶点\n        \n        * 两个方块相邻，就在它们之间连两条有向边，并且边的权值都是1\n   \n    * 这个问题就转化成了\n    \t* 在一个有向有权图中，找某个顶点到另一个顶点的路径问题\n        * 然后套用A* 算法","layout":null},"children":[]}]}]}]},{"data":{"id":"by5eppvr84g0","created":1572697439992,"text":"字符串匹配算法","expandState":"collapse","layout":null},"children":[{"data":{"id":"by5eq9p19tk0","created":1572697483122,"text":"BF算法（Brute Force）","note":"* BF算法\n\t* Brute Force\n    \t* 暴力匹配算法\n        * 或朴素匹配算法   \n    * 简单、易懂\n    \t* 但相应的性能也不高\n        \n* 主串和模式串\n\t* 字符串A中查找字符串B\n    \t* A就是主串\n        * B就是模式串   \n    * 主串的长度n，模式串m\n    \t* 则n>m\n\n* 算法思想\n\t* 在主串中，检查起始位置分别是0、1、2…n-m\n    \t* 且长度为m 的n-m+1个子串\n        * 看有没有跟模式串匹配的\n        \n \n* 时间复杂度\n\t* 极端情况下，对比n-m+1次\n    \t* 每次m个字符    \n    * O(n * m)\n\n* BF算法时间复杂度较高\n    * 但是实际的软件开发中\n    \t* 大部分情况下，模式串和主串的长度都不会太长\n        * 且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了\n        \n    * 因此实际算法执行效率要比这个时间复杂度高很多\n    \n    * 优点\n    \t* 算法思想简单，代码实现也简单。简单意味着不容易出错\n        * 在工程中，在满足性能要求的前提下，简单是首选。\n        * 这也是我们常说的KISS（Keep it Simple and Stupid）设计原则。","layout":null},"children":[]},{"data":{"id":"by5f1ewil0g0","created":1572698356464,"text":"RK算法","note":"* Rabin-Karp算法\n\t* 两位发明者Rabin和Karp的名字来命名的\n    * 是BF算法的升级版\n    \n\n* 相对BF算法的优化点\n\t* BF算法需要每次逐个对比每个字符\n    \n    * RK算法中\n    \t* 通过哈希算法对主串中的n-m+1个子串分别求哈希值\n        * 然后逐个与模式串的哈希值比较大小\n        \n    * 因为哈希值是一个数字，数字之间比较是否相等是非常快速的\n    \t* 所以模式串和子串比较的效率就提高了\n        \n        \n* 时间复杂度\n\t* 计算子串哈希值\n    \t* O(n)\n        * 通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值\n        \t* 除了第一个子串，后，后面每一个子串都是根据上一个子串计算获得\n            * 计算过程中其他额外的参数也是固定的前后2个\n            \n    * 模式串哈希值与子串哈希值的比较\n    \t* 每一次比较是O(1)\n\t\t* O(n)\n        \t* 比较 n-m+1 次\n\n    * 总体就是O(n)\n       \n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"by5fkyfg3200","created":1572699887886,"text":"子串哈希值计算","note":"* 计算子串哈希值的时候，我们需要遍历子串中的每个字符\n\t* 模式串与子串比较的效率提高了\n    \t* 但是，算法整体的效率并没有提高\n\n\n* 计算子串哈希值效率优化\n    * 假设比较的字符串的字符集包含k个字符\n    \t* 字符串的字符集如果是有范围的\n        * 则用一个K进制数来表示一个子串\n        \t* 这个K进制数转化成十进制数，作为子串的哈希值\n    \n    * 例：字符集是26位小数\n    \t* a表示0，b表示1...\n    \t* 则 cba = 2* 26* 26+ 1* 26 + 0* 1 = 1353\n        \n        \n        \n    * 相邻的字符串只是前后字符不一样\n    \t* [相同部分]的字符串\n        * 前子串* 26 = 后子串\n        \t* 26为进制\n            \n    * 所以后子串的计算公式\n    *  = （前子串 - 首位）* 26 + 后子串末位\n    \t* 首位记得乘以26^(m-1)\n\n* 另外\n\t* 通过查表的方法来提高效率。事先计算好26^0、26^1、26^2……26^(m-1)\n    \t* 并且存储在一个长度为m的数组中\n        * 直接取，省去计算时间","layout":null},"children":[]},{"data":{"id":"by5gqpu2asg0","created":1572703160474,"text":"hash冲突","note":"* 以上的hash函数是没有hash冲突的\n\t* 假如子串很长\n    \t* hash值有可能超过int的最大值\n        \n\n* 方案\n\t* 则允许hash冲突\n    \t* 保证在int的范围内\n        \n* 例：不再每位数乘以进制\n    * 直接将各个位数加起来\n       * 则数据范围旧会小很多\n    \n    * 使用时，先对比hash值\n    \t* hash值相同再全量对比\n        \n\n* 因此\n\t* 哈希算法的冲突概率要相对控制得低一些\n    \t* 如果存在大量冲突，就会导致RK算法的时间复杂度退化\n        * 极端情况下，每次都要再对比子串和模式串本身，那时间复杂度就会退化成 O(n*m)\n        \n\n* 字符串的值\n\t* 不直接使用自然数\n    * 而是适应素数（质数）\n    \t* 则冲突更少一些\n        * 如 10 之可能是 2、3、5的质数和。但是对应的自然数就很多\n        \t* 如 4 4 2 ，3 6 1 等\n        \n   \n        \t","layout":null},"children":[]}]},{"data":{"id":"by5iwsfa4540","created":1572709278515,"text":"BM算法","note":"* BM（Boyer-Moore）算法\n\t* 名字是人名\n    * 一种非常高效的字符串匹配算法\n    \n    \n    \n","expandState":"collapse","layout":null},"children":[{"data":{"id":"by5yclydrbc0","created":1572752834908,"text":"坏字符规则","note":"* 坏字符规则 bad character rule\n\t* 模式串与主串的子串匹配时\n    \t* 从模式串的末尾往前匹配\n        * 好处：主串的位置越靠后，滑动的距离越长，如坏字符串在模式串中不存在的情况\n        \n    * 发现某个字符无法匹配\n    \t* 主串的这个字符就是坏字符\n        \n        \n* 坏字符在模式串中不存在\n\t* 模式串往后移\n    \t* 首位移到坏字符的后一位\n        * 有坏字符的部分肯定不匹配\n        \n        \n* 坏字符在模式串中存下\n\t* 则将模式串中的该字符与坏字符对齐\n    \t* 因为他们有可能令模式串与主串刚好匹配\n        * 模式串中有多个匹配，则选最后一个，防止过多的移动\n\n\n* 总结：\n\t* 坏字符串的下标记为si\n    \t* 注意是模式串的下标\n        \n    * 模式串中，存在与坏字符串相同的字符\n    \t* 下标记为xi\n        * 则xi 的大小是[0，m-1]，不存在记为 -1\n        \n        \n    * 移动的位数公式\n    \t* si-xi\n        \n        * 因为需要对齐，对齐的距离就是相差的距离，所以减法\n        \t* 不存在相当于与模式串的前面一位（不存在）的对齐，所以要再加一位，因此减去-1","layout":null},"children":[{"data":{"id":"by629kn80d40","created":1572763881404,"text":"时间复杂度与注意事项","note":"* 时间复杂度\n\t* 最好情况下\n    \t* O(n/m)\n        * 每次最后一位都不匹配，并在模式串中不存在的情况\n        \n        \n   \n* 注意\n\t* 坏字符串出现的位置si\n    \t* 可能比模式串中最后出现的该字符xi还靠前\n        \n        * 因此si-xi有可能是负数\n        \n    * 单纯使用坏字符规则时\n    \t* 主串的对其操作，应该只从坏字符串前的该相同字符对齐\n        * 但是实际操作中，由于我们为了快速计算，只保留了最后一位\n        \n    * 因此，这种场景，要么只仅仅往后后移动一位保险\n    \t* 或者使用第二种规则，即好后缀规则\n        \n        \n        \n* 由于坏字符在已经匹配的部分找到相同的字符\n\t* 才会有这种情况\n    * 因此肯定是已经匹配好的一部分\n    \t* 即肯定适用好后缀规则\n        \n* 只要满足好后缀规则\n\t* 肯定移动正数的位置\n    \n    * 所以同时满足两个规则的话\n    \t* 选择往前移动最多的即可","layout":null},"children":[]},{"data":{"id":"by63i3atqqw0","created":1572767370036,"text":"实现","note":"* 坏字符规则中\n\t* 需要计算xi的位置\n    \t* 即模式串中，最后一次出现该坏字符的位置\n        \n* 如果顺序遍历\n\t* 肯定性能较差，影响时间复杂度\n    \n    \n* 使用散列表\n\t* 假设字符集不是很大，此时直接使用数组作为散列表\n    \t* 字符的ASCII码作为角标\n        * hash肯定不冲突\n    \n    * 实际应用中，可以使用hashmap等\n    \t* 记录模式串中每个字符最后出现的角标\n        \n \n* 本例中\n\t* 使用数组，能快速定位\n    \t* 但是使用场景有限制\n        * 字符集不大，如小写字母等，如果是汉字则要用hash表","layout":null},"children":[]}]},{"data":{"id":"by5ycxz4rko0","created":1572752861075,"text":"好后缀规则","note":"* 好后缀规则（good suffix shift）\n\t\n* 主串的子串，与模式串已经匹配好的部分\n\t* 叫好后缀\n    \t* 记为{u}\n        \n    * 将好后缀在模式串中查找\n    \t* 找到的另外一个匹配的子串\n        * 记为{u*}\n        \n        * 从后往前，能找到的第一个{u*}。则直接移动与当前的后缀{u}对齐\n\n\n* 如果找不到{u*}\n\t* 说明当前这个好后缀不可能与主串完全对的上\n    \n    * 但是还是有可能部分对的上\n    \t* 此时只可能是与模式串的前面部分对的上才有意义\n        \t* 因为好后缀的一部分与模式串的中间部分可以匹配，但是模式串最前面肯定还是不匹配。这样是没有意义的\n      \n      \n      \n* 以上场景总结为：\n    * 好后缀字符串中，它的后缀子串是否与模式串的前面部分吻合\n    \t* 即模式串中，已经确定的好后缀部分，它自己的部分后缀\n        * 是否与模式串的部分前缀匹配","layout":null},"children":[{"data":{"id":"by63w8ep6e80","created":1572768478253,"text":"实现","note":"* 两个查找\n\t* 模式串中，与好后缀相同的子串\n    * 模式串的前缀子串，与好后缀的部分后缀匹配的\n    \n* 暴力匹配\n\t* 性能低\n    \n* 对模式串进行预处理\n\t* 预处理的时候，好后缀还没确定\n    \t* 因此逐个处理\n        \n \n* 使用suffix数组记录{u*}\n\t* suffix数组的下标\n    \t* 表示模式串的后几位后缀字符\n        \n        * 例： suffix[1] 最后一位\n        * suffix[2] 最后两位\n        * ...\n\t* 即第0位不使用\n    \n    * 数组中的值，存储的是\n    \t* 对应下标代表的后缀字符串，在模式串中最后出现的子串[{u*}的起始下标值]\n        * 当然{u*}肯定是不包含{u}的\n        \n        * 代码中，如果从后往前，某一个后缀子串找不到对应的{u*}了\n        \t* 则没必要继续往前了\n   \n   \n   \n* 使用一个boolean类型的prefix数组\n\t* 记录模式串的后缀子串是否能匹配模式串的前缀子串\n    \t* 即当前这个后缀子串，是不是与模式串的前缀吻合","expandState":"expand","layout":null},"children":[{"data":{"id":"by65g7ig7r40","created":1572772864696,"text":"预处理模式串实现细节","note":"* 拿下标从0到i的子串（i可以是0到m-2）与整个模式串\n\t* 求公共后缀子串\n    \n* 即i逐渐变大\n    * 子串从一个元素到m-1个元素\n        \n    *  每个子串与整个模式串对比\n    \t* 逐个字符对比\n        \t* 从后往前，子串的最后一个字符，与模式串的最后一个对比\n            * 子串倒数第二个字符，与模式串倒数第二个字符对比\n            * ...\n            \n    * 匹配吻合的，记录在[suffix]数组内\n    \t* 模式串往前移的指针移动了多少位\n        \t* 下标就是多少位\n            * 值为起始的i-移动的位数k\n        * 每次匹配上了就记录在数组中\n        \t* 注意是每次，相同的i可能有多次吻合\n        \t* 后面范围更大的子串吻合后将覆盖前面的\n            \n    * 匹配不吻合的\n    \t* 结束当前i的循环\n     \n    * 如果当前i能一直匹配后缀，直到下标为0都吻合\n    \t* 则在[prefix]数组中记录\n   ","layout":null},"children":[]},{"data":{"id":"by660x5lx280","created":1572774487799,"text":"使用两预处理的数组","note":"* 先对比主串，获取好后缀\n\t* 在suffix数组中查找\n    \t* 根据好后缀有多少位，就是对应的数组下标\n        * 对应数组值存在（不是-1）\n        \t* 则直接将模式串移动\n            \n        * si - suffix[k] +1\n        \t* 坏字符串的后一位对齐{u*}的第一位\n            \n    * 找不到{u*}\n    \t* 逐个遍历比{u*}小的各个子串对应的prefix[k] 是否为ture\n        \n        * 从大到小，第一个为true的\n        \t* 说明此好后缀的子串与模式串前k位吻合\n            \n      \n        * 移动：m-k\n        \t* 全部不吻合移动m位，吻合多少位，就少移动多少\n     \n    * 全部为false\n    \t* 全部不吻合\n        * 移动m位（一个模式串长度）","layout":null},"children":[]}]}]},{"data":{"id":"by693vn87n40","created":1572783182933,"text":"总体效率","note":"* 空间复杂度\n\t* 坏字符规则需要一个hash表或数组\n    \t* 来储存模式串中出现的字符\n    \n    * 使用数组的话\n    \t* 适用字符集少，字符集越大，空间消耗越大\n        * 优点：速度快\n        \n    * 一般hash表\n    \t* 适合模式串不长的情况，一般也不会太长\n        * 但是会有hash冲突，速度会慢\n        \n        \n    * （一定要用数组的话）如果我们处理字符集很大的字符串匹配问题\n    \t* bc数组对内存的消耗就会比较多\n        * 因为好后缀和坏字符规则是独立的\n        \t* 如果我们运行的环境对内存要求苛刻\n            * 可以只使用好后缀规则，不使用坏字符规则，这样就可以避免bc数组过多的内存消耗。\n        * 不过，单纯使用好后缀规则的BM算法效率就会下降一些了。 \n        \n        \n\n    * 好后缀需要两个数组\n    \t* suffix，prefix\n        * 与模式串大小有关\n \n* 时间复杂度\n\t* 本版本中，不是最优版本\n    * suffix，prefix预处理的时间复杂度最坏是O(m^2)。\n    \t* 如：aaaaaaa\n        \n    * BM算法的时间复杂度分析较为复杂\n    \t* 最坏情况下，BM算法的比较次数上限是3n","layout":null},"children":[]}]},{"data":{"id":"by6brg4j1fc0","created":1572790672110,"text":"KMP算法","note":"* KMP 名字是三位作者的名字组合\n\t* BM更快，KMP更出名\n\n* KMP算法中\n\t* 从前往后匹配\n    \t* 匹配到坏字符后，前面吻合的部分称为：好前缀\n        \n\n* 好前缀中\n\t* 如果它的某个后缀字符串，与模式串的前缀匹配上\n    \t* 则移动对齐该后缀子串\n        * 为了防止移动过多，因此要选最长的后缀子串\n    \n    \n* 移动的距离\n    * 模式串中坏字符下标，记为j\n    \t* 对应主串为i\n        \n    * 最长可匹配前缀子串的长度k\n        * 则还有k位重合\n        * 移动距离 = 总长度 - 重合长度\n            \n    * 总长度 = 坏字符下标\n        * 移动 j-k\n        \n        \n* 距离的另外一种思路\n\t* 坏字符\n    \t* 主串下标是i，模式串是j\n        \n    * 好前缀末尾字符下标 j-1\n    \t* next数组对应 next[j-1]\n\n\t* next[j-1]的值\n    \t* 最长可匹配前缀末尾下标\n        * 其实就是长度k，再-1，但是此处长度没用上\n        \n    * 新的j要跟i继续对比\n    \t* 所以 j = next[j-1] +1\n\n\n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"by6d2d9bcbk0","created":1572794348985,"text":"next数组","note":"* 使用一个next数组存储\n\t* 每个前缀的最长可匹配前缀子串的[结尾字符下标]\n    * 也叫叫失效函数（failure function）\n\t\n    \n* 对于模式串每个前缀子串\n\t* 都有可能是最好前缀\n    \t* 求他们各自的最长可匹配前缀\n        \n        * [数组下标]是当前好前缀的结尾字符下标\n        \t* 如只有一个字符的好前缀。结尾就是0（这个比较特殊，他肯定没有最长好前缀，因为没有子串）\n        \n        * [数组值]为最长可匹配前缀结尾字符下标\n        \n    * 如next[i] =k\n        * 则在好前缀 b[0~i]中\n        * 0~k 可匹配吻合好前缀的后缀部分\n        * 0~k 匹配 i-k ~i\n            \n\n    \n    \n    * 每个前缀求最长可匹配子串时\n    \t* 不包含当前整个前缀\n        \t* 至少从第二位开始\n            \n\n","layout":null},"children":[{"data":{"id":"by6ssok1bvk0","created":1572838727695,"text":"next数组预处理","note":"1. 如果next[i-1]=k-1\n\t* 即 0~k-1 匹配 i-k ~i-1\n    \t* 见上一页推导\n    \n    * 如果b[i] == b[k]\n    \t* 即各自指针往后移动一位的元素相等\n    \t* 则 `next[i]=k `\n        \t* 0~k 匹配 i-k ~i\n            \n            \n    * 如果b[i] ！= b[k]\n    \t* 那么就找0~i-1中\n        \t* 次长的后缀\n        \n        * 这个次长后缀对应长度的模式串前缀 的下一个字符y，如果y等于b[i]\n        \t* 那么 `next[i] = y`\n            \n        * 这个[次长后缀]肯定是0~i-1[最长后缀]的后缀\n        \t* 所以问题变成：求这个最长后缀的最长后缀\n            \n \n        * 次长后缀如果不符合，则继续求次后缀的最长后缀\n        \t* 直到没有符合的后缀了，再次判断 b[i] ！= b[k]\n            * 所以`next[i] = -1`","layout":null},"children":[]}]},{"data":{"id":"by702jjsq8g0","created":1572859248208,"text":"复杂度分析","note":"* 空间复杂度\n\t* 只需要一个额外的next数组，数组的大小跟模式串相同。\n    \t* 所以空间复杂度是O(m)\n        * m表示模式串的长度\n        \n        \n* 时间复杂度\n\t* next数组构建\n    \t* 遍历一次模式串O(m)\n        * 每次遍历时，不断减少k的值\n        \t* k一定小于m\n        * 总体大约O(m)\n        \n    * 使用next数组进行匹配 \n    \t* 遍历主串O(n)\n        * j的减少不会大于n（实际也不会大于m）\n        * 总体O(n)\n        \n    * KMP算法的时间复杂度就是O(m+n)\n        ","layout":null},"children":[]},{"data":{"id":"by7txrwlops0","created":1572943507872,"text":"实现总结","note":"1. next数组的构建\n\t1. for循环处理每一个模式串的前缀子串\n    \t* 每结束一次循环，就确定当前的前缀子串的最长后缀子串\n    \n    2. 每个前缀子串b[0,i]，依赖前一个前缀子串b[0,i-1]来计算\n    \t* 下一元素相同\n        \t* b[i] == b[j+1]\n            * j+1 就是所求\n            \n        * 不相同，则next[j]中求次长的后缀\n        \t* j = next[j]\n            \n        * while循环中继续判断第二点\n        \t* 直至相同，或者没有匹配的后缀\n            \n            \n            \n\n2. 主串中查找\n\t1. for循环中是遍历主串\n    \t* 每次循环主串指针右移一位\n        * 模式串要么对不上\n        \t* j变0 ，与下一次循环的i再比较\n        * 要么b[i] == b[j]\n        \n    2. while循环中\n    \t* 每次对不上先获取已经匹配的前缀的后缀\n        \t* j = next[j-1] +1\n            \n            \n* 总结：两个步骤的逻辑都是差不多的\n\t* j有时候对应的是坏字符，有时候对应的是坏字符前一个\n    \t* 跟j的应用有关（数组值-1表示不存在最长后缀）\n            \n            \n ","layout":null},"children":[]}]},{"data":{"id":"by7xk8y9wf40","created":1572953732320,"text":"Trie树","note":"* Trie树\n\t* 也叫“字典树”\n    \t* 一种专门处理字符串匹配的数据结构\n        * 用来解决在一组字符串集合中快速查找某个字符串的问题\n        \n    * 利用字符串之间的公共前缀，将重复的前缀合并在一起\n    \t* 根节点不包含任何信息。每个节点表示一个字符串中的字符\n        \n    * Trie树的节点\n    \t* 可以使用一个标记，表示到当前节点就是一个完整字符串\n        \n        * 该节点不一定是叶子节点\n        \n        \n        \n* Trie树结构\n\t* Trie树是一棵多叉树\t\n    \t* 例如使用数组存储子节点的指针集合\n        \t* 数组里面继续嵌套数组\n            \n        * 如字符串只有小写字母\n        \t* 则数组大小就是26\n\n\n\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"by8h1bxz68w0","created":1573008672112,"text":"实现与复杂度分析","note":"* 实现\n\t* 遍历需要插入的字符\n    \t* 对应节点存在，则下一个字符\n        * 不存在，创建节点\n        \n     * 查找，遍历字符进行查找\n     \n     \n* 时间复杂度\n\t* 构建Trie树\n    \t* O(n)\n        * n是所有字符串的长度和\n     \n    * 查找字符串\n    \t* O(k)\n        * k表示要查找的字符串的长度\n        \n\n\n* 空间复杂度\n\t* 如使用数组\n    \t* 一个节点即使只有少量子节点\n        \t* 也需要26个长度的数组\n            * 字符集不是小写英文的则更大\n    \n    * Trie树的本质\n    \t* 是避免重复存储一组字符串的相同前缀子串\n        \t* 但是数组的方式反而导致消耗更多内存\n            * 在重复的前缀并不多的情况下，Trie树不但不能节省内存，还有可能会浪费更多的内存。\n        \n        * 将数组换成其他，会节省更多空间，但是会损失一定性能\n        \t* 如有序数组\n            * 每次二分查找获取节点\n            * 插入也要维护有序性","layout":null},"children":[{"data":{"id":"by8he3dufvk0","created":1573009672214,"text":"缩点优化","note":"* 缩点优化\n\t* 只有一个子节点的节点\n    \t* 而且此节点不是一个串的结束节点\n        * 可以将此节点与子节点合并。这样可以节省空间\n        \t* 此处不一定就是只有一个串，也有可能是多个串，中间都共同拥有一个相同节点\n            * 这几个节点可以合并\n            \n        * 缺点：增加了编码难度","layout":null},"children":[]}]},{"data":{"id":"by8hel67uu80","created":1573009710935,"text":"使用场景","note":"* Trie树中\n\t* 字符串中包含的字符集不能太大\n    \n    * 要求字符串的前缀重合比较多\n    \t* 不然空间消耗会变大很多\n     \n    * Trie树中用到了指针\n    \t* 对缓存并不友好，性能上会打个折扣\n        \n    * 要自己从零开始实现一个Trie树\n    \t* 代价大\n        \n        \n* 因此\n\t* 精确查找字符串\n    \t* 更倾向于用散列表或者红黑树\n        * 这两种不需要自己实现\n    \n    \n* Trie树\n\t* 适合查找[前缀匹配]的字符串\n    \t* 例如输入一定字符，就把符合这个前缀的字符展示出来","layout":null},"children":[]}]},{"data":{"id":"by8mrqcltc00","created":1573024846489,"text":"AC自动机","note":"* 单模式串匹配算法\n\t* BF算法、RK算法、BM算法、KMP算法\n    \n* 多模式串匹配算法\n\t* Trie树、AC自动机\n    * 即多个模式串和一个主串之间做匹配\n    \t* 只需要扫描一遍主串，就能在主串中一次性查找多个模式串是否存在\n        \n        * 从而大大提高匹配效率\n        \n       \n* AC自动机 （Aho-Corasick算法）\n\t* 在Trie树上，加上了类似KMP的next数组的机制\n    \n \n","expandState":"collapse","layout":null},"children":[{"data":{"id":"by8mz5xjofc0","created":1573025428956,"text":"AC自动机构建","note":"1. 将多个模式串构建成Trie树\n\n2. 在Trie树上构建失败指针\n\t* 失败指针\n    \t* 下一个节点如果不匹配了。\n        * 那么当前字符的最长可匹配后后缀的尾部字符，就是失败指针指向的节点\n        \t* 类似KMP中的好前缀\n    \n    1. 父节点是root\n    \t* 失败指针指向root\n        \t* 因为已经没有子串了\n            \n        * 注：root的fail是null，这是while循环的结束条件\n            \n    2. 其他节点，例如父节点p，当前pc\n    \t* 看父节点的失败指针指向谁\n        \n        * 如果父节点的失败指针q的下一位（数组中pc对应的位置）\n        \t* 存在，则pc失败指针就指向q对应pc位置的子节点\n            * q可能有很多子节点，就看对应位置即可\n            \n        * q没有对应子节点，则继续\n        \t* q = q.fail\n            * 即当前不匹配，找次长的后缀\n            \n    3. 求全部节点的失败指针\n    \t* 可以按照广度优先进行处理\n        ","layout":null},"children":[]},{"data":{"id":"by8rw020qo00","created":1573039286623,"text":"与主串匹配","note":"* 将主串遍历\n\t* p开始是root\n    \t* p表示已经处理的节点\n        * 主串中当前需要遍历的节点跟p的子节点比较\n        \t* 比较Trie树上没有匹配的子节点的话\n            * 则继续看失败指针的下一节点是否符合\n            * 直至指针指向root\n    \n    * 下一次循环开启时，p要么是root，要么已经在Trie树上匹配上了\n    \t* 准备与下一节点进行匹配\n        \n        \n* 注：\n\t* 只有root的fail是null，其他的fail可能是其他节点，或者是root\n    \t* fail是root表示没有后缀字符","layout":null},"children":[]},{"data":{"id":"by9a801ky3s0","created":1573091006945,"text":"时间复杂度","note":"* 时间复杂度\n\t* Trie树构建的时间复杂度是\n    \t* O(m * len)\n    \t* len 表示敏感词的平均长度\n        * m 表示敏感词的个数\n    \n    * 构建失败指针\n    \t* 每个节点构建失败指针的时间复杂度是O(len)\n        \t* 最耗时的时候，就是失败指针每次减1，所以最高不超过len\n            \n        * 整个失败指针的构建过程就是O(k * len)\n        \t* k为Trie树节点数\n        \n    * AC自动机做匹配的时间复杂度\n    \t* 以上的构建过程都是预先处理好的，主要看匹配的复杂度\n        \n \n* 匹配时间复杂度\n\t* for循环中嵌套while\n    \t* while里面就是继续寻找失败指针\n        * 这部分为O(len)\n        * len为敏感词平均长度，也是Trie树的高度\n        \n    * 总复杂度：O(n * len)\n    \t* len不会很大，近似O(n)\n        \n        \n\n    \t","expandState":"expand","layout":null},"children":[{"data":{"id":"by9alxkdq9c0","created":1573092098649,"text":"对比Trie树","note":"     \n* 注意失败指针是有可能指向当前字符串的上一节点\n    \n\n* 对比单纯的Trie树上做多字符串匹配\n\t* 判断当前起点的字符串不匹配任何模式串后\n    \t* 即到最后也没有遇到isEndingChar = true的\n    \n    * 单纯Trie树此时是需要从上次开头的下一个字符重新开始\n    \t* 此字符可能已经匹配过\n        * 相当于每一个字符为起点匹配一次trie树\n        * O(n * len)\n    \n    \n\t* AC自动机如果每次指针都指向上一层\n    \t* 也是O(n * len)\n        \t* 见前文推导\n        * 即不一定指向自己的前一节点才算是。只要每次只减一层的都算(猜测)\n        * （猜测）即使这样还是会比纯Trie树快，因为一次len的时间复杂度，是解决了当前已匹配的所有字符\n        \t* 但是总时间复杂度并不能体现，因为并不知道能匹配多长的字符\n\n        \n\n* 失效指针可能大部分情况下都指向root节点\n    * 所以绝大部分情况下\n    \t* 在AC自动机上做匹配的效率要远高于上面的比较宽泛的时间复杂度\n        \n    * 只有在极端情况下\n    \t* AC自动机的性能才会退化的跟Trie树一样","layout":null},"children":[]}]}]}]},{"data":{"id":"by9d11e6ia80","created":1573098924664,"text":"算法思想","expandState":"collapse","layout":null},"children":[{"data":{"id":"by9d1egin0g0","created":1573098953103,"text":"贪心算法","note":"* 贪心算法应用场景\n\n\t1. 针对一组数据，我们定义了限制值和期望值\n    \t* 希望从中选出几个数据，在满足限制值的情况下，期望值最大\n        \n        * 这类问题首先要联想到贪心算法\n    \n    2. 每次选择当前情况下\n    \t* 在对限制值同等贡献量的情况下\n    \t\t* 对期望值贡献最大的数据。\n        * 这类问题，可以尝试看下是否可以用贪心算法解决\n    \n    \n    3. 对于第二点，有时候贪心算法产生的结果并不是最优的\n    \t* 例如在有权图中搜索最短路径\n        \n        * 假如每次都选附近权重最小的\n        \t* 得到的结果可能不是最优的\n        * 因为当前的选择可能影响后面的选择\n        \t* 走其他的点可能开始权重值大，但是后面的小\n            \n            \n\n* 贪心算法的正确性\n\t* 严格地证明贪心算法的正确性\n    \t* 是非常复杂的，需要涉及比较多的数学推理\n        \n    * 从实践的角度来说，大部分能用贪心算法解决的问题\n    \t* 贪心算法的正确性都是显而易见的\n        * 也不需要严格的数学推导证明。","expandState":"collapse","layout":null},"children":[{"data":{"id":"by9gounimtk0","created":1573109254061,"text":"实战例子","note":"1. 分糖果\n\t* 有m个糖果和n个孩子（m<n）\n    * 这m个糖果的大小分别是s1，s2，s3，……，sm\n    * n个孩子对糖果大小的需求分别是g1，g2，g3，……，gn\n    \n    * 如何分配糖果，能尽可能满足最多数量的孩子\n    \n    * 解决\n    \t* 此处限制值为m,满足孩子的个数就是期望值\n        \n    \t* 每次找出对糖果需求最小的，给他能满足的最小糖果\n        \t* 则可以满足最多的孩子\n            \n\n2. 钱币找零\n\t* 假设我们有1元、2元、5元、10元、20元、50元、100元这些面额的纸币\n    \n    * 它们的张数分别是c1、c2、c5、c10、c20、c50、c100\n    \n    * 现在要用这些钱来支付K元，最少要用多少张纸币\n    \n    * 解决\n    \t* 贡献相同期望值（纸币数目）的情况下，我们希望多贡献点金额\n        * 这样就可以让纸币数更少\n        \n        * 先用面值最大的来支付\n        \t* 如果不够，就继续用更小一点面值的\n            * 以此类推，最后剩下的用1元来补齐","expandState":"expand","layout":null},"children":[{"data":{"id":"by9hfwya57s0","created":1573111374897,"text":"例子2","note":"3. 区间覆盖\n\t* 假设我们有n个区间，区间的起始端点和结束端点分别是[l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]\n    \n    * 从这n个区间中选出一部分区间，这部分区间满足两两不相交\n    \t* 端点相交的情况不算相交\n        * 则最多能选出多少个区间\n        \n        \n    * 类似问题\n    \t* 任务调度、教师排课等\n        \n        \n    * 解决\n    \t* 这n个区间中最左端点是lmin，最右端点是rmax\n        * 选择几个不相交的区间，从左到右将[lmin, rmax]覆盖上\n        \n        * 每次选择的时候，左端点跟前面的已经覆盖的区间不重合的\n        \t* 右端点最小的\n            \n        * 这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间。\n        \t* 这实际上就是一种贪心的选择方法。\n            \n            \n","layout":null},"children":[]}]},{"data":{"id":"by9gpiae2940","created":1573109305510,"text":"霍夫曼编码","note":"* 霍夫曼编码是一种十分有效的编码方法\n\t* 广泛用于数据压缩中\n    * 压缩率通常在20%～90%之间\n    \n    \n* 特点\n\t1. 霍夫曼编码不仅会考察文本中有多少个不同字符\n    \t* 还会考察每个字符出现的频率\n        * 根据频率的不同，选择不同长度的编码\n        \n        \n    2. 霍夫曼编码试图用这种不等长的编码方法，来进一步增加压缩的效率\n    \t* 根据贪心的思想\n        \t* 出现频率比较多的字符，用稍微短一些的编码\n            \n        \t* 出现频率比较少的字符，用长一些的编码\n            \n            \n            \n    3. 霍夫曼编码的这种不等长的编码\n    \t* 解压的时候要防止歧义\n        \t* 等长的就按照长度判断即可\n        * 为了避免解压缩过程中的歧义\n        \n        * 霍夫曼编码要求各个字符的编码之间\n        \t* 不会出现某个编码是另一个编码前缀的情况\n            \n            * 只要一个编码不是另外一个编码的前缀，则已经扫描的数字满足对应某一个字符，则就肯定就是这个字符","expandState":"expand","layout":null},"children":[{"data":{"id":"by9iie5bipc0","created":1573114390167,"text":"背景","note":"* 背景\n\t* 1 Byte = 8 bit\n    \t* Byte : 字节\n        * bit：比特（或叫位）\n        \t* 一个位就代表一个0或1\n            \n    * 数据存储是以字节Byte为单位\n    \t* 数据传输大多是以比特为单位\n        \n        \n* 一个字符占一个字节\n\t* 文件中假如只重复几个字符\n    \t* 例如6个\n        \n    * 可以用010等几个比特来表示这些字符\n    \t* 2^3 = 8，所以只需要3个比特就可以表示8个不同字符\n    \n    * 霍夫曼编码中，实现了更加节省空间的存储方式\n    \t* 例如某些字符如果只用1比特来表示，就更省空间，只有小的用完了再用大的","layout":null},"children":[]},{"data":{"id":"by9jvphtu3k0","created":1573118254712,"text":"字符根据频率对应编码","note":"* 先取出2个最少频率的节点\n\t* 节点附带频率数据\n    * 如a，b\n    \n* 俩节点频率之和也作为一个新节点c\n\t* 将a,b,c三节点组成一个优先级队列\n    \t* 即堆\n        \n        \n* 则c肯定是 a，b的父节点（大顶堆）\n\t* 不断获取下一个节点\n    \t* 并与当前根节点（最大的）组成优先级队列\n        * 两者频率之和为父节点\n        \n        \n* 则最后从根节点开始\n\t* 左边的边记为 0\n    * 右边记为 1\n    \n    * 根节点到叶节点的路径\n    \t* 就是叶节点对应字符的霍夫曼编码\n        \n        \n* 霍夫曼编码\n\t* 例\n    \t* 1\n        * 01\n        * 001\n        * ...\n        * 0000001\n        * 0000000\n        \t* 全部是0的就是最底层的","layout":null},"children":[]}]}]},{"data":{"id":"bya4biu2la00","created":1573175916239,"text":"分治算法","note":"* 分治算法（divide and conquer）\n\t* 核心思想：分而治之\n    \n    * 将原问题划分成n个规模较小，并且结构与原问题相似的子问题\n    \t* 递归地解决这些子问题\n        * 然后再合并其结果，就得到原问题的解\n        \n\n* 与递归关系（前文也有记录）\n\t* 分治算法是一种处理问题的思想，递归是一种编程技巧\n    \t* 分治算法一般都比较适合用递归来实现\n        \n        \n        \n        \n* 一台机器过于低效，那我们就把任务拆分到多台机器上来处理。\n\t* 如果拆分之后的小任务之间互不干扰，独立计算\n    \t* 最后再将结果合并\n     \n    * 这就是分治思想\n    \t* 多线程处理同理\n\n        \n \n","expandState":"collapse","layout":null},"children":[{"data":{"id":"bya4hpbqbe00","created":1573176400553,"text":"分治算法具体操作","note":"* 分治算法的递归实现中，每一层递归都会涉及这样三个操作\n\t* 分解\n    \t* 将原问题分解成一系列子问题        \n\t* 解决\n    \t* 递归地求解各个子问题，若子问题足够小，则直接求解\n        \n    * 合并\n    \t* 将子问题的结果合并成原问题\n        \n  \n  \n  \n* 分治算法能解决的问题，一般需要满足下面这几个条件\n\t* 原问题与分解成的小问题具有相同的模式\n\n\t* 原问题分解成的子问题可以独立求解\n    \t* 子问题之间没有相关性\n        * 这一点是分治算法跟动态规划的明显区别\n\n\t* 具有分解终止条件\n    \t* 也就是说，当问题足够小时，可以直接求解；\n\n\t* 可以将子问题合并成原问题\n    \t* 而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了","layout":null},"children":[]},{"data":{"id":"bya6csoan2g0","created":1573181658242,"text":"应用例子","note":"1. 求一组数据的逆序度\n\t* 逐个遍历\n    \t* O(n^2)  \n    * 使用分治算法\n    \t* 在归并排序中，增加逆序度的统计\n    * 在左右两个子行合并的时候\n    \t* 如果是右区间的节点j小，则该节点j对于左区间剩余元素都是一个逆序度\n        \n        * num += (q-i+1)\n            * q 是左区间最右节点\n            * i是当前与右区间比较的节点下标\n            * 该节点都比j小，说明剩余的左区间节点都比j小\n            \n       * 总逆序度等于两个子区间逆序度加上当前合并的逆序度\n            \n        \n        \n* 10G订单排序问题\n\t* 之前的处理方案是桶排序\n    * 将数据遍历，不同金额区间的放不同的文件\n    \t* 则每个文件大小减少，可满足单个文件加载进内存，使用快排等处理\n        \n    * 并且每个文件可以并行处理，提高效率\n    \t* 全部处理后，再合并数据文件\n    * 以上处理就使用了分治算法思想\n    \n    \n* 多机器处理注意事项：\n\t* 数据的存储与计算所在的机器是同一个或者在网络中靠的很近\n    \t* 比如一个局域网内，数据存取速度很快\n        * 否则就会因为数据访问的速度，导致整个处理过程不但不会变快，反而有可能变慢\n    * 即多机器处理，他们之间的传输不能太慢慢","layout":null},"children":[]}]},{"data":{"id":"bya7ojz5nko0","created":1573185400787,"text":"回溯算法","note":"* 回溯算法很多时候都应用在“搜索”这类问题上。\n\t* 不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法\n    \t* 而是在一组可能的解中，搜索满足期望的解。\n        \n \n* 回溯的处理思想\n\t* 有点类似枚举搜索\n    \t* 我们枚举所有的解，找到满足期望的解\n        \n    * 把问题求解的过程分为多个阶段\n    \t* 每个阶段，我们都会面对一个岔路口，我们先随意选一条路走\n        * 当发现这条路走不通的时候（不符合期望的解）\n        \t* 就回退到上一个岔路口，另选一种走法继续走","expandState":"collapse","layout":null},"children":[{"data":{"id":"byaauwrwdxk0","created":1573194362161,"text":"八皇后问题","note":"* 例1：八皇后问题\n\t* 8x8的棋盘，希望往里放8个棋子（皇后）\n    \t* 每个棋子所在的行、列、对角线都不能有另一个棋子\n        \n    * 详情见代码 \n\t\t* 此外还有 0-1背包问题\n\t\t* 正则匹配问题\n        \n\n","layout":null},"children":[]},{"data":{"id":"bybeqoqv9tc0","created":1573306875624,"text":"总结","note":"* 总结\n\t1. 与贪心算法的一大区别\n    \t* 就是当前步骤的选择是不是一定正确\n        * 贪心算法是肯定正确的\n        * 但是回溯则不是\n    \n    * 因此，当前做出选择后（循环中递归）\n    \t* 不能直接break，要全部情况都走一遍\n    \n\n    2. 当前选择是否正确要想清楚\n    \t* 0-1背包中，假设A 100斤，B 90斤，c 50斤 ，背包能装140斤\n        \n        * 选了一个石头A后，后面石头的重量都不能再放进去了\n        \n        * 但是不要A，选了重量更少的B，就可以继续装，所以此处使用贪心算法就不行了\n        \n    * 但是钱币找零问题却是贪心算法\n    \t*  因为钱币的金额是已知的\n        * 每一种都是前面的2倍以上，不会出现这种问题\n        \n        \n    3. 正则匹配这种，匹配上了就可以结束\n    \t* 跟8皇后这种列出全部情况的不一样\n        * 因此要具体问题具体分析\n        \n        * 要提前结束的，就要防止当前递归结束后，上一级方法继续运行递归\n        \t* 每次进来递归的先判断\n            \n        * 不能提前结束的，就不要加break\n        \t* 既防止不正确 \n            * 也防止没有全部列举\n   \t","layout":null},"children":[]}]},{"data":{"id":"bybgby6u5aw0","created":1573311362938,"text":"动态规划","note":"* 把问题分解为多个阶段，每个阶段对应一个决策\n\t* 记录每一个阶段可达的状态集合\n    \t* 去掉重复的\n        \n    * 然后通过当前阶段的状态集合，来推导下一个阶段的状态集合，动态地往前推进。\n    \n    \n* 动态规划中，要记录上一阶段的状态\n\t* 因此是以空间换时间\n    * 有不少是可以只保存 上一次的状态\n    \t* 下一次可在上一次 的基础上做修改\n        \n \n* 动态规划笔记不多，主要是思路及代码\n\t* 详细见代码","expandState":"collapse","layout":null},"children":[{"data":{"id":"bybwuyjbluo0","created":1573357990371,"text":"0-1背包问题","note":"* 使用动态规划解决0-1背包问题\n    * 使用二维数组，值为boolean，表示该重量是否匹配上物品\n    \t* 第一维大小是物品个数\n        * 第二维大小是背包重量上限+1\n        \n    * 第i个一维数组在i-1数组下计算\n    \t* 分别计算装与不装下的重量\n        \n    * 具体实现见代码\n    \n\n* 优点\n\t* 当前物品装、与不装，产生的状态可能重复\n    \t* 例如物品i与i-1重量一样\n        \t* i-1不装，i 装\n        \t* i-1装，i不装\n        * 结论是一样的\n     \n    * 以上的实现中，会把同一个二维数组的值两次变成true\n    \t* 相当于合并了\n        * 下一物品计算时，就没有这个多状态，不会再指数式增长\n        \n     \n     \n* 双十一凑单满减\n\t* 类似本例\n    \t* 取值取得是刚好满足的最小值\n    \n    * 从已选择的金额反推选择了哪些商品\n    \t* 当前总金额 - 当前商品金额\n        \t* 对应数组值为true，则选择\n        * 以此类推\n     \n    * 详情见代码\n        \n        \n","expandState":"collapse","layout":null},"children":[{"data":{"id":"byc0i49zcpc0","created":1573368268572,"text":"效率比较","note":"* 时间复杂度\n\t* O(n* w)\n    \t* n表示物品个数\n        * w表示背包可以承载的总重量\n        \n    * 相比于回溯算法的O(2^n)指数级别\n    \t* 肯定是比指数小很多\n       \n       \n       \n* 空间复杂度\n\t* 二维数组使用的空间复杂度较高\n    \t* 可以使用一维数组代替\n    \n    * 当前行处理完空间就可以释放了\n    \t* 注意此时需从大到小比较\n        \t* 否则重复计算\n            \n    * 使用一维数组，大小为 w+1\n    \t* 即O(w)\n        * w 较大的话，也算是以空间换时间","layout":null},"children":[]}]},{"data":{"id":"byc0s81b2ow0","created":1573369060397,"text":"0-1背包价值问题","note":"* 求一定重量限制条件下的最大价值\n\t* 限制条件不变\n    * 期望条件变了\n    \n    * 原先是合并重量相同的\n    \t* 现在是重量相同的，价值可能不同\n        * 因此，对于重量相同的，选择价值大的即可\n    \n    * 可以同样以二维数组实现\n    \t* 数组值是价值\n        \t* 详细见代码\n        * 时间复杂度O(n* w)\n        \t* 空间复杂度也是\n        \n        \n* 注\n\t1. 二维数组要初始化\n    \t* 默认的0区别不了当前情况是存在的，只是价值为0，还是当前情况不存在\n        \n    2. 最后一个物品放进去后\n    \t* 要遍历获取最大价值\n        * 最右的不一定最大\n        \n        \n    3. 此例也可以使用一维数组实现\n    \t* 见代码","layout":null},"children":[]},{"data":{"id":"byc4f6j1dgg0","created":1573379322821,"text":"动态规划理论","note":"* 动态规划与其他算法的区别\n\t* 分治算法\n    \t* 分治要求分割成的子问题，[不能]有重复子问题\n        \n        * 而动态规划正好相反\n        \t* 动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题\n            \n    * 贪心算法\n    \t* 实际上是动态规划算法的一种特殊情况\n        * 解决问题起来更加高效，代码实现也更加简洁。\n        \t* 不过，它可以解决的问题也更加有限\n        * 它能解决的问题需要满足三个条件\n        \t* 最优子结构\n            * 无后效性\n            * 贪心选择性（这里我们不怎么强调重复子问题）\n            \n        * 贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。\n        \n    * 回溯算法\n    \t* 基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。\n        * 不过，回溯算法的时间复杂度非常高，是指数级别的，只能用来解决小规模数据的问题。\n        \n        * 但是，并不是所有问题，都可以用动态规划来解决。\n        \t* 能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。","expandState":"collapse","layout":null},"children":[{"data":{"id":"byc4ulg8wl40","created":1573380530766,"text":"一个模型三个特征","note":"* 一个模型指动态规划适合解决的问题的模型\n        \n    * 多阶段决策最优解模型\n    \t* 一般用动态规划来解决最优问题\n        \t* 解决问题的过程，需要经历多个决策阶段\n            * 每个决策阶段都对应着一组状态\n            \n        * 寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值\n        \n        \n* 三个特征\n\t* 最优子结构\n    \t* 问题的最优解 包含 子问题的最优解\n        \t* 即我们可以通过子问题的最优解，推导出问题的最优解。\n            \n        * 也就是： 后面阶段的状态可以通过 前面阶段 的状态推导出来\n        \n        \n    * 无后效性\n    \t1. 在推导后面阶段的状态的时候，我们只关心前面阶段的状态值\n        \t* 不关心这个状态是怎么一步一步推导出来的\n        2. 某阶段状态一旦确定，就不受之后阶段的决策影响\n            \n        * 满足此模型的一般也满足此要求\n            \n    * 重复子问题\n    \t* 不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态\n        \n   \t","layout":null},"children":[]},{"data":{"id":"byc4v5f2gpk0","created":1573380574230,"text":"解题思路","note":"1. 状态转移表法\n\t* 状态表一般都是二维的\n    \t* 可以把它想象成二维数组\n        * 包含行、列、数组值三个变量\n        \n    * 据决策的先后过程，从前往后\n    \t* 根据递推关系，分阶段填充状态表中的每个状态\n        \n    * 可以使用穷举的方式，使用回溯暴力搜索\n    \t* 然后画出递归树\n        * 从递归树中寻找规律\n        \t* 寻找相同子问题\n            * 看相同子问题是如何产生的\n        * 合并子问题，翻译成代码\n        \n        \n        \n2. 状态转移方程法\n\t* 类似于递归的思路\n    \t* 即当前问题如何通过子问题推导解决当前问题\n        * 递归公式，也就是所谓的状态转移方程\n    \n    * 实现可以直接使用递归\n    \t* 递归 + 重复问题先记录，不重复处理\n        * 或者用正常的动态规划代码（循环）处理\n    \n","layout":null},"children":[{"data":{"id":"byc80dmf91s0","created":1573389447240,"text":"总结","note":"* 状态转移表与状态转移方程式\n\t* 只是思路不一样\n    \t* 代码是一样的\n        \n        * 两种都可以使用递归+备忘录，或者使用动态规划自己的方式\n        \t* （也就是循环中，当前状态怎么根据前面状态推导）\n        \n        \n* 思路区别\n    * 状态转移表法\n    \t1. 回溯算法实现\n        2. 定义状态\n        3. 画递归树\n        4. 找重复子问题\n        5. 画状态转移表\n        6. 根据递推关系填表\n        7. 将填表过程翻译成代码\n    \n    * 状态转移方程法\n    \t1. 找最优子结构\n        2. 写状态转移方程\n        3. 将状态转移方程翻译成代码\n        \n        \n        \n    * 不是每个问题都同时适合这两种解题思路\n    \t* 有的问题可能用第一种思路更清晰\n        * 而有的问题可能用第二种思路更清晰\n        * 所以，你要结合具体的题目来看，到底选择用哪种解题思路\n        \n    * 例如之前的0-1背包问题\n    \t* 使用状态转移表更清晰\n        \n        * 而二位数组的最短路径问题，则是转移方程更清晰（自我感觉）\n        \n\n","layout":null},"children":[]}]}]},{"data":{"id":"bycato1emrs0","created":1573397384694,"text":"拼写纠错例子","note":"* 编辑距离（Edit Distance）\n\t* 量化两个字符串的相似度\n    * 将一个字符串转化成另一个字符串\n    \t* 需要的最少编辑操作次数\n        \n        \n* 莱文斯坦距离（Levenshtein distance）\n\t* 允许增加、删除、替换字符\n    \t* 这三个编辑操作\n    * 表示两个字符串差异的大小\n\n\n\n\n* 最长公共子串长度（Longest common substring length）\n\t* 只允许增加、删除字符\n    \t* 这两个编辑操作。\n        \n    * 表示两个字符串相似程度的大小\n    \t* 注意是编辑次数，此处公共子串不一定是连续的\n        \t* 相同字符个数\n            \n\n* 详情见代码\n\t* 下一个对比，基于\n    \t* i-1，j\n        * j-1,i\n        * i-1,j-1\n","expandState":"collapse","layout":null},"children":[{"data":{"id":"byejxnk801s0","created":1573626207014,"text":"实际应用","note":"* 实际应用\n\t* 输入一个拼写错误的单词时\n    \t* 拿这个单词跟词库中的单词一一进行比较，计算编辑距离\n        \n        * 将编辑距离最小的单词，作为纠正之后的单词，提示给用户。\n  \n  \n* 缺点\n\t* 单纯对比编辑距离，效果不一定好\n    * 词库中的数据量可能很大，全部对比性能不高\n    \n    \n* 优化\n\t1. 取出编辑距离最小的TOP 10\n    \t* 根据其他参数，决策选择哪个单词作为拼写纠错单词\n        \t* 比如使用搜索热门程度\n        \n    2. 多种编辑距离，分别求出TOP10\n    \t* 如莱文斯坦距离、最长公共子串\n        \t* 取交集，再作其他优化\n         \n    3. 通过统计(所有)用户的搜索日志\n    \t* 得到最常被拼错的单词列表\n        \t* 以及对应的拼写正确的单词\n        * 拼写纠错时首先在这个最长被拼错单词列表中查找。\n        \t* 如果一旦找到，直接返回对应的正确的单词\n            \n    4. 引入个性化因素。针对每个用户，维护这个用户特有的搜索喜好\n    \t* 即常用的搜索关键词\n        \n    \t* 当用户输入错误的单词的时候，我们首先在这个用户常用的搜索关键词中，计算编辑距离\n        \t* 查找编辑距离最小的单词。\n   \t","layout":null},"children":[]},{"data":{"id":"byek7hfcl8g0","created":1573626977301,"text":"性能优化","note":"* 注 TPS、QBS\n    * TPS 是每秒事务数\n    \t* 1TPS = N QBS\n        \n        \n* 分治优化思路\n\t* TPS不高（每秒能处理的请求不多）\n    \t* 可以部署多台机器， 每台机器运行一个独立的纠错功能\n        \n        * 当有一个纠错请求的时候，通过负载均衡，分配到其中一台机器\n        \t* 来计算编辑距离，得到纠错单词\n            \n    * 纠错系统的响应时间太长\n    \t* 将纠错的词库，分割到很多台机器。\n        * 当有纠错请求时，将这个拼写错误的单词，同时发送到这多台机器\n        \t* 让多台机器并行处理，分别得到编辑距离最小的单词\n            * 然后再比对合并，最终决定出一个最优的纠错单词。","layout":null},"children":[]}]}]}]},{"data":{"id":"byjh5mvfuwo0","created":1574126168883,"text":"其他","expandState":"collapse"},"children":[{"data":{"id":"byhbycglq9k0","created":1573908372206,"text":"位图","note":"* 位图（BitMap）\n\t* 位即bit,一字节 = 8位\n    \n    * 位图存储结构，就是使用到位来存储数据\n    \t* 对某些只需存储2种情况的，更加节省空间\n        \n        \n        \n* Java中的BitSet类就是一个位图\n\t* Redis也提供了BitMap位图类\n    \n    * Google的Guava工具包提供了BloomFilter布隆过滤器的实现","expandState":"collapse","layout":null},"children":[{"data":{"id":"byhczo5swxc0","created":1573911297148,"text":"位图应用场景","note":"* 有1千万个整数，整数的范围在1到1亿之间。\n\t* 如何快速查找某个整数是否在这1千万个整数中？\n    \n    \n    \n* 解决\n\t* 最快的方法，当然是存对应的数组\n    \t* 直接根据数组下标定位数据\n        * 数组中存储true 或 flase\n        \t* 则可以知道数据是否存在\n            \n    * 但是，一亿的范围，int类型每个占4个字节（32位）\n    \t* 2^32\n    \t* 耗费太多空间\n        * 而只存储是否存在，可以使用占用空间更小的单位：位\n        \n        \n        \n    * 实际使用时\n    \t* 可以先对数据除以8，得到数组下标\n        * 余数作为对应bit的位置\n        \n        * 写入时使用或运算\n        * bytes[byteIndex] |= (1 << bitIndex);\n        \t* 1 左移 0~7位\n            \n        * 同理取数据时使用与运算获取\n        \n   * 同时，定位数组以及位运算效率都很快","layout":null},"children":[]},{"data":{"id":"byhdjj236t40","created":1573912853323,"text":"布隆过滤器","note":"* 位图应用例子中\n\t* 如果数据范围更大\n    \t* 如1~10亿\n        * 数据数量不变\n        \n    * 还是每个数据都对应一个二进制位的话\n    \t* 则反而浪费更多空间\n \n \n* 布隆过滤器\n\t* 对位图的一种改进\n    \n    * 如刚刚的例子中，数据个数与范围差别达到1：100\n    \t* 此时还是使用1~ 一亿的位图\n        \n        * 对数据进行hash函数计算，对应到对应的二进制位上\n        \n        * 例如取模\n    \n    \n* 一般的取模，hash冲突的概率大\n    * 布隆过滤器的处理方法是\n    \t* 使用多个hash函数同一个数字进行求哈希值，如k个\n        * 则会得到对应k个哈希值\n        \n    * 把这几个hash值对应的二进制位都设为true\n    \t* 比较时，只有这几位二进制位都为true，才证名这个数存在\n     \n    * 不同数值多个hash函数得到hash值相同得概率就变得非常低\n    \n    * 但是，有可能一个数不存在，但其他数值对应hash值跟他相同\n    \t* 导致[误判]该数据[存在]\n        * 即一个数据经过布隆过滤器判断为存在\n        \t* 有可能实际并不存在","expandState":"expand","layout":null},"children":[{"data":{"id":"byhdu1p8h9c0","created":1573913677546,"text":"应用特点","note":"* 布隆过滤器的应用\n\t* 非常适合不需要100%准确的、允许存在小概率误判的大规模判重场景。\n    \n    \t* 如大量网址的爬虫去重等问题\n        \n        * 还有统计一个大型网站的每天的UV数\n        \t* 也就是每天有多少用户访问了网站，我们就可以使用布隆过滤器，对重复访问的用户，进行去重\n    \n    \n    * 另外： 通过调整哈希函数的个数、位图大小跟要存储数字的个数之间的比例\n    \t* 可以将这种误判的概率降到非常低\n    \n    * 当数据越来越多\n    \t* 位图中不是true的位置就越来越少了，误判率就越来越高\n        \n        * 所以对于无法事先知道要判重的数据个数的情况\n        \t* 需要支持自动扩容功能\n            \n    * 布隆过滤器中，数据个数与位图大小的比例超过某个阈值的时候\n        * 重新申请一个新的位图\n        \n        * 后面来的新数据，会被放置到新的位图中\n        \n        * 判断某个数据是否在布隆过滤器中已经存在，我们就需要查看多个位图\n        \t* 相应的执行效率就降低了一些\n        \n","layout":null},"children":[]},{"data":{"id":"byhf3liikig0","created":1573917247063,"text":"应用实例","note":"* 10亿网址去重\n\t* 直接存储平均64字节的字符串\n    \t* 如散列表\n        \t* 数据约64G\n        \t* 加上数组装载因子\n            * 链表的指针\n        * 加起来约100G\n        \n        * 当然这100G其实也可以使用分治的思想，分别存放在不同机器\n        \n        \n        * 另外，从时间复杂度上说，虽然散列表是O(1)\n        \t* 但是系数其实还是有的，所以还有优化的空间\n            * 散列表中的链表内存不连续，不能很好地利用到CPU高速缓存\n            * 链表中的url，对比需要逐个字符遍历对比\n            \n        * 还有，读取多个url，这个操作涉及很多内存数据的读取，所以是[内存密集型]\n            \n            \n    * 布隆过滤器：例如使用10倍的位图\n    \t* 即100亿的二进制位，约1.2G\n        \t* 1.2 * 8 =10\n        * 内存上节省更多空间\n        \n        * 时间上，对URl进行多次hash计算\n        \t* 理论上讲这组操作是CPU密集型的（网址只需从内存读取一次）\n            \n        * CPU计算可能是要比内存访问更快速的，所以，理论上讲，布隆过滤器的判重方式，更加快速。\n            ","layout":null},"children":[]}]},{"data":{"id":"byhqig4th3c0","created":1573949443022,"text":"对比数据库存储","note":"* 学习mysql中也有这个字符串的例子\n\t* 总的来说，需要永久存储的用数据库\n    * 不需要存储，需要快速计算的使用内存，用布隆过滤器。","layout":null},"children":[]}]},{"data":{"id":"byhuljmkdkw0","created":1573960970158,"text":"概率统计","expandState":"collapse","layout":null},"children":[{"data":{"id":"byhumswgiqw0","created":1573961068711,"text":"垃圾短信过滤","note":"* 在实际工程中\n\t* 以结合三种不同的过滤方式的结果，对同一个短信处理\n    \t* 如果三者都表明这个短信是垃圾短信，我们才把它当作垃圾短信拦截过滤\n        \t* 这样就会更精准\n    \n    * 降低了如布隆过滤器中出现误判\n    \t* 导致错误拦截的概率","layout":null},"children":[{"data":{"id":"byhuzhu3q9s0","created":1573962063358,"text":"基于黑名单","note":"1. 基于黑名单的过滤器\n\n\t* 维护一个骚扰电话号码和垃圾短信发送号码的黑名单\n    \t* 黑名单的获取可以是用户标记等\n        * 标记超过一定阈值，就定义为骚扰短信、电话，加入黑名单\n        \n        \n        \n    * 量不大的话，可以使用散列表、二叉树等存储\n    \t* 量大的话，为了拦截功能，消耗上百M的内存，就有点多了\n        \n        \n    * 量大可以使用布隆过滤器\n        * 注意布隆过滤器中，一般使用10倍的位图，而一字节是8位\n            \n        * 所以相当于原数据量的1.25倍（原数据一个数据可能不止一字节，因此还是位图空间小）\n        \n        * 布隆过滤器会有判错的概率\n            \n    \n    \n    \n    * 时间换空间\n    \t* 黑名单存于服务器\n        * 不需要占用手机内存\n        \t* 不过有利就有弊。网络通信是比较慢的，网络延迟会导致处理速度降低\n            * 而且，只有在联网的情况下才能正常工作","layout":null},"children":[]},{"data":{"id":"byhuzpc9hjk0","created":1573962079694,"text":"基于规则的过滤器","note":"* 黑名单的过滤器\n\t* 如果某个垃圾短信发送者的号码并不在黑名单中，就没办法拦截了\n    \n* 可以通过短信内容等规则\n\t* 如\n    \t* 包含特殊单词（或词语），比如一些非法、淫秽、反动词语等\n        \n        * 发送号码是群发号码\n        \n        * 包含回拨的联系方式（因为群发短信的号码一般都是无法回拨）\n        \n        * 短信格式花哨、内容很长，比如包含各种表情、图片、网页链接等\n        \n        * 符合已知垃圾短信的模板\n        \t* 垃圾短信一般都是重复群发\n            * 对于已经判定为垃圾短信的短信，可以抽象成模板，将获取到的短信与模板匹配\n            * 一旦匹配，我们就可以判定为垃圾短信\n            \n    * 为降低误判的概率\n    \t* 可以综合多条规则进行判断\n        * 如满足2条以上才会被判定为垃圾短信\n        \t* 或者每条规则对应一个不同的得分，满足哪条规则，我们就累加对应的分数\n            * 某条短信的总得分超过某个阈值，才会被判定为垃圾短信\n","expandState":"expand","layout":null},"children":[{"data":{"id":"byhvd791nps0","created":1573963137416,"text":"特殊单词的定义","note":"* 人自己思考哪些单词属于特殊单词，那势必有比较大的主观性，容易漏掉某\n    * 可以基于概率统计\n    \n    1. 需要有大量的样本数据\n    \t* 且已经标记这些样本数据属于垃圾短信还是非垃圾短信\n        \n    2. 数据分词处理\n    \t* 去掉stop words等，得到多个不同单词\n        \n    3. 针对每个单词\n    \t* 统计在垃圾短信和非垃圾短信次数\n        * 转化为出现概率\n        \n    4. 如果某个单词出现在垃圾短信中的概率，远大于出现在非垃圾短信中的概率\n    \t* 那我们就把这个单词作为特殊单词，用来过滤垃圾短信。","layout":null},"children":[]}]},{"data":{"id":"byhvg9prcig0","created":1573963377872,"text":"基于概率统计的过滤器","note":"* 基于规则的过滤\n\t* 规则受人的思维方式局限，规则未免太过简单\n    \t* 而且垃圾短信发送者可能会针对规则，精心设计短信，绕过这些规则的拦截。\n    \n\n\n* 基于概率\n\t* 将短信分词，分割成n个单词，这n个单词是一个特征项\n    \t* 特征项全权代表这个短信\n        \n    * 判定一个短信是否是垃圾短信这样一个问题\n    \t* 就变成了，判定[同时包含这几个单词]的短信是否是垃圾短信。 \n        \n    * 这里并不像基于规则的过滤器那样，非黑即白\n    \t* 而是使用概率，来表征一个短信是垃圾短信的可信程度\n        \n    * 公式：P(短信是垃圾短信|w1.w2...wn 同时出现在一条短信)\n    \t* 即在w1，w2...wn这几个单词同时出现的前提下，短信是垃圾短信的概率\n        \n        \n* 直接根据样本求该概率方式\n\t* P(短信是垃圾短信|w1.w2...wn 同时出现在一条短信)\n    \t* 就要获取同时包含这些单词的短信的个数，以及他们其中是垃圾短信的个数\n    \n    * 但是即使样本的数量再大，毕竟也是有限的\n    \t* 样本中不会有太多同时包含这n个单词的短信的\n        * 甚至样本中根本不存在这样的短信\n        \n    * 没有样本，也就无法计算概率","expandState":"expand","layout":null},"children":[{"data":{"id":"byi1bm6egao0","created":1573979939839,"text":"根据朴素贝叶斯公式求概率","note":"\n* 根据朴素贝叶斯推断\n\t* 则P(短信是垃圾短信|w1.w2...wn 同时出现在一条短信)\n    \t* 需要先获得以下三个概率\n    \n    1. P(w1,w2...wn同时出现 | 短信是垃圾短信)\n    \t*  此概率跟之前一样，无法根据样本获取（转化方式见：概率转化）\n    \n    2. P(w1,w2...wn同时出现)\n    \t* 这个由于样本问题，还是不好统计，但是这个不是一定要计算出来\n        \t* 转化为比例\n    \n    3. P(短信是垃圾短信)\n    \t* 这个概率容易得到，就是垃圾短信数/总短信数\n    \n\n\n\n","layout":null},"children":[]},{"data":{"id":"byi21wa2e8o0","created":1573981999296,"text":"概率转化","note":"* 概率规则\n\t* P（A* B） = P(A) * P(B)\n    \t* 如果事件A和事件B是独立事件，两者的发生没有相关性\n        \n        * 那两个同时发生的概率P(A* B)= P(A)* P(B)\n        \n    * 因此P(w1,w2...wn同时出现 | 短信是垃圾短信) 分解为\n`P (w1出现在短信中|短信是垃圾短信)* P (w2出现在短信中|短信是垃圾短信)*\n...\n*P (wn出现在短信中|短信是垃圾短信)`\n\n\n*  短信是垃圾短信时，某个单词出现的概率\n\t* 是可以通过样本统计的","layout":null},"children":[]},{"data":{"id":"byi23bpnowg0","created":1573982111255,"text":"比例转化","note":"* P(w1,w2...wn同时出现)的概率不好根据样本获得\n\t* 此时，可以再求w1,w2...wn同时出现时\n    \t* 不是垃圾短信的概率\n     \n    * 计算方式与之前相同，也是利用朴素贝叶斯公式\n    \t* P(w1,w2...wn同时出现)都是作为分母\n    \n    * 此时两概率相除，获取比例\n    \t* 则分母可以抵消，获得了在这几个单词同时出现的前提下\n        \t* 是垃圾短信与不是垃圾短信的概率比例\n            \n            \n            \n     * 可以定义，比例是多倍，如1：10才确定该短信是垃圾短信","layout":null},"children":[]}]}]},{"data":{"id":"byi6utvbxvs0","created":1573995551052,"text":"朴素贝叶斯算法","note":"* 基于朴素贝叶斯算法\n\t* P(A|B) = ( P(B|A) * P(A) ) /P(B)\n    \t* 其中P(A|B)表示：在事件B发生的前提下，事件A发生的概率\n        \n        * P(A) 表示事件A发生的概率","layout":null},"children":[]}]},{"data":{"id":"byi6iuycuq00","created":1573994613042,"text":"向量空间","note":null,"expandState":"collapse","layout":null},"children":[{"data":{"id":"byi6rxg9rk00","created":1573995323756,"text":"音乐推荐系统","note":"* 实现音乐推荐系统\n\t* 基于相似用户做推荐\n    \n    * 基于相似歌曲做推荐\n    \n    \n1. 相似用户\n\t* 即口味偏好相似\n    \n    * 把跟你听[类似歌曲]的人，看做口味相似的用户\n    \t* 遍历所有用户，对比每个用户跟你[共同喜爱]的歌曲个数\n        \n        * 设置阈值,共同喜爱的歌曲个数超过这个阈值\n        \n        * 就把这个用户看作跟你口味相似的用户，把这个用户喜爱但你还没听过的歌曲，推荐给你。\n    \n    * 喜爱程度统计\n    \t* 通过行为定义喜爱程度\n        * 如单曲循环5分，分享4分，收藏3分，搜索2分，听完1分，没听过0分，跳过-1分等\n        \n    * 用户对于每首歌的喜爱程度不一样\n    \t* 对应得分也不一样\n        * 但是对不同歌的喜爱，不能简单的将数值加起来\n        \t* 因为是不同的事物\n            * 因此此处使用欧几里得距离来表示相似程度\n            \n    * 欧几里得距离越小\n    \t* 高维空间中靠得最近，即口味最相似","layout":null},"children":[{"data":{"id":"byi7fjd0r0o0","created":1573997173825,"text":"新用户","note":"* 对于新用户\n\t* 还没有收集到足够多的行为数据\n    \t* 只喜欢几首歌等\n    * 则可以基于相似歌曲做推荐\n    \n    \n* 相似度分析\n\t1. 对歌曲定义一些特征项\n    \t* (`不理想的处理方案`)\n    \n    \t* 如是伤感的还是愉快的，是摇滚还是民谣，是柔和的还是高亢的等等\n        \t* 每个特征项打一个分数\n            \n        * 歌曲就都对应一个特征项向量\n        \n        * 基于这个特征项向量，来计算两个歌曲之间的欧几里得距离。\n        \t* 欧几里得距离越小，表示两个歌曲的相似程度越大\n     \n    * 但是，前提是我们能够找到足够多，并且能够全面代表歌曲特点的特征项\n    \t* 此外，人工给每首歌标注每个特征项的得分。\n        \t* 对于收录了海量歌曲的音乐App来说，这显然是一个非常大的工程。\n        * 人工标注有很大的主观性，也会影响到推荐的准确性。\n        \n    ","layout":null},"children":[]},{"data":{"id":"byi826e1fl40","created":1573998947964,"text":"新用户2","note":"2. 对于仅有的几首歌，以用户的打分作为向量\n\t* 每个用户虽然喜爱不一样，但是他们对于相似歌曲的评分是类似的\n    \n    * 与基于用户的推荐的主次互换\n    \t* 一首歌的向量由多个用户打分组成\n        \t* 与其他歌曲计算距离时，一一对应的就是同一个用户（用户是维度）\n            \n    * 根据歌曲的向量表示\n        * 通过计算向量之间的欧几里得距离获取相似的歌曲\n            \n    * 当前用户没有其他歌曲的打分\n    \t* 但是此处是用其他用户的即可\n        \n","layout":null},"children":[]}]},{"data":{"id":"byi6tsv5j1c0","created":1573995470500,"text":"欧几里得距离","note":"* 欧几里得距离（Euclidean distance）\n\t* 用来计算两个向量之间的距离\n\n\n* 向量 vector\n\n\t* 一维使用1，3，5等表示位置\n    * 二维用(1,3) (2,4) (5,7)表示位置\n    * 三维：(2,4,2)(1,3,5)(4,1,3)\n    * ....\n    * k维\n    \t* (x1,x2.x3...xn)\n        \n    * 这些表示方法就是向量\n    \n    \n* 向量之间的距离\n\t* 二维\n    \t* (x1,x2) 与 (y1,y2)距离\n        \n    \t* √￣(x1-y1)^2 +(x2-y2)^2\n        \n        * 此处根号是一直包含整个公式的\n        \n    * 三维\n    \t* (x1,x2，x3)与(y1,y2,y3)\n        \n    \t* √￣(x1-y1)^2 +(x2-y2)^2+(x3-y3)^2\n        \n    * k维(欧几里得距离的计算公式)\n    \t* (x1,x2..xk)与(y1,y2..yk)\n        \n    \t* √￣(x1-y1)^2 +(x2-y2)^2+...+(xk-yk)^2","layout":null},"children":[]}]},{"data":{"id":"byjh6jld02g0","created":1574126240107,"text":"索引","note":null,"expandState":"collapse"},"children":[{"data":{"id":"byjhhhdido80","created":1574127097285,"text":"功能性需求","note":"1. 数据是格式化数据还是非格式化数据\n    * 即要构建索引的原始数据是否为结构化数据\n        \n        * MySQL的数据就是结构化数据\n        * 搜索引擎中网页等就是非结构化数据\n        \t* 一般需要做预处理，提取出查询关键词，对关键词构建索引\n            \n            \n2. 数据是静态数据还是动态数据\n    * 静态数据不会有增删改\n        * 构建索引的时候，只需要考虑查询效率就可以\n            \n    * 动态数据\n        * 不仅要考虑到索引的查询效率，还需要支持动态地更新索引\n            \n3. 索引存储在内存还是硬盘\n    * 存储在内存中，那查询的速度肯定要比存储在磁盘中的高\n        * 但是数据量有限制\n            \n    * 数据量大则可以存在硬盘\n        * 或者部分加载到内存\n            \n            \n4. 单值查找还是区间查找\n    * 如题\n        \n5. 单关键词查找还是多关键词组合查找\n\t* 多关键字的，像MySQL这种结构化数据的查询需求\n    \t* 我们可以实现针对多个关键词的组合，建立索引\n        \n    * 对于像搜索引擎这样的非结构数据的查询需求\n    \t* 可以针对单个关键词构建索引，然后通过集合操作，如并集、[交集]等\n        * 计算出多个关键词组合的查询结果。 "},"children":[]},{"data":{"id":"byjhkw830480","created":1574127364701,"text":"非功能性需求","note":"1. 不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大\n\t* 内存本身非常有限，所以有苛刻限制\n    * 硬盘中相对限制会放宽一点\n    \t* 但是也不能太大，例如说超过原始数据大小\n        \n        \n2. 在考虑索引查询效率的同时，我们还要考虑索引的维护成本\n\t* 索引的目的是提高查询效率\n    * 但是也要考虑维护的成本\n    \t* 即原始数据动态增删改"},"children":[]},{"data":{"id":"byjhoo5hzdk0","created":1574127660587,"text":"常见索引","note":"1. 散列表\n\t* 增删改查操作的性能非常好\n    \t* 时间复杂度是O(1)\n        \n    * 一些键值数据库，比如Redis、Memcache，就是使用散列表来构建索引的\n    \t* 这类索引，一般都构建在内存中\n        \n\n2. 红黑树\n\t* 作为一种常用的平衡二叉查找树\n    \t* 数据插入、删除、查找的时间复杂度是O(logn)\n    \n    * 也非常适合用来构建内存索引。Ext文件系统中，对磁盘块的索引，用的就是红黑树\n    \n\n3. B+树\n\t* 比起红黑树来说，更加适合构建存储在磁盘中的索引\n    \t* B+树是一个多叉树，对相同个数的数据构建索引，B+树的高度要低于红黑树\n        * 借助索引查询数据的时候，读取B+树索引，需要的磁盘IO次数非常更少\n    \n    * 大部分关系型数据库的索引，比如MySQL、Oracle，都是用B+树来实现的\n    \n4. 跳表\n\t* 支持快速添加、删除、查找数据\n    \n    * 通过灵活调整索引结点个数和数据个数之间的比例\n    \t* 可以很好地平衡索引对内存的消耗及其查询效率。\n        \n    * Redis中的有序集合，就是用跳表来构建的\n"},"children":[]},{"data":{"id":"byji2ix3bts0","created":1574128746293,"text":"辅助索引","note":"1. 布隆过滤器\n\t* 有一定的判错率\n    * 但是布隆过滤器判断不存在的，就肯定不存在\n    \t* 可以只用于判断是否存在\n        * 如果存在，再用其他方式再次判断\n    * 最大优点：内存占用非常少\n    \n    * 应用：通过构建在内存的布隆过滤器\n    \t* 判断是否存在\n        * 不存在的直接返回，则对于这种不存在的情况，查询就非常快速了\n        \n\n* 此外\n\t* 有序数组也可以被作为索引\n    \t* 前提是静态数据\n        * 对数据进行排序后，二分查找"},"children":[]}]},{"data":{"id":"byjj3ai6skg0","created":1574131627452,"text":"并行算法","note":"* 并行计算\n\t* 当算法无法再继续优化的情况下，使用并行算法进一步提高效率\n    \n\n","expandState":"collapse"},"children":[{"data":{"id":"byjjazqo34w0","created":1574132230933,"text":"并行排序","note":"* 并行排序\n\t* 如归并\n    \t* 先把数据划分为N个小数据集合，如16个\n        * 使用16个线程并行的处理\n        * 最后再合并全部数据\n        \n    * 快排\n    \t* 先扫描一次数据，按照大小分为16个区间\n        * 使用16个线程并行对各个区间排序\n        * 排序完后就是有序的数据了\n        \n        \n    * 这两种分治的思路\n    \t* 一个是先随意分片，排序后合并\n        * 另一个是先按照大小划分，再排序，排序完后不需再处理\n        \n        * 两种思路刚好就是归并与快排之间的区别"},"children":[]},{"data":{"id":"byjjb70ijlc0","created":1574132246766,"text":"并行查找","note":"* 散列表的查找\n\t* 一个大的散列表，假如达到扩容的阈值后，就会进行动态扩容\n    \t* 原本2G的散列表，扩容后变成3G，在没有更多数据进来之前，此时有1G的空闲内存\n    \n    * 如果将数据随机分割为16份\n    \t* 构建16个小散列表\n        * 此时某个散列表装载因子过大，扩容时，只需针对某个散列表即可\n        * 内存利用率更高\n        \n    * 查找时\n    \t* 16个线程并行查询\n        * 性能会更好\n        \n    * 插入数据时，可以选择装载因子最小的\n    \t* 这样也有助于减少散列冲突"},"children":[]},{"data":{"id":"byjjiha6ffk0","created":1574132817667,"text":"并行字符串匹配","note":"* 超长文本匹配\n\t* 将大文本分割成k个小文本，如16\n    \t* 启动16个线程并行在小文本中查找\n    \n    * 特殊处理关键字被分割成2个小文本的情况\n    \t* 假设关键字长度为m\n        * 在每个小文本前后各取m个字符，组成2m个字符\n        \n        * 在2m长度的字符中查找关键字，就可以补上刚才的漏洞了\n        \n    * 记录匹配的关键字，就记录下标即可\n    \t* 因此不怕重复数据"},"children":[]},{"data":{"id":"byjjmwo07w00","created":1574133164612,"text":"并行搜索","note":"* 对于广度优先算法\n\t* 广度优先搜索是按层搜索\n    \t* 可以对同一个顶点的各个下一层顶点，启用多线程并行处理\n        \n    * 单线程中，使用一个队列记录已经遍历但是没处理的顶点\n    \t* 多线程中，可以使用2个队列\n        \n        * 多线程处理队列A的顶点\n        \n        * 扩展得到的顶点存储在队列B\n        \n    * 在队列A的顶点处理完后\n    \t* 队列A将清空\n        \n        * 此时开始遍历队列B，并将扩展得到的新顶点放到队列A"},"children":[]}]}]},{"data":{"id":"byjnxz4ynmw0","created":1574145316436,"text":"算法实战","expandState":"collapse"},"children":[{"data":{"id":"byjo0unc9uw0","created":1574145541756,"text":"Redis","note":"* Redis是一种键值（Key-Value）数据库\n\t* 相对关系型数据库，也被叫做非关系型数据库\n    \n* Redis中只包含“键”和“值”两部分\n\t* 只能通过“键”来查询“值”\n    \t* 键都是字符串\n        * 值有：字符串、列表、字典、集合、有序集合\n    \n    * 正是因为这样简单的存储结构，也让Redis的[读写效率非常高]\n    \n    \n \n* 数据的持久化一般2种方式\n\t1. 清除原有的存储结构，只将数据存储到磁盘中\n    \t* 需要从磁盘还原数据到内存的时候，再重新将数据组织成原来的数据结构\n    \t* Redis采用的就是这种持久化思路\n        \t* 应该指的是RDB\n            \n        * 还原过程较为耗时\n        \t* 如对散列表进行持久化\n            * 需要重新计算每个数据的哈希值\n            \n    2. 保留原来的存储格式\t\n    \t* 将散列表的大小、每个数据被散列到的槽的编号等信息，都保存在磁盘中\n        \t* 有了这些信息，还原数据到内存中的时候，就可以避免重新计算哈希值\n    \t","expandState":"collapse","layout":null},"children":[{"data":{"id":"byl85bd4h680","created":1574303873755,"text":"list 列表","note":"* 列表\n\t* 支持存储一组数据\n    \n    * 实现\n    \t* 压缩列表（ziplist）\n        * 双向循环列表\n  \n \n* 压缩列表\n\t* 列表list中存储的数据量比较小的时候采用[压缩列表]的方式实现\n    \t1. 列表中单个数据小于64字节\n    \t\t* 有可能是字符串类型的\n        \n    \t2. 列表中数据个数少于512个\n        \n        3. 不能同时满足以上两条件时，使用双向链表\n        \n        \n        \n\n        \n        \n   \n       ","layout":null},"children":[]},{"data":{"id":"byleegfgjm00","created":1574321516717,"text":"hash 字典","note":"* 用来存储一组数据对\n\t* 每个数据对又包含键值两部分\n    \n* 字典类型也有两种实现方式\n\t* 一种是压缩列表（满足以下条件使用压缩列表）\n    \t* 字典中保存的键和值的大小都要小于64字节\n        * 字典中键值对的个数要小于512个\n        \n    * 另一种是散列表\n    \t* 使用MurmurHash2作为哈希函数\n        * 是一种运行速度快、随机性好的哈希算法\n        \n\n\n* 哈希冲突时，Redis使用链表法来解决\n\t* 还支持散列表的动态扩容、缩容\n    \n    * 装载因子大于1时\n    \t* 扩容为原来的2倍\n    * 小于0.1时\n    \t* 缩容为字典数据个数的2倍\n        \n    * 扩容缩容要做大量的数据搬移和哈希值的重新计算，所以比较耗时。\n    \t* 针对这个问题，Redis使用了渐进式扩容缩容策略\n        \n        * 将数据的搬移分批进行，避免了大量数据一次性搬移导致的服务停顿。","layout":null},"children":[]},{"data":{"id":"byleuz4a9800","created":1574322811227,"text":"set 集合","note":"* 存储一组不重复的数据\n\n* 实现\n\t* 一种是基于有序数组（满足以下条件）\n    \t* 存储的数据都是整数\n        \t* 其他三个都是要求大小不超64字节\n        * 但是此处[不是压缩列表]\n        \n        * 存储的数据元素个数不超过512个\n        \n    * 另一种是基于散列表","layout":null},"children":[]},{"data":{"id":"bylewz1d6gw0","created":1574322967779,"text":"sorted set 有序集合","note":"* 存储一组数据，并且每个数据会附带一个得分\n\t* 通过得分的大小，我们将数据组织成跳表\n    \t* 以支持快速地按照得分值、得分区间获取数据\n        \n        \n* 实现\n\t* 数据量比较小的时候，用压缩列表来实现\n\t\t* 所有数据的大小都要小于64字节\n\t\t* 元素个数要小于128个\n        \t* 其他三个都是512\n            \n            * 猜测是由于跳表的实现占用更大内存（索引列的指针数组）","layout":null},"children":[]},{"data":{"id":"bym1m3vv2ao0","created":1574387001852,"text":"ziplist 压缩列表","note":"* ziplist\n\t* 是Redis设计的一种数据存储结构 \n    \t* 类似数组，通过一片连续的内存空间，来存储数据\n        \n        * 但它允许存储的数据大小不同\n        \t* 记录数据的同时，也记录数据的大小\n            \n        * 还支持不同类型数据的存储\n        \t\n     \n    * 名字叫压缩，肯定是节省内存了\n    \t* 由于支持存储的数据大小不一样，所以相对数组而言，不会浪费多余的空间\n        * 数组中设定每个元素的大小一样，不满最大尺寸的也消耗一个最大尺寸\n        \n    \n    * 不支持随机访问\n    \t* 但是一般直接根据key获取整个value，即整个压缩列表的数据\n        \t* 并不需要随机访问\n            \n        * 因为使用压缩列表的时候都是数据存储较少的。\n        \n        \n* 注意：\n\t* 此处提到的数据结构，都是一个key对应多个数据（一组数据）\n    \t* 数据结构指的是这个key如何组织对应的数据\n        \n    * 而不同的key获取不同的数据\n    \t* 此处猜测都是使用散列表实现\n    \n    * string类型由于只对应一个值，所以直接通过外层散列表就可以定位\n        \n        ","layout":null},"children":[]}]},{"data":{"id":"bylgdjnmqk00","created":1574327087597,"text":"搜索引擎","note":"* 搜索引擎大致可以分为四个部分\n\t* 搜集\n    \t* 利用爬虫爬取网页\n    * 分析\n    \t* 负责网页内容抽取、分词\n        * 构建临时索引\n        * 计算PageRank值(网页权重)\n        \n    * 索引\n    \t* 通过分析阶段得到的临时索引，构建倒排索引\n        \n    * 查询\n    \t* 负责响应用户的请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果给用户\n        \n        \n* 使用到的算法\n\t1. 广度优先搜索的网页爬取\n    \t* 字符串匹配获取网页中的链接\n    \n    2. 布隆过滤器网页判重\n    \n    3. AC自动机多模式串匹配\n    \t* 替换js，css等标签\n     \n    4. Trie树：匹配最长字符串进行分词\n    \n    5. 多路归并排序组建倒排索引\n    \t* 多路：即多线程\n        * 文件太大，不能使用内存排序算法\n        \t* 拆成多个小文件，排序后合并\n     \n    6. 散列表\n    \t* 统计单词出现次数，用于页面展示文章的顺序\n    ","expandState":"collapse"},"children":[{"data":{"id":"bylgl28gti80","created":1574327676588,"text":"搜集","note":"* 搜索引擎把整个互联网看作数据结构中的有向图\n\t* 把每个页面看作一个顶点\n    \n    * 如果某个页面中包含另外一个页面的链接\n    \t* 那我们就在两个顶点之间连一条有向边\n        \n    * 我们可以利用图的遍历搜索算法，来遍历整个互联网中的网页\n    \t* 搜索引擎采用的是广度优先搜索策略\n        * 先找一些比较知名的网页的链接作为种子网页链接，放入到队列中\n        \t* 知名：专业的叫法是权重比较高\n        * 爬虫按照广度优先的策略搜索","expandState":"expand"},"children":[{"data":{"id":"bym39yaltzs0","created":1574391691532,"text":"待爬取网页链接文件 links.bin ","note":"* 进行广度优先搜索时，需要一个队列存储当前页面解析出来的页面链接\n\t* 内存可能会放不下\n    \t* 因此使用磁盘文件 links.bin存储\n    \n    * 页面解析出链接\t\n    \t* 使用字符串匹配算法搜索\n    \n    \n    \n* 优点\n\t* 存储在文件是持久化的\n    * 断电后可以支持断点续爬\n    \t* 从之前爬取到的位置继续爬取"},"children":[]},{"data":{"id":"bym3fjrz5xs0","created":1574392130116,"text":"网页判重文件 bloom_filter.bin","note":"* 使用布隆过滤器\n\t* 快速，且非常节省内存地实现网页的判重\n    \t* 防止爬取相同网页\n        \n        \n        \n    * 防止机器宕机重启，布隆过滤器被清空\n    \t* 隔一段时间将布隆过滤器持久化到磁盘\n        * 内存中也保留一份，实际使用的时候使用内存的\n        \t* 宕机重启才获取磁盘的"},"children":[]},{"data":{"id":"bym3jeapdcw0","created":1574392431644,"text":"原始网页存储文件：doc_raw.bin","note":"* 爬取到网页之后\n\t* 需要将其存储下来，以备后面离线分析、索引之用\n    \n    \n    * 网页太多，可以把多个网页存储在一个文件中。\n    \t* 每个网页之间，通过一定的标识进行分隔，方便后续读取\n        \n        \n        * 每个网页还对应存储一个唯一编号\n        \t* 方便后续对网页进行分析、索引\n            * 网页与网页编号\n    \n    \n    * 文件也需限制大小，免得过大\n    \t* 如几百M ~ 1G等"},"children":[]},{"data":{"id":"bym3vx675080","created":1574393413101,"text":"网页链接及其编号的对应文件：doc_id.bin","note":"* 按照网页被爬取的先后顺序，从小到大依次编号\n\t* 维护一个中心的计数器\n    \t* 每爬取到一个网页之后，就从计数器中拿一个号码，分配给这个网页，然后计数器加一\n        \n    * 存储网页的同时\n    \t* 我们将[网页链接]跟[编号]之间的对应关系，存储在另一个doc_id.bin文件中。"},"children":[]}]},{"data":{"id":"bym3z1jyc7c0","created":1574393657732,"text":"分析","note":"* 网页爬取下来之后，需要对网页进行离线分析\n\t* 抽取网页文本信息\n    * 分词并创建临时索引"},"children":[{"data":{"id":"bym40cn5qq00","created":1574393760235,"text":"抽取网页文本信息","note":"* 网页是半结构化数据\n\t* 里面夹杂着各种标签、JavaScript代码、CSS样式\n    \t* 对于搜索引擎来说，它只关心网页中的文本信息\n        \n    * 网页是半结构化数据，是因为它本身是按照一定的规则来书写的\n    \t* 即HTML语法规范\n     \n     \n* 可以依靠HTML标签来抽取网页中的文本信息\n\t1. 去掉JavaScript代码、CSS格式以及下拉框中的内容\n    \t* 使用AC自动机这种多模式串匹配算法\n        * 在网页这个大字符串中，一次性查找<style>, <script>, <option>这三个关键词。当找到某个关键词出现的位置之后，我们只需要依次往后遍历，直到对应结束标签（</style>, </script>, </option）为止\n        * 期间遍历到的字符串连带着标签就应该从网页中删除\n    \n    2. 去掉所有HTML标签\n    \t* 也是字符串匹配算法实现\n        * 不一样的是，标签内的内容bushanchu"},"children":[]},{"data":{"id":"bym40ggef200","created":1574393768533,"text":"分词并创建临时索引","note":"* 抽取完文本信息后，对文本信息进行分词，并且创建临时索引\n\n* 基于字典和规则的分词方法\n\t* 字典即是词库，里面包含大量常用的词语\n    \t* 可以直接从网上下载别人整理好的\n        \n    * 借助词库并采用最长匹配规则，来对文本进行分词。\n    \t* 即匹配尽可能长的词语\n        \n    * 实现：将词库中单词构建成Trie树\n    \t* 将网页文本在Trie树中进行匹配\n        \n \n* 网页的文本分词后，得到一组单词列表\n\t* 将单词与网页之间的对应关系，写入到一个临时索引文件中\n    \t* `tmp_Index.bin`\n        \n        * 文件中记录的是[单词编号]，及其对应的[网页编号]\n        \t* 没有记录单词本身，是为了节省空间\n\n        \n    * 临时索引文件用来构建倒排索引\n    \n* 单词编号的方式，跟给网页编号类似\n    * 维护一个计数器，每当从网页文本信息中分割出一个新的单词\n    \t* 就从计数器中取一个编号，分配给它，然后计数器加一\n    \n    * 另外使用散列表\n    \t* 记录已经编过号的单词\n        \n* [单词跟单词编号]之间的对应关系，写入到磁盘文件中，并命名为`term_id.bin`"},"children":[]},{"data":{"id":"bym9bz4t92o0","created":1574408776747,"text":"阶段处理获得的文件","note":"* 临时索引文件 tmp_Index.bin\n\n* 单词跟单词编号 对应文件 term_id.bin"},"children":[]}]},{"data":{"id":"bym8ywyv77k0","created":1574407753299,"text":"索引","note":"* 负责将分析阶段产生的临时索引\n\t* 构建成倒排索引 （Inverted index）\n    \t* 记录了每个单词以及包含它的网页列表\n        * 记录单词编号，网页编号\n        \n        \n* 临时索引中，已经记录单词跟每个包含它的文档之间的对应关系\n\t* 但是这个对应是一对一的\n    \n    * 构建到倒排索引\n    \t* 就是构建每个单词 对应的全部文章列表\n       \n       \n* 实现\n    1. 先对临时索引文件，按照单词编号排序\n    \t* 临时索引较大，基于内存的排序无法处理\n        * 可以使用归并排序，先分割为多个小文件，排序后合并\n        \n        * 实际的软件开发中，我们其实可以直接利用MapReduce来处理...\n        \n    2. 排序后，相同的单词就排列在一起\n    \t* 顺序地遍历排好序的临时索引文件\n        * 就能将每个单词对应的网页编号列表找出来，然后把它们存储在倒排索引文件中 \n  \n* 其他\n\t* 需要一个文件，来记录每个单词编号在倒排索引文件中的偏移位置\n    \t* term_offset.bin\n        \n        * 快速地查找某个单词编号在倒排索引中存储的位置\n        * 进而快速地从倒排索引中读取单词编号对应的网页编号列表。","expandState":"expand"},"children":[{"data":{"id":"bym9cyzuvzk0","created":1574408854811,"text":"阶段处理所得文件","note":"* 倒排索引文件（index.bin）\n\n* 单词编号在索引文件中的偏移位置的文件（term_offset.bin）"},"children":[]}]},{"data":{"id":"bym9dq95zk00","created":1574408914147,"text":"查询","note":"1. 前三阶段文件\n\t1. doc_id.bin\n    \t* 记录网页链接和编号之间的对应关系\n\n\t2. term_id.bin\n    \t* 记录单词和编号之间的对应关系\n\n\t3. index.bin：\n    \t* 倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表。\n\n\t4. term_offsert.bin\n    \t* 记录每个单词编号在倒排索引文件中的偏移位置\n        \n    * 其他两个文件\n    \t* 原始网页文件:用于分词用，已经不需要了\n        * 临时索引文件：用于构建倒排索引，也不需要了\n        \n        \n"},"children":[{"data":{"id":"bymbndxzicg0","created":1574415313211,"text":"查询具体操作","note":"* 4个文件中，除了倒排索引较大，其他的可以加载进内存\n\t* 组织成散列表形式\n    \n    \n*  用户输入文本\n\t1. 先对用户输入的文本进行分词处理，得到k个单词\n    \t* 将k个单词去term_id.bin中获取对应单词编号\n        \n    2. 根据单词编号去term_offsert.bin中获取每个单词的偏移量\n    \t* 根据偏移量去去倒排索引中获取对应的网页编号\n        \n    3. 针对这k个网页编号列表，统计每个网页编号出现的次数\n    \t* 可以借助散列表来进行统计\n        \n        * 我们按照出现次数的多少，从小到大排序\n        \t* 出现次数越多，说明包含越多的用户查询单词\n            \n    4. 根据排好序的网页编号、\n    \t* 去doc_id.bin文件中查找对应的网页链接，分页显示给用户"},"children":[]}]}]},{"data":{"id":"bymwojowxm80","created":1574474647396,"text":"高性能队列Disruptor","note":"* Disruptor是一个并发框架\n\t* 一种内存消息队列\n    \t* 是线程之间用于消息传递的队列\n    * 性能很好\n    \t* 比ArrayBlockingQueue的性能，要高一个数量级\n        \n        * 可以算得上是最快的内存消息队列了。它还因此获得过Oracle官方的Duke大奖\n        \n        \n        \n* 队列实现\n\t* 一种是基于链表实现的链式队列\n    \t* 适用于构建无界队列\n        \n    * 基于数组实现的顺序队列\n    \t* 更常用。因为内存是有限的\n        \n        * 循环队列（Disruptor采用）\n        \t* 不用数据搬移，比一般数组性能更好\n            * 大部分用到顺序队列的场景中，都选择用顺序队列中的循环队列","expandState":"collapse"},"children":[{"data":{"id":"bymxrf3o0rk0","created":1574477693606,"text":"线程安全处理","note":"* 除了加锁，CAS外\n\t* Disruptor有自己的实现方式\n    \n    \n1. 添加数据时\n\t* 批量地申请连续的n个（n≥1）存储单元（过程加锁）\n    \t* 后续往队列中添加元素，就可以不用加锁\n        \t* 因为这组存储单元是这个线程独享的\n            \n\n2. 消费数据时\n\t* 类似的，申请一批连续可读的存储单元\n    \t* 这个申请的过程也是需要加锁的\n        * 当申请到这批存储单元之后，后续的读取操作就可以不用加锁了\n        \n        \n        \n* 注：\n\t* 生产者A申请到了一组连续的存储单元\n    \t* 假设是下标为3到6的存储单元\n    * 生产者B紧跟着申请到了下标是7到9的存储单元\n    \n    * 那在3到6没有完全写入数据之前，7到9的数据是[无法读取]的\n    \n    \t* 此处猜测是（实际要看源码），写入，读取还是基于唯一的head、tail指针\n        * 一个线程占了多个存储空间，但是没有处理完的话，其他线程需要等待"},"children":[]},{"data":{"id":"bynrtliuzx40","created":1574562497611,"text":"分布式唯一id可参考此方式"},"children":[]}]},{"data":{"id":"byn2ee0vt140","created":1574490778076,"text":"鉴权限流","expandState":"collapse"},"children":[{"data":{"id":"byn2goyw6dk0","created":1574490958629,"text":"鉴权","note":"1. 微服务中\n\t* 对于每个接口，可能只有部分应用才能访问\n    \n\n\n            \n"},"children":[{"data":{"id":"byn3w0fpv080","created":1574494980163,"text":"精确匹配","note":"* 精确匹配\n\t* 只有当请求URL跟规则中配置的某个接口精确匹配时，\n    \t* 这个请求才会被接受\n        \n        \n    * 每个应用对应匹配一组可访问的url\n    \t* 应用对应url可以使用散列表来存储\n        \n    \t* 可访问的url配置格式为： /user/info; /user/login \n        \n    * 规则不会经常变动\n    \t* 可以将字符串按照字母排序，放进有序数组\n        \n        * 使用二分进行查找\n    \t"},"children":[]},{"data":{"id":"byn3yofo5o00","created":1574495189132,"text":"前缀匹配","note":"* 精确匹配\n\t* 即访问的url的前缀在规则中有记录\n    \t* 则允许访问\n        \n        \n\t* 可以将多个字符串规则组织成Trie树\n        * 此时Trie树的结构就不是每个节点一个字母\n        \t* 而是每个节点一个单词\n            \n        * 节点的子节点是多个单词\n        \t* 可以按照字母编号进行排序，形成有序数组，可以进行二分查找"},"children":[]},{"data":{"id":"byn3yr8d6bs0","created":1574495195220,"text":"模糊匹配","note":"* 模糊匹配\n\t* 如包含通配符 * 等\n    \t* 使用回溯算法进行匹配，跟之前回溯解决正则表达式的方案一样\n        \n    * 回溯的时间复杂度非常高\n    \t* 可以将规则分为含有通配符以及不含有通配符的\n        \n        * 不含有通配符的按照精确匹配处理"},"children":[]}]},{"data":{"id":"byn4t8coqqo0","created":1574497583412,"text":"鉴权"},"children":[{"data":{"id":"byn4tnqojxs0","created":1574497616910,"text":"固定时间窗口限流算法","note":"* 固定时间窗口\n\t* 例如1秒内\n    \t* 累加访问次数。超过就拒绝\n    * 1s后清零重新计算\n    \n* 缺点\n\t* 无法应对两个时间窗口临界时间内的突发流量\n    \t* 前1s的最后10ms，与下一秒的前10ms，访问数量达到限制值\n        \t* 此时并不能拦截"},"children":[]},{"data":{"id":"byn5dy9wzfc0","created":1574499207124,"text":"滑动时间窗口限流算法","note":"* 维护一个K+1的循环队列\n\t* 循环队列浪费一个存储单元\n    \n* 有新的请求来的时候\n\t* 把当前队列中已经超过1s的请求删掉\n    \t* 此时有空闲位置，则通过，记录此请求\n        * 否则拒绝\n        \n        \n* 缺点\n\t* 还是不能防止细时间粒度上访问过于集中的问题"},"children":[]}]}]},{"data":{"id":"bynseflkj200","created":1574564130362,"text":"短网址","note":"* 即将长网址转化为短网址","expandState":"collapse"},"children":[{"data":{"id":"bynsftuyow80","created":1574564239769,"text":"哈希算法方式","note":"* 使用hash算法生成短网址\n\t* 此时，并不需要考虑反向解密的难度\n    * 只关心哈希算法的计算速度、冲突概率\n    \n\n* MurmurHash算法\n\t* 这个哈希算法在2008年才被发明出来\n    \t* 但现在它已经广泛应用到Redis、MemCache、Cassandra、HBase、Lucene等众多著名的软件中。\n        \n    * 提供了两种长度的哈希值，一种是32bits，一种是128bits\n    \n    \n    \n* 算法算出来的是数字\n\t* 将原来的十进制，换成更高进制\n    \t* 网址会更短\n    * 常用字符：数字+大小写英文\n    \t* 一共62个，因此转化为62进制\n        \n        \n* 哈希冲突解决\n\t* 一般长短网址是需要将映射关系存储起来\n    \t* 如mysql：方便跳转使用\n    \n    * 当一个长网址的hash值冲突时\n    \t* 先确定是不是同一个长网址\n        \t* 是则直接返回原短地址\n            \n    \t* 在长网址上加上特定的字符串标记\n        \t* 重新hash\n            * 还不行再添加第二个特定标记\n        * 查询时，取出长网址后，有特定字符的先取去除掉","expandState":"collapse"},"children":[{"data":{"id":"bynsp86qe080","created":1574564976233,"text":"性能优化","note":"*  生成的短网址，每次都要数据库检查是否存在\n\t* 首先短网址的索引是必须加的\n\t* 耗费性能，因为重复的概率不大\n    \n    1. 可以将短网址设置为唯一索引\n    \t* 直接插进数据库\n        * 唯一索引限制报错后，针对特定异常捕获\n        \t* 再进行加特定字符，重新哈希等等操作\n        \n        * 此方法，个人觉得并不好\n        \n    2. 使用布隆过滤器\n    \t* 不存在的肯定就是不存在\n        * 且快速，省内存\n        \t* 长度是10亿的布隆过滤器，也只需要125MB左右的内存空间\n        \n        * 布隆过滤器中存在的才查询数据库"},"children":[]}]},{"data":{"id":"bynsu18c6nk0","created":1574565352913,"text":"id生成器","note":"* 自增方式\n\t* 每来一个网址，就加一\n    \t* 转化为62进制，完成\n        \n* 缺点\n\t1. 相同网址处理\n    \t* 不处理，用户并不关心短网址的样子\n        \t* 但是重复网址浪费存储空间\n        \n        * 长网址也加索引，先查询长网址是否存在（比上面的更占空间）\n        \t* 多一个索引浪费更多空间\n            * 并且长网址一般也比较长\n            \n            \n* 高性能id生成器实现\n\t* id生成器需要加锁来保证唯一性\n    \t* 还需要有较高并发能力\n        \n    1. id 生成器将多个小批量的id发给前置发号器\n    \t* 所有获取id的都分别去找各个前置发号器获取\n        \n        * 由此增加了并发能力\n        \n    \n    2. 直接使用多个id生成器\n    \t* 每个id生成器有一定规则保证不重复\n        \t* 如最后一位是机器位，每个生成器不一样\n        \n     "},"children":[]}]}]}]},"template":"right","theme":"fresh-green","version":"1.4.43"}